diff --git a/Makefile b/Makefile
index 44efd1252ab8..ecc989f84637 100644
--- a/Makefile
+++ b/Makefile
@@ -399,6 +399,8 @@ KBUILD_CFLAGS   := -Wall -Wundef -Wstrict-prototypes -Wno-trigraphs \
 		   -Wno-format-security \
 		   -std=gnu89 $(call cc-option,-fno-PIE)
 
+KBUILD_CFLAGS += $(call cc-disable-warning, attribute-alias) # for newer gcc (8)
+KBUILD_CFLAGS += $(call cc-disable-warning, stringop-truncation) # for newer gcc (8)
 
 KBUILD_AFLAGS_KERNEL :=
 KBUILD_CFLAGS_KERNEL :=
diff --git a/README.md b/README.md
index d57329fa391a..9bcb597df0b0 100644
--- a/README.md
+++ b/README.md
@@ -1,3 +1,14 @@
+"pophype-dshm" branch is for PopHype project
+
+"pop-hype" branch is for PuzzleHype project
+
+"master-tso-develop" is a private & development branch which is only used for synchronizing/backing up code on different machines.
+ATTENTION: If you are looking for the lastest release code of TSO implementation, please checkout "master-tso" branch.
+
+"*-dev" branch are under development
+
+----------------------------------------------
+
 Popcorn Linux for Distributed Thread Execution
 ----------------------------------------------
 
diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c
index e8835d4e173c..44edae61fea3 100644
--- a/arch/arm/kvm/mmu.c
+++ b/arch/arm/kvm/mmu.c
@@ -30,6 +30,8 @@
 #include <asm/kvm_emulate.h>
 
 #include "trace.h"
+#include "../../../kernel/popcorn/pgtable.h"
+#include <popcorn/debug.h>
 
 extern char  __hyp_idmap_text_start[], __hyp_idmap_text_end[];
 
@@ -52,6 +54,12 @@ static phys_addr_t hyp_idmap_vector;
 
 static bool memslot_is_logging(struct kvm_memory_slot *memslot)
 {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (memslot->flags & KVM_MEM_READONLY) {
+		POPPK_GDSHM("%s(): memslot->flags & KVM_MEM_READONLY 0x%lx (0)\n",
+				__func__, memslot->flags & KVM_MEM_READONLY); // result is 0
+	}
+#endif
 	return memslot->dirty_bitmap && !(memslot->flags & KVM_MEM_READONLY);
 }
 
@@ -66,7 +74,11 @@ void kvm_flush_remote_tlbs(struct kvm *kvm)
 	kvm_call_hyp(__kvm_tlb_flush_vmid, kvm);
 }
 
+#ifdef CONFIG_POPCORN_DSHM
+void kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
+#else
 static void kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
+#endif
 {
 	/*
 	 * This function also gets called when dealing with HYP page
@@ -83,7 +95,11 @@ static void kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
  * value, as they are flushing the cache using the kernel mapping (or
  * kmap on 32bit).
  */
+#if defined(CONFIG_POPCORN_DSHM)
+void kvm_flush_dcache_pte(pte_t pte)
+#else
 static void kvm_flush_dcache_pte(pte_t pte)
+#endif
 {
 	__kvm_flush_dcache_pte(pte);
 }
@@ -211,17 +227,40 @@ static void unmap_ptes(struct kvm *kvm, pmd_t *pmd,
 
 	start_pte = pte = pte_offset_kernel(pmd, addr);
 	do {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+		// msg_layer: yes here
+		if (addr == 0x80000000) { // GPA(IPA)
+			POPPK_GDSHM("\t%s(): GPA(IPA) 0x%llx pte %p *pte 0x%llx pfn 0x%llx "
+					"pte_none(*pte) %s\n",
+					__func__, addr, pte, *pte, pte_pfn(*pte),
+					pte_none(*pte) ? "O*** didn't inv..." :
+									"X -> do kvm_set_pte(0) + "
+					"kvm_tlb_flush_vmid_ipa() + kvm_flush_dcache_pte()");
+		}
+#endif
 		if (!pte_none(*pte)) {
+			/* has content in entry, pte_none = no page allocated */
 			pte_t old_pte = *pte;
 
-			kvm_set_pte(pte, __pte(0));
-			kvm_tlb_flush_vmid_ipa(kvm, addr);
+			kvm_set_pte(pte, __pte(0)); /* pophype: this is just zero out */
+			kvm_tlb_flush_vmid_ipa(kvm, addr); /* pophype: inv 2pt */
 
 			/* No need to invalidate the cache for device mappings */
 			if (!kvm_is_device_pfn(pte_pfn(old_pte)))
 				kvm_flush_dcache_pte(old_pte);
 
 			put_page(virt_to_page(pte));
+
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+			if (addr == 0x80000000) { // GPA(IPA)
+				POPPK_GDSHM("\t%s(): GPA(IPA) 0x%llx "
+						"kvm_is_device_pfn(pte_pfn(old_pte) %s "
+						"(This does all things...)\n",
+						__func__, addr,
+						kvm_is_device_pfn(pte_pfn(old_pte)) ? "O***" :
+										"X -> kvm_flush_dcache_pte()");
+			}
+#endif
 		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 
@@ -318,8 +357,17 @@ static void stage2_flush_ptes(struct kvm *kvm, pmd_t *pmd,
 
 	pte = pte_offset_kernel(pmd, addr);
 	do {
-		if (!pte_none(*pte) && !kvm_is_device_pfn(pte_pfn(*pte)))
+		if (!pte_none(*pte) && !kvm_is_device_pfn(pte_pfn(*pte))) {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+			// only init...........no..........
+			// msg_layer: not entered........
+			//if (addr == 0x80000000) { // GPA(IPA)
+				POPPK_GDSHM("%s(): 0x%llx pte %p *pte %llx\n",
+							__func__, addr, pte, *pte);
+			//}
+#endif
 			kvm_flush_dcache_pte(*pte);
+		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 }
 
@@ -381,7 +429,11 @@ static void stage2_flush_memslot(struct kvm *kvm,
  * Go through the stage 2 page tables and invalidate any cache lines
  * backing memory already mapped to the VM.
  */
+#if defined(CONFIG_POPCORN_DSHM)
+void stage2_flush_vm(struct kvm *kvm)
+#else
 static void stage2_flush_vm(struct kvm *kvm)
+#endif
 {
 	struct kvm_memslots *slots;
 	struct kvm_memory_slot *memslot;
@@ -730,6 +782,9 @@ int kvm_alloc_stage2_pgd(struct kvm *kvm)
 
 	kvm_clean_pgd(pgd);
 	kvm->arch.pgd = pgd;
+#if defined(CONFIG_POPCORN_DSHM)
+	printk("%s %s(): kvm installs pgd %p\n", __FILE__, __func__, pgd);
+#endif
 	return 0;
 }
 
@@ -744,7 +799,11 @@ int kvm_alloc_stage2_pgd(struct kvm *kvm)
  * destroying the VM), otherwise another faulting VCPU may come in and mess
  * with things behind our backs.
  */
+#ifdef CONFIG_POPCORN_DSHM
+void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size)
+#else
 static void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size)
+#endif
 {
 	assert_spin_locked(&kvm->mmu_lock);
 	unmap_range(kvm, kvm->arch.pgd, start, size);
@@ -933,6 +992,11 @@ static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
 		 * Ignore calls from kvm_set_spte_hva for unallocated
 		 * address ranges.
 		 */
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+		if (addr == 0x80000000) {
+			POPPK_GDSHM("\n\n\n%s %s(): 0x%llx !pmd ret 0\n\n\n", __FILE__, __func__, addr);
+		}
+#endif
 		return 0;
 	}
 
@@ -945,8 +1009,15 @@ static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
 
 	/* Create stage-2 page mappings - Level 2 */
 	if (pmd_none(*pmd)) {
-		if (!cache)
+		if (!cache) {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+			if (addr == 0x80000000) {
+				POPPK_GDSHM("\n\n\n%s %s(): 0x%llx clean pmd & "
+							"!cache ret 0\n\n\n", __FILE__, __func__, addr);
+			}
+#endif
 			return 0; /* ignore calls from kvm_set_spte_hva */
+		}
 		pte = mmu_memory_cache_alloc(cache);
 		kvm_clean_pte(pte);
 		pmd_populate_kernel(NULL, pmd, pte);
@@ -955,15 +1026,36 @@ static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
 
 	pte = pte_offset_kernel(pmd, addr);
 
-	if (iomap && pte_present(*pte))
+	if (iomap && pte_present(*pte)) {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+		if (addr == 0x80000000) {
+			POPPK_GDSHM("\n\n\n%s %s(): 0x%llx iomap "
+				"!pte_present ret -EFAULT\n\n\n", __FILE__, __func__, addr);
+		}
+#endif
 		return -EFAULT;
+	}
 
 	/* Create 2nd stage page table mapping - Level 3 */
 	old_pte = *pte;
-	if (pte_present(old_pte)) {
+	if (pte_present(old_pte)) { /* pophype huntch: clean up and inv */
 		kvm_set_pte(pte, __pte(0));
 		kvm_tlb_flush_vmid_ipa(kvm, addr);
+		/* pophype: maybe some thing oppsite to get_page is missing? */
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+		if (addr == 0x80000000) {
+			POPPK_GDSHM("%s %s(): 0x%llx pte_present (GOOD)\n",
+									__FILE__, __func__, addr);
+		}
+#endif
 	} else {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+		if (addr == 0x80000000) {
+			POPPK_GDSHM("%s %s(): 0x%llx !pte_present (GOOD)\n",
+										__FILE__, __func__, addr);
+			//dump_stack();
+		}
+#endif
 		get_page(virt_to_page(pte));
 	}
 
@@ -1015,6 +1107,118 @@ out:
 	return ret;
 }
 
+#if defined(CONFIG_POPCORN_DSHM) && defined(__aarch64__)
+extern struct kvm_vcpu *first_vcpu;
+void mimic_abort_behav(gpa_t gpa) { // gap=ipa // Jack
+	pmd_t *pmd;
+	pte_t *pte;
+	pfn_t pfn;
+	struct kvm *kvm;
+	phys_addr_t fault_ipa = (phys_addr_t)gpa;
+	bool fault_ipa_uncached = false;
+	struct kvm_mmu_memory_cache *memcache = &first_vcpu->arch.mmu_page_cache;
+	unsigned long flags = 0 | KVM_S2_FLAG_LOGGING_ACTIVE; // KVM_S2PTE_FLAG_IS_IOMAP
+
+	if (!first_vcpu) {
+		POPPK_GDSHM("\n\n\n\n\n\n[WRONG] !first_vcpu\n\n\n\n\n\n\n");
+		return;
+	}
+
+	kvm = first_vcpu->kvm;
+
+	/* Get pte (ref kvm_age_hva_handler()) */
+	pmd = stage2_get_pmd(kvm, NULL, (phys_addr_t)gpa);
+	if (!pmd || pmd_none(*pmd)) {	/* Nothing there */
+		POPPK_GDSHM("\n\n\n\n[WRONG] %s %s(): No pmd 0x%llx\n\n\n",
+										__FILE__, __func__, gpa);
+		return;
+	}
+
+	pte = pte_offset_kernel(pmd, gpa);
+	if (pte_none(*pte)) {
+		POPPK_GDSHM("\n\n\n\n[WRONG] %s %s(): No pte 0x%llx\n\n\n",
+									__FILE__, __func__, gpa);
+		return;
+	}
+
+	pfn = pte_pfn(*pte);
+	//pte_t n pte = pfn_pte(pfn, mem_type);
+	POPPK_GDSHM("\t%s(): ME try to mmic abort "
+			"pfn 0x%lx pte %p *pte 0x%lx (same as unmap_ptes) "
+			"fault_ipa 0x%llx == gpa 0x%llx !kvm_is_reserved_pfn(pfn) %s\n",
+			__func__, pfn, pte, *pte, fault_ipa, gpa,
+			kvm_is_reserved_pfn(pfn) ? "O*** !get_page" : "X");
+
+	/* This doesn't afftect stage2_set_pte...... the pte still indicates present */
+	clear_pte_bit(*pte, PTE_VALID); // more than what DSM does
+	//clear_pte_bit(*pte, PTE_VALID | PTE_PROT_NONE); // more than what DSM does
+	kvm_flush_remote_tlbs(kvm);
+	// kvm_tlb_flush_vmid_ipa
+
+	/* mimc user_mem_abort() */
+	/* w/ kvm_get_pfn or using others will make host kernel panic */
+	kvm_get_pfn(pfn); // get
+	// !kvm_is_reserved_pfn(pfn) good
+	//get_page(virt_to_page(pte)); // force stage2_set_pte's behaviour
+	// or donothing let stage2 to trigger !pte_present -> get_page()
+	// Testing if don't get_page here, will let stage2_set_pte do !pte_present -> get_page() .... ret: kernel panic
+	// !pte_present: swap
+
+	/* fully mimic... R is fine (if all enable still not working, idk....)*/
+	coherent_cache_guest_page(first_vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);
+	//stage2_set_pte(first_vcpu->kvm, memcache, fault_ipa, &new_pte, flags);
+	stage2_set_pte(first_vcpu->kvm, memcache, fault_ipa, pte, flags);
+
+	// ../../../kernel/popcorn/pgtable.h
+	kvm_set_pfn_accessed(pfn);
+	/* My make it in swap = !present */
+	//pte_clear_flags(pte, _PAGE_PRESENT | _PAGE_PROTNONE);
+	//pte_clear_flags(*pte, PTE_VALID | PTE_PROT_NONE);
+	//pte_make_invalid(*pte); // *pte = entry
+	//clear_pte_bit(*pte, PTE_VALID | PTE_PROT_NONE); // more than what DSM does
+	clear_pte_bit(*pte, PTE_VALID); // more than what DSM does
+	kvm_flush_remote_tlbs(kvm);
+	// kvm_tlb_flush_vmid_ipa
+	kvm_release_pfn_clean(pfn); // put
+}
+
+pte_t* gpa_to_guest_pte(gpa_t gpa) {
+	pmd_t *pmd;
+	pte_t *pte = NULL;
+	pfn_t pfn;
+	struct kvm *kvm;
+
+	if (!first_vcpu) {
+		POPPK_GDSHM("\n\n\n\n\n\n[WRONG] !first_vcpu\n\n\n\n\n\n\n");
+		return NULL;
+	}
+
+	kvm = first_vcpu->kvm;
+
+	/* Get pte (ref kvm_age_hva_handler()) */
+	pmd = stage2_get_pmd(kvm, NULL, (phys_addr_t)gpa);
+	if (!pmd || pmd_none(*pmd)) {	/* Nothing there */
+		POPPK_GDSHM("\n\n\n\n[WRONG] %s %s(): No pmd 0x%llx\n\n\n",
+										__FILE__, __func__, gpa);
+		return NULL;
+	}
+
+	pte = pte_offset_kernel(pmd, gpa);
+	if (pte_none(*pte)) {
+		POPPK_GDSHM("\n\n\n\n[WRONG] %s %s(): No pte 0x%llx\n\n\n",
+									__FILE__, __func__, gpa);
+		return NULL;
+	}
+
+	pfn = pte_pfn(*pte);
+	//pte_t n pte = pfn_pte(pfn, mem_type);
+	POPPK_GDSHM("\t%s(): pfn 0x%lx pte %p *pte 0x%lx (same as unmap_ptes)\n",
+					__func__, pfn, pte, *pte);
+
+	return pte;
+}
+#endif
+
 static bool transparent_hugepage_adjust(pfn_t *pfnp, phys_addr_t *ipap)
 {
 	pfn_t pfn = *pfnp;
@@ -1224,8 +1428,13 @@ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 	kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);
 }
 
+#ifdef CONFIG_POPCORN_DSHM
+void coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,
+				      unsigned long size, bool uncached)
+#else
 static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn,
 				      unsigned long size, bool uncached)
+#endif
 {
 	__coherent_cache_guest_page(vcpu, pfn, size, uncached);
 }
@@ -1246,12 +1455,31 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	bool fault_ipa_uncached;
 	bool logging_active = memslot_is_logging(memslot);
 	unsigned long flags = 0;
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	int jack = -1; // do inv
+	pte_t *jack_pte = NULL;
+#endif
+
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (hva == 0x7ffd64000000) {
+		POPPK_GDSHM("\t\t%s(): 1 hva %lx fault_ipa %llx %s\n",
+				__func__, hva, fault_ipa,
+				kvm_is_write_fault(vcpu) ? "W" : "R"); // print fault_ipa
+	}
+#endif
 
 	write_fault = kvm_is_write_fault(vcpu);
 	if (fault_status == FSC_PERM && !write_fault) {
 		kvm_err("Unexpected L2 read permission error\n");
 		return -EFAULT;
 	}
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (hva == 0x7ffd64000000) {
+		POPPK_GDSHM("\t\t%s(): 1-1 hva %lx fault_ipa %llx %s\n",
+				__func__, hva, fault_ipa,
+				kvm_is_write_fault(vcpu) ? "W" : "R"); // print fault_ipa
+	}
+#endif
 
 	/* Let's check if we will get back a huge page backed by hugetlbfs */
 	down_read(&current->mm->mmap_sem);
@@ -1261,6 +1489,13 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 		up_read(&current->mm->mmap_sem);
 		return -EFAULT;
 	}
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (hva == 0x7ffd64000000) {
+		POPPK_GDSHM("\t\t%s(): 1-2 hva %lx fault_ipa %llx %s\n",
+				__func__, hva, fault_ipa,
+				kvm_is_write_fault(vcpu) ? "W" : "R"); // print fault_ipa
+	}
+#endif
 
 	if (is_vm_hugetlb_page(vma) && !logging_active) {
 		hugetlb = true;
@@ -1287,6 +1522,14 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	if (ret)
 		return ret;
 
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (hva == 0x7ffd64000000) {
+		POPPK_GDSHM("\t\t%s(): 1-3 hva %lx fault_ipa %llx %s\n",
+				__func__, hva, fault_ipa,
+				kvm_is_write_fault(vcpu) ? "W" : "R"); // print fault_ipa
+	}
+#endif
+
 	mmu_seq = vcpu->kvm->mmu_notifier_seq;
 	/*
 	 * Ensure the read of mmu_notifier_seq happens before we call
@@ -1303,6 +1546,14 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	if (is_error_pfn(pfn))
 		return -EFAULT;
 
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	/* gfn_to_pfn_prot (which calls get_user_pages) */
+	if (hva == 0x7ffd64000000) {
+		POPPK_GDSHM("\t\t%s(): 1-4 hva %lx fault_ipa %llx %s\n",
+				__func__, hva, fault_ipa,
+				kvm_is_write_fault(vcpu) ? "W" : "R"); // print fault_ipa
+	}
+#endif
 	if (kvm_is_device_pfn(pfn)) {
 		mem_type = PAGE_S2_DEVICE;
 		flags |= KVM_S2PTE_FLAG_IS_IOMAP;
@@ -1330,8 +1581,32 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	if (!hugetlb && !force_pte)
 		hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa);
 
-	fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT;
-
+	fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT; /* readonly */
+#if defined(CONFIG_POPCORN_DSHM)
+	// many
+	if (hva == 0x7ffd64000000) {
+		POPPK_GDSHM("\t\t%s(): I'm forcing fault_ipa_uncached to be 1 now "
+				"by forcing !KVM_MEMSLOT_INCOHERENT "
+				"to make fault_ipa_uncached 0\n",
+				__func__);
+	}
+#endif
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (hva == 0x7ffd64000000) {
+		POPPK_GDSHM("\t\t%s(): 2 WOW "
+			"hva %lx fault_ipa %llx %s "
+			"fault_ipa_uncached %d (READ-ONLY) "
+			"hugetlb %d kvm_is_device_pfn(pfn) %d "
+			"pfn %llx "
+			"---> coherent_cache_guest_page() + "
+			"stage2_set_pte()(kvm_set_pte(0)+kvm_tlb_flush_vmid_ipa()) + "
+			"(kvm_set_s2pte_writable())\n",
+			__func__, hva, fault_ipa,
+			kvm_is_write_fault(vcpu) ? "W" : "R",
+			fault_ipa_uncached, hugetlb, kvm_is_device_pfn(pfn),
+			pfn);
+	}
+#endif
 	if (hugetlb) {
 		pmd_t new_pmd = pfn_pmd(pfn, mem_type);
 		new_pmd = pmd_mkhuge(new_pmd);
@@ -1343,20 +1618,81 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);
 	} else {
 		pte_t new_pte = pfn_pte(pfn, mem_type);
-
-		if (writable) {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+		jack_pte = &new_pte;
+		if (hva == 0x7ffd64000000) {
+			POPPK_GDSHM("  [dbg] 0x%lx - 1 pte_present %s pte_none %s\n",
+						hva, pte_present(*jack_pte) ? "O***" : "X",
+						pte_none(*jack_pte) ? "O***" : "X");
+		}
+#endif
+		if (writable) { /* pophpye: only prepare */
 			kvm_set_s2pte_writable(&new_pte);
 			kvm_set_pfn_dirty(pfn);
 			mark_page_dirty(kvm, gfn);
 		}
+		/* pophype: must use fault_ipa to do things */
 		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);
 		ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags);
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+		if (hva == 0x7ffd64000000) {
+			POPPK_GDSHM("  [dbg] 0x%lx - 2 pte_present %s pte_none %s\n",
+						hva, pte_present(*jack_pte) ? "O***" : "X",
+						pte_none(*jack_pte) ? "O***" : "X");
+		}
+		jack = 1; // do inv
+#endif
 	}
 
 out_unlock:
 	spin_unlock(&kvm->mmu_lock);
 	kvm_set_pfn_accessed(pfn);
 	kvm_release_pfn_clean(pfn);
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (hva == 0x7ffd64000000) {
+		if (jack > 0) { // 這邊要成功要同時滿足兩點 1.code正確 2.觀念正確
+			// Result: Doesnt loop forever.... (deson't crash)
+			//kvm_get_pfn(pfn);
+			//pte_t new_pte = pfn_pte(pfn, mem_type);
+			//printk("%s %s(): force kvm_tlb_flush_vmid_ipa() - 1 "
+			//		"to test if inv stage2pt works "
+			//		"(This doesn't cause looping...) %lx "
+			//		"pte_none(new_pte) %d\n",
+			//		__FILE__, __func__, hva,
+			//		pte_none(new_pte) ? "O***" :
+			//			"X good to do the following 4 steps");
+			//kvm_set_pte(&new_pte, __pte(0));
+			//printk("\t\tforce kvm_tlb_flush_vmid_ipa() - 2\n");
+			//kvm_tlb_flush_vmid_ipa(kvm, fault_ipa);
+			//printk("\t\tforce kvm_tlb_flush_vmid_ipa() - 3\n");
+			//if (!kvm_is_device_pfn(pte_pfn(new_pte))) {
+			//	printk("\t\tforce kvm_tlb_flush_vmid_ipa() - 3 - 2\n");
+			//	kvm_flush_dcache_pte(new_pte);
+			//}
+			//printk("\t\tforce kvm_tlb_flush_vmid_ipa() - 4\n");
+			////put_page(virt_to_page(&new_pte));
+			//kvm_release_pfn_clean(pfn);
+			//printk("\t\tforce kvm_tlb_flush_vmid_ipa() - 5 done %lx\n", hva);
+
+			// way 2 - this works
+			//printk("%s %s(): my mmu_notifier_invalidate_page() - 1 "
+			//		"to test if inv stage2pt works "
+			//		"(This doesn't cause looping...) %lx mm %p\n",
+			//		__FILE__, __func__, hva);
+			//printk("  [dbg] 0x%lx - 3 pte_present %s pte_none %s\n",
+			//			hva, pte_present(*jack_pte) ? "O***" : "X",
+			//			pte_none(*jack_pte) ? "O***" : "X");
+			//mmu_notifier_invalidate_page(current->mm, hva);
+			//printk("  [dbg] 0x%lx - 4 pte_present %s pte_none %s\n",
+			//			hva, pte_present(*jack_pte) ? "O***" : "X",
+			//			pte_none(*jack_pte) ? "O***" : "X");
+			//printk("%s %s(): my mmu_notifier_invalidate_page(mm %p, 0x%lx) "
+			//		"- 2 \n", __FILE__, __func__, current->mm, hva);
+		} else {
+			POPPK_GDSHM("\t\tskip force kvm_tlb_flush_vmid_ipa() - 0 %lx\n", hva);
+		}
+	}
+#endif
 	return ret;
 }
 
@@ -1768,6 +2104,12 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 	hva_t reg_end = hva + mem->memory_size;
 	bool writable = !(mem->flags & KVM_MEM_READONLY);
 	int ret = 0;
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (mem->flags & KVM_MEM_READONLY) {
+		POPPK_GDSHM("%s(): mem->flags & KVM_MEM_READONLY 0x%lx\n",
+				__func__, mem->flags & KVM_MEM_READONLY);
+	}
+#endif
 
 	if (change != KVM_MR_CREATE && change != KVM_MR_MOVE &&
 			change != KVM_MR_FLAGS_ONLY)
@@ -1868,8 +2210,30 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 	 * To prevent incoherency issues in these cases, tag all readonly
 	 * regions as incoherent.
 	 */
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (slot->flags & KVM_MEM_READONLY) {
+		/* flags |= KVM_MEM_READONLY; at ./accel/kvm/kvm-all.c */
+		POPPK_GDSHM("%s(): memslot->userspace_addr 0x%lx ~ 0x%lx "
+			"memslot->base_gfn 0x%llx npages %lu "
+			"slot->flags & KVM_MEM_READONLY 0x%lx (>0) "
+			"(I'm thinking to disable KVM_MEMSLOT_INCOHERENT "
+			"hope we are 1 at KVM_MEM_READONLY now)\n",
+			__func__, slot->userspace_addr,
+			slot->userspace_addr + (slot->npages << PAGE_SHIFT),
+			slot->base_gfn,
+			npages, slot->flags & KVM_MEM_READONLY);
+	}
+#endif
+#if defined(CONFIG_POPCORN_DSHM)
+	POPPK_GDSHM("\n\n\n******************************************\n"
+			"[WARNNING] pophype: force to use !KVM_MEMSLOT_INCOHERENT for accessing host memory every time (=no cache)\n"
+			"*********************************************\n\n\n");
+	if (slot->flags & KVM_MEM_READONLY)
+		slot->flags |= KVM_MEMSLOT_INCOHERENT;
+#else
 	if (slot->flags & KVM_MEM_READONLY)
 		slot->flags |= KVM_MEMSLOT_INCOHERENT;
+#endif
 	return 0;
 }
 
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a35ce7266aac..181407a53e40 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -217,6 +217,13 @@ int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 							 unsigned long address)
 {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	//if (address == 0x7ffd64000000) { // ARM path
+	//	POPPK_GDSHM("%s %s(): do nothing "
+	//			"since ARM doesn't have shadow page tables\n",
+	//			__FILE__, __func__);
+	//}
+#endif
 }
 
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 819b21a9851c..a579d6d0f88a 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -269,6 +269,9 @@ static inline void __kvm_flush_dcache_pud(pud_t pud)
 
 void kvm_set_way_flush(struct kvm_vcpu *vcpu);
 void kvm_toggle_cache(struct kvm_vcpu *vcpu, bool was_enabled);
+#if defined(CONFIG_POPCORN_DSHM)
+void stage2_flush_vm(struct kvm *kvm);
+#endif
 
 static inline bool __kvm_cpu_uses_extended_idmap(void)
 {
diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 5295aef7c8f0..1c1e30761629 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -142,7 +142,7 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_CP14_64]	= kvm_handle_cp14_64,
 	[ESR_ELx_EC_HVC32]	= handle_hvc,
 	[ESR_ELx_EC_SMC32]	= handle_smc,
-	[ESR_ELx_EC_HVC64]	= handle_hvc,
+	[ESR_ELx_EC_HVC64]	= handle_hvc, /* hyper call */
 	[ESR_ELx_EC_SMC64]	= handle_smc,
 	[ESR_ELx_EC_SYS64]	= kvm_handle_sys_reg,
 	[ESR_ELx_EC_IABT_LOW]	= kvm_handle_guest_abort,
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index c2489f62c4fb..c1b08ea23665 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -40,6 +40,12 @@
 
 #include "trace.h"
 
+#include <popcorn/debug.h>
+#include <popcorn/hype_kvm.h>
+#include <linux/irqchip/arm-gic-v3.h> // debug
+#include <linux/delay.h>
+extern struct kvm_vcpu *first_vcpu;
+
 /*
  * All of this file is extremly similar to the ARM coproc.c, but the
  * types are different. My gut feeling is that it should be pretty
@@ -123,6 +129,39 @@ static bool access_gic_sgi(struct kvm_vcpu *vcpu,
 			   struct sys_reg_params *p,
 			   const struct sys_reg_desc *r)
 {
+#if POPHYPE_KVM
+    int vcpu_id = vcpu->vcpu_id;
+	u32 sgi = (p->regval & ICC_SGI1R_SGI_ID_MASK) >> ICC_SGI1R_SGI_ID_SHIFT;
+	/* SGI (inter core interrupt) IPI */
+	KVMPK("[guest]%s %s(): "
+			"p->regval 0x%llx from guest SGI 0x%x from CPU%d to CPU?\n",
+			__FILE__, __func__, p->regval, sgi, vcpu_id);
+	if (sgi == IPI_CROSS_ARCH) {
+//		//KVMPK("%s %s(): HIT return false hope to host userspace\n",
+//		//		__FILE__, __func__);
+//		//return true; // guest
+//		//return false; // userspace
+//		KVMPK("%s %s(): new test injection on arm \"local(1) -> local(0)\" "
+//				"=next> \"local(1) -> remote(0)\"\n",
+//				__FILE__, __func__);
+//
+//		/* local */
+		//KVMPK("%s %s(): don't forwarding for dbg (sleep3s)\n", __FILE__, __func__);
+		//msleep(3000);
+		//vgic_v3_dispatch_sgi(vcpu, 0x7000001); // virt/kvm/arm/vgic-v3-emul.c
+
+		/* remote */
+		KVMPK("[guest]%s %s(): ARM KVM CROSS_IPI forwarding\n", __FILE__, __func__);
+		popcorn_cross_ipi_send(0); // 2node hardcode
+
+		return true; // will call another function: kvm_skip_instr()
+//		KVMPK("%s %s(): ARM KVM vanilla\n", __FILE__, __func__);
+	} else {
+		KVMPK("[WARNNING][guest]%s %s(): "
+				"SGI 0x%x from CPU%d to CPU? (not from mine)\n",
+				__FILE__, __func__, sgi, vcpu_id); // need attention
+	}
+#endif
 	if (!p->is_write)
 		return read_from_write_only(vcpu, p);
 
@@ -170,14 +209,14 @@ static bool trap_dbgauthstatus_el1(struct kvm_vcpu *vcpu,
 /*
  * We want to avoid world-switching all the DBG registers all the
  * time:
- * 
+ *
  * - If we've touched any debug register, it is likely that we're
  *   going to touch more of them. It then makes sense to disable the
  *   traps and start doing the save/restore dance
  * - If debug is active (DBG_MDSCR_KDE or DBG_MDSCR_MDE set), it is
  *   then mandatory to save/restore the registers, as the guest
  *   depends on them.
- * 
+ *
  * For this, we use a DIRTY bit, indicating the guest has modified the
  * debug registers, used as follow:
  *
@@ -1173,7 +1212,14 @@ static int emulate_sys_reg(struct kvm_vcpu *vcpu,
 
 	table = get_target_table(vcpu->arch.target, true, &num);
 
+	/* Pophype: currenty need to return 0 here but how to know this
+		will call access_gic_sgi() */
+	// TODO: check if call access_gic_sgi() -> return 0 and see if userspace can take over
+
 	/* Search target-specific then generic table. */
+#if POPHYPE_KVM
+	/* Pophype: ARM KVM IPI function access_gic_sgi() is in generic table */
+#endif
 	r = find_reg(params, table, num);
 	if (!r)
 		r = find_reg(params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
@@ -1187,11 +1233,32 @@ static int emulate_sys_reg(struct kvm_vcpu *vcpu,
 		 */
 		BUG_ON(!r->access);
 
+#if POPHYPE_KVM
+		/* Pophype: this is the callback -> access_gic_sgi() */
+#endif
 		if (likely(r->access(vcpu, params, r))) {
 			/* Skip instruction, since it was emulated */
 			kvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));
 			return 1;
 		}
+#if POPHYPE_KVM
+		else { // after fail
+			/* Userspace cannot handle this - failed */
+			//KVMPK("%s %s(): since I've asked access_gic_sgi() "
+			//	"to return here, I'm here "
+			//	"(2node case - doesn't consider to its own node)\n",
+			//	__FILE__, __func__);
+			//return KVM_EXIT_CROSS_IPI; // hope this can return to userland...
+
+			//TODO
+			// kh x86 saves target vcpu_struct, apic_struct, forward irq (all #s)
+			//					 vcpu
+			//									vcpu->arch->apic
+			//KVMPK("%s %s(): ARM KVM CROSS_IPI forwarding\n", __FILE__, __func__);
+			KVMPK("%s %s(): NEVER HAPPEN\n", __FILE__, __func__);
+			return 1;
+		}
+#endif
 		/* If access function fails, it should complain. */
 	} else {
 		kvm_err("Unsupported guest sys_reg access at: %lx\n",
@@ -1238,6 +1305,17 @@ int kvm_handle_sys_reg(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 	ret = emulate_sys_reg(vcpu, &params);
 
+#if POPHYPE_KVM
+	/* Userspace cannot handle this - failed */
+	//if (ret == KVM_EXIT_CROSS_IPI) {
+	//	/* Also need to set struct kvm_run *run */
+	//	run->exit_reason = KVM_EXIT_CROSS_IPI;
+	//	ret = 0;
+	//	KVMPK("%s %s(): return to userspace & set exit_reason to "
+	//		"KVM_EXIT_INTERNAL_ERROR\n", __FILE__, __func__);
+	//}
+#endif
+
 	if (!params.is_write)
 		vcpu_set_reg(vcpu, Rt, params.regval);
 	return ret;
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0b419ed725a8..d708294d9e0c 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -211,6 +211,10 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 
 #define ESR_LNX_EXEC		(1 << 24)
 
+#ifdef CONFIG_POPCORN_DSHM
+extern unsigned long dshm_addr;
+extern unsigned long dshm_len;
+#endif
 static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 			   unsigned int mm_flags, unsigned long vm_flags,
 			   struct task_struct *tsk)
@@ -224,7 +228,21 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 	BUG_ON(tsk->is_worker);
 
 	if (distributed_remote_process(tsk)) {
+#ifdef CONFIG_POPCORN_DSHM
+		//POPPK_DSHM("[%d] %s %s(): ARCH CODE addr 0x%lx "
+		//			"dshm_addr 0x%lx dshm_len 0x%lx\n",
+		//	current->pid, __FILE__, __func__, addr, dshm_addr, dshm_len);
+		// many: x86 can handle but arm doesn't
+		if (dshm_addr && dshm_len &&
+			addr >= dshm_addr && addr < (dshm_addr + dshm_len) &&
+			(!vma || vma->vm_start > addr)) {
+			POPPK_DSHM("[%d]: %s %s(): ARCH CODE "
+						"addr 0x%lx hits (dshm 0x%lx ~ 0x%lx) VMAFAULT\n",
+						current->pid, __FILE__, __func__,
+						addr, dshm_addr, dshm_addr + dshm_len);
+#else
 		if (!vma || vma->vm_start > addr) {
+#endif
 			if (vma_server_fetch_vma(tsk, addr) == 0) {
 				/* Replace with updated VMA */
 				vma = find_vma(mm, addr);
diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c
index 6d5b682280a2..69835a0eb9ea 100644
--- a/arch/powerpc/mm/fault.c
+++ b/arch/powerpc/mm/fault.c
@@ -342,6 +342,9 @@ retry:
 
 	if (distributed_remote_process(current)) {
 		if (!vma || vma->vm_start > address) {
+#ifdef CONFIG_POPCORN_DSHM
+			printk("Not support\n");
+#endif
 			if (vma_server_fetch_vma(current, address) == 0) {
 				/* Replace with updated VMA */
 				vma = find_vma(mm, address);
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 6a7cfd9f4a4e..ac8793546cf0 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -336,6 +336,8 @@
 331 64	popcorn_propose_migration	sys_popcorn_propose_migration
 332 64	popcorn_get_thread_status	sys_popcorn_get_thread_status
 333 64	popcorn_get_node_info	sys_popcorn_get_node_info
+334 64	pcn_dshm_mmap	sys_pcn_dshm_mmap
+335 64	pcn_dshm_munmap	sys_pcn_dshm_munmap
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index d09544e826f6..591b24c76b7c 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -63,6 +63,7 @@ static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
 				line_status);
 }
 
+/* !src: from msi */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, unsigned long *dest_map)
 {
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index a1afd80a68aa..3f4c6b239e97 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -42,6 +42,12 @@
 #include "x86.h"
 #include "cpuid.h"
 
+#include <popcorn/debug.h>
+#include <popcorn/hype_kvm.h>
+#include <linux/delay.h>
+extern struct kvm_vcpu *first_vcpu;
+
+
 #ifndef CONFIG_X86_64
 #define mod_64(x, y) ((x) - (y) * div64_u64(x, y))
 #else
@@ -754,10 +760,65 @@ bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 	}
 
 	for_each_set_bit(i, &bitmap, 16) {
+#if POPHYPE_KVM
+		if (!dst[i]) {
+			// debug - verified - true
+//#if 1
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+			if (i == 1 && // TODO: assuming only 2vcpu in total now
+				(irq->vector == CROSS_ARCH_IPI_X86_64_VECTOR ||
+					(irq->vector & 0xff) == CROSS_ARCH_IPI_X86_64_VECTOR)
+				) {
+				KVMPK("%s %s(): NEVER REACHED\n", __FILE__, __func__);
+				KVMPK("%s %s(): (x86) "
+						"send *cross* IPI inject kvm_apic_set_irq() => "
+						"HOWEVEVER, there is no vcpu1 data struct in kernel... "
+						"so fail in silence. SO TODO TODO TODO "
+						"Should not see this sice I've return 0 to host userspace "
+						"0x%x\n",
+						__FILE__, __func__, irq->vector);
+				/* I return before trying to inject */
+			}
+#endif
+			continue;
+		}
+#else
 		if (!dst[i])
 			continue;
+#endif
 		if (*r < 0)
 			*r = 0;
+
+#if POPHYPE_KVM
+#ifdef CONFIG_POPCORN_CHECK_SANITY // dbg
+		if (irq->vector == CROSS_ARCH_IPI_X86_64_VECTOR) {
+			KVMPK("\t ==vect, i %d\n", i); // dbg
+		}
+		if (i == 0 && // TODO: assuming only 2vcpu in total now
+			irq->vector == CROSS_ARCH_IPI_X86_64_VECTOR) {
+			KVMPK("%s %s(): new test injection on x86 \"local(1) -> local(0)\" "
+					"=next> \"local(1) -> remote(0)\"\n",
+					__FILE__, __func__);
+		}
+#endif
+
+		if (irq->vector == CROSS_ARCH_IPI_X86_64_VECTOR ||
+			(irq->vector & 0xff) == CROSS_ARCH_IPI_X86_64_VECTOR) {
+			/* We are at kvm_irq_delivery_to_apic_fast()
+				from kvm_irq_delivery_to_apic() at ./arch/x86/kvm/irq_comm.c
+				IPI inject
+				(local and remote inject will both end up being here in x86 cases) */
+			KVMPK("%s %s(): (x86) send *cross* IPI inject kvm_apic_set_irq() => "
+					"or this is a re-injection\n",
+					__FILE__, __func__);
+			// stop here
+		}
+#ifdef CONFIG_POPCORN_CHECK_SANITY // dbg
+		if (irq->vector == CROSS_ARCH_IPI_X86_64_VECTOR) {
+			KVMPK("\t inject ipi to <%d>\n", i); // dbg
+		}
+#endif
+#endif
 		*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
 	}
 out:
@@ -1528,8 +1589,9 @@ void kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);
 
+
 /* emulate APIC access in a trap manner */
-void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
+int kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 {
 	u32 val = 0;
 
@@ -1538,8 +1600,88 @@ void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)
 
 	apic_reg_read(vcpu->arch.apic, offset, 4, &val);
 
+#if POPHYPE_KVM
+	if (offset == APIC_ICR) { // APIC_ICR reg = IPI
+		/* from apic_send_ipi()
+			that is the place to reinject IPIs to target vCPU */
+		u32 icr_low = kvm_apic_get_reg(vcpu->arch.apic, APIC_ICR);
+		u32 vector = icr_low & APIC_VECTOR_MASK;
+		if (vector == CROSS_ARCH_IPI_X86_64_VECTOR) {
+			KVMPK("%s %s(): (x86) got kvm from gest user. "
+					"0x%x (vector) "
+					"(2node case - doesn't consider to its own node)\n",
+					__FILE__, __func__, vector);
+
+			/* Userspace cannot handle this - failed */
+			//vcpu->run->exit_reason = KVM_EXIT_CROSS_IPI;
+			//return 0; // return to host userspace
+
+			// TODO
+			/* pophype: currently vcpu = 0 (target) dest_id = 0 (target) */
+			/* I think the injected target is determined by
+					```map = rcu_dereference(kvm->arch.apic_map);
+					dst = &map->phys_map[irq->dest_id];```
+				so irq.dest_id is the key !! */
+			/* Dest is determined by vcpu (current thought) */
+            // kh x86 saves target vcpu_struct, apic_struct, forward irq (all #s)
+            //                   vcpu
+            //                                  vcpu->arch->apic
+            struct kvm_lapic *apic = vcpu->arch.apic;
+            struct kvm_lapic_irq irq = {
+                .vector = vector, // = CROSS_ARCH_IPI_X86_64_VECTOR
+                .delivery_mode = APIC_DM_FIXED, // kh: // many <0> doing APIC_ICR and <1>(remote) receives this
+                .dest_mode = APIC_DEST_PHYSICAL,
+                .level = 1, // guess
+                //.trig_mode = 0, // guess
+                .trig_mode = 1, // guess
+                //.shorthand = 0x0, // guess APIC_DEST_PHYSICAL 0x0
+                .shorthand = APIC_DEST_SELF, // guess APIC_DEST_PHYSICAL 0x0
+                .dest_id = 0, // pophyp hardcode for 2node case
+                .msi_redir_hint = false
+                // u32 vector; u16 delivery_mode; u16 dest_mode; bool level;
+                // u16 trig_mode; u32 shorthand; u32 dest_id; bool msi_redir_hint;
+            };
+			int r = 0;
+            // u32 icr_low = kvm_apic_get_reg(apic, APIC_ICR);
+            // irq.vector = icr_low & APIC_VECTOR_MASK;
+            //KVMPK("\t\t%s(): replay irq injection (("
+            //      "src %p apic->vcpu->vcpu_id(lapic) <%d> "
+            //      "vector %u delivery_mode %u "
+            //      "dest_mode %u level %d trig_mode %u "
+            //      "shorthand %u dest_id <%u> msi_redir_hint %d\n",
+            //      __func__, apic, apic->vcpu->vcpu_id, irq->vector,
+            //      irq->delivery_mode,
+            //      irq->dest_mode, irq->level, irq->trig_mode,
+            //      irq->shorthand, irq->dest_id, irq->msi_redir_hint);
+
+//#ifdef CONFIG_POPCORN_CHECK_SANITY
+//			bool ret = false;
+//			ret = kvm_irq_delivery_to_apic_fast(apic->vcpu->kvm, apic,
+//												&irq, &r, NULL);
+//				//struct kvm *kvm, struct kvm_lapic *src,
+//				//struct kvm_lapic_irq *irq, int *r, unsigned long *dest_map)
+//			if (!ret) {
+//				printk("ERROR\n");
+//			}
+//			return 1;
+//#else
+			/* local */
+			//KVMPK("%s %s(): don't forwarding for dbg (sleep3s)\n", __FILE__, __func__);
+			//msleep(3000);
+			//kvm_irq_delivery_to_apic_fast(apic->vcpu->kvm, apic, &irq, &r, NULL);
+			//return r; // error: -1 good: 1 unknow: 0
+
+			/* remote */
+			KVMPK("%s %s(): X86 KVM CROSS_IPI forwarding\n", __FILE__, __func__);
+			popcorn_cross_ipi_send(1); // 2node hardcode
+			return 1; // error: -1 good: 1 unknow: 0
+//#endif
+		}
+	}
+#endif
 	/* TODO: optimize to just emulate side effect w/o one more write */
 	apic_reg_write(vcpu->arch.apic, offset, val);
+	return 1; // return to guestVM
 }
 EXPORT_SYMBOL_GPL(kvm_apic_write_nodecode);
 
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index eb418fd670ff..9f2ebb87d7b8 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -75,7 +75,7 @@ int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu);
 u64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu);
 void kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data);
 
-void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset);
+int kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset);
 void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector);
 
 int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr);
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index a750fc7c7458..78d375e76fbf 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -57,6 +57,8 @@
 #define __ex_clear(x, reg) \
 	____kvm_handle_fault_on_reboot(x, "xor " reg " , " reg)
 
+#include <popcorn/debug.h>
+
 MODULE_AUTHOR("Qumranet");
 MODULE_LICENSE("GPL");
 
@@ -5848,9 +5850,16 @@ static int handle_apic_write(struct kvm_vcpu *vcpu)
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 	u32 offset = exit_qualification & 0xfff;
 
+#if POPHYPE_KVM
+	/* pophype: we want to return 0 back to userspace
+		TODO: Since only one place calling kvm_apic_write_nodecode(),
+			=> I will change it to return a value so that I can return 1/0 here */
+	return kvm_apic_write_nodecode(vcpu, offset);
+#else
 	/* APIC-write VM exit is trap-like and thus no need to adjust IP */
 	kvm_apic_write_nodecode(vcpu, offset);
 	return 1;
+#endif
 }
 
 static int handle_task_switch(struct kvm_vcpu *vcpu)
@@ -8103,6 +8112,9 @@ static void dump_vmcs(void)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+#if POPHYPE_KVM
+/* From arch/x86/kvm/x86.c vcpu_enter_guest() */
+#endif
 static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -8110,6 +8122,18 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 	u32 vectoring_info = vmx->idt_vectoring_info;
 
 	trace_kvm_exit(exit_reason, vcpu, KVM_ISA_VMX);
+#if POPHYPE_KVM
+    if (0) { // many
+        if (exit_reason == EXIT_REASON_APIC_WRITE) {
+            static unsigned long cnt = 0;
+            cnt++;
+            if (cnt < 1000 || !(cnt % 1000)) {
+                KVMPK("%s %s(): EXIT_REASON_APIC_WRITE "
+                        "handle_apic_write() #%lu\n", __FILE__, __func__, cnt);
+            }
+        }
+    }
+#endif
 
 	/*
 	 * Flush logged GPAs PML buffer, this will make dirty_bitmap more
@@ -9765,7 +9789,7 @@ static void prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)
 	/* vmcs12's VM_ENTRY_LOAD_IA32_EFER and VM_ENTRY_IA32E_MODE are
 	 * emulated by vmx_set_efer(), below.
 	 */
-	vm_entry_controls_init(vmx, 
+	vm_entry_controls_init(vmx,
 		(vmcs12->vm_entry_controls & ~VM_ENTRY_LOAD_IA32_EFER &
 			~VM_ENTRY_IA32E_MODE) |
 		(vmcs_config.vmentry_ctrl & ~VM_ENTRY_IA32E_MODE));
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 9cea09597d66..4b1ca124cd54 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -6659,6 +6659,7 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.apic_attention)
 		kvm_lapic_sync_from_vapic(vcpu);
 
+	/* pophype */
 	r = kvm_x86_ops->handle_exit(vcpu);
 	return r;
 
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index fa565913ab65..62bdfdebcb9c 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -184,7 +184,7 @@ force_sig_info_fault(int si_signo, int si_code, unsigned long address,
 	info.si_code	= si_code;
 	info.si_addr	= (void __user *)address;
 	if (fault & VM_FAULT_HWPOISON_LARGE)
-		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault)); 
+		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault));
 	if (fault & VM_FAULT_HWPOISON)
 		lsb = PAGE_SHIFT;
 	info.si_addr_lsb = lsb;
@@ -446,7 +446,7 @@ NOKPROBE_SYMBOL(vmalloc_fault);
 
 #ifdef CONFIG_CPU_SUP_AMD
 static const char errata93_warning[] =
-KERN_ERR 
+KERN_ERR
 "******* Your BIOS seems to not contain a fix for K8 errata #93\n"
 "******* Working around it, but it may cause SEGVs or burn power.\n"
 "******* Please consider a BIOS update.\n"
@@ -1057,6 +1057,10 @@ static inline bool smap_violation(int error_code, struct pt_regs *regs)
 	return true;
 }
 
+#ifdef CONFIG_POPCORN_DSHM
+extern unsigned long dshm_addr;
+extern unsigned long dshm_len;
+#endif
 /*
  * This routine handles page faults.  It determines the address,
  * and the problem, and then passes it off to one of the appropriate
@@ -1209,7 +1213,24 @@ retry:
 	BUG_ON(tsk->is_worker);
 
 	if (distributed_remote_process(tsk)) {
+#ifdef CONFIG_POPCORN_DSHM
+        POPPK_DSHM("[%d] %s %s(): ARCH CODE "
+				"address 0x%lx dshm_addr 0x%lx dshm_len 0x%lx\n",
+				current->pid, __FILE__, __func__, address, dshm_addr, dshm_len);
+        POPPK_DSHM("[%d][arch][attention] %s %s(): "
+				"interested region, we shouldn't vma fault. "
+				"address 0x%lx dshm_addr 0x%lx dshm_len 0x%lx\n",
+				current->pid, __FILE__, __func__, address, dshm_addr, dshm_len);
+		if (dshm_addr && dshm_len &&
+			address >= dshm_addr && address < (dshm_addr + dshm_len) &&
+			(!vma || vma->vm_start > address)) {
+			POPPK_DSHM("[%d]: %s %s(): ARCH CODE "
+						"addr 0x%lx hits (dshm 0x%lx ~ 0x%lx) VMAFAULT\n",
+						current->pid, __FILE__, __func__,
+						address, dshm_addr, dshm_addr + dshm_len);
+#else
 		if (!vma || vma->vm_start > address) {
+#endif
 			if (vma_server_fetch_vma(tsk, address) == 0) {
 				/* Replace with updated VMA */
 				vma = find_vma(mm, address);
diff --git a/build.sh b/build.sh
new file mode 100755
index 000000000000..591549bc9f7b
--- /dev/null
+++ b/build.sh
@@ -0,0 +1,71 @@
+#!/bin/bash
+
+KERNEL_PATH=~/kh
+CORES="96"
+echo -e "\n\nTHIS FILE IS FROM MIR* OR MIR\n\n"
+echo "0: skip \$make"
+echo "Input counts: $#"
+echo "First arg: $1"
+ret=0
+if [ "$1" = "0" ];
+then
+    echo "skiped \$make"
+else
+    echo -e "\n\n\n\n\n\nmake -j$CORES\n\n\n\n\n\n"
+	make -j$CORES
+	ret=$(($ret+$?))
+fi
+
+echo "restore #define POPHYPE_HOST_KERNEL 1 in $KERNEL_PATH/include/popcorn/debug.h"
+sed -i 's/#define POPHYPE_HOST_KERNEL 0/#define POPHYPE_HOST_KERNEL 1/g' $KERNEL_PATH/include/popcorn/debug.h
+
+echo -e "\n\n\n\n\n\nmake -j$CORES -C msg_layer\n\n\n\n\n\n"
+make -j$CORES -C msg_layer
+ret=$(($ret+$?))
+echo "ret=$ret"
+if [[ $ret != 0 ]]; then
+	echo "ret=$ret not zero exit"
+	exit -1
+fi
+
+echo -e "\n\n\n\n\n\nmake modules -j$CORES\n\n\n\n\n\n"
+make modules -j$CORES
+ret=$(($ret+$?))
+echo "ret=$ret"
+echo -e "\n\n\n\n\n\nsudo make modules_install -j$CORES\n\n\n\n\n"
+sudo make modules_install -j$CORES
+ret=$(($ret+$?))
+echo "ret=$ret"
+echo -e "\n\n\n\n\nsudo make install -j$CORES\n\n\n\n\n"
+sudo make install -j$CORES
+ret=$(($ret+$?))
+echo "ret=$ret"
+
+echo -e "\n\n\n\n\n\nmake modules -j$CORES (for arm's ib modules)\n\n\n\n\n\n"
+make modules -j$CORES
+ret=$(($ret+$?))
+echo "ret=$ret"
+exit $ret
+
+#echo "\n\n\n\n\n\n"
+#sudo make modules_install -j$CORES -C /home/jackchuang/share/popcorn-rack
+
+#echo "\n\n\n\n\n\n"
+#sudo make install -C /home/jackchuang/share/popcorn-rack
+#sudo grub-set-default 1
+
+#ssh echo5 "make -C /mnt/popcorn-rack -j99"
+#ssh echo5 "sudo make modules_install -j99"
+#for i in {5..6..1}
+#do
+#	ssh echo$i "sudo make install -C /mnt/popcorn-rack"
+#	ssh echo$i "sudo reboot"
+#
+#    for j in {1..10..1}
+#    do
+#        echo "done on echo$i"
+#    done
+#
+#	sleep 5
+#	sudo ipmitool -I lanplus -H ipmi$i -U ADMIN -P ADMIN chassis power cycle
+#done
diff --git a/fs/read_write.c b/fs/read_write.c
index 3f9c8a038138..4b6ac13c64b5 100644
--- a/fs/read_write.c
+++ b/fs/read_write.c
@@ -569,8 +569,9 @@ SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)
 	ssize_t ret = -EBADF;
 
 #ifdef CONFIG_POPCORN_CHECK_SANITY
-	if (WARN_ON(distributed_remote_process(current))) {
-		printk("  file read at remote thread is not supported yet\n");
+	if (WARN_ON_ONCE(distributed_remote_process(current))) {
+		POPPK_DSHMV("  file read at remote thread is not supported yet. fd %u "
+					"(f.file %p)\n", fd,  f.file);
 	}
 #endif
 
@@ -591,8 +592,9 @@ SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf,
 	ssize_t ret = -EBADF;
 
 #ifdef CONFIG_POPCORN_CHECK_SANITY
-	if (WARN_ON(distributed_remote_process(current))) {
-		printk("  file write at remote thread is not supported yet\n");
+	if (WARN_ON_ONCE(distributed_remote_process(current))) {
+		POPPK_DSHMV("  file write at remote thread is not supported yet. fd %u "
+					"(f.file %p)\n", fd, f.file);
 	}
 #endif
 
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d7ce4e3280db..8e657bbaadd9 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -563,6 +563,9 @@ enum kvm_mr_change {
 	KVM_MR_FLAGS_ONLY,
 };
 
+#if defined(CONFIG_POPCORN_DSHM)
+void kvm_mmu_notifier_release(struct mmu_notifier *mn, struct mm_struct *mm);
+#endif
 int kvm_set_memory_region(struct kvm *kvm,
 			  const struct kvm_userspace_memory_region *mem);
 int __kvm_set_memory_region(struct kvm *kvm,
@@ -667,6 +670,16 @@ void kvm_load_guest_fpu(struct kvm_vcpu *vcpu);
 void kvm_put_guest_fpu(struct kvm_vcpu *vcpu);
 
 void kvm_flush_remote_tlbs(struct kvm *kvm);
+#ifdef CONFIG_POPCORN_DSHM
+#if defined(__aarch64__)
+void mimic_abort_behav(gpa_t gpa);
+pte_t* gpa_to_guest_pte(gpa_t gpa);
+#endif
+void kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
+void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size);
+void kvm_flush_dcache_pte(pte_t pte);
+void coherent_cache_guest_page(struct kvm_vcpu *vcpu, pfn_t pfn, unsigned long size, bool uncached);
+#endif
 void kvm_reload_remote_mmus(struct kvm *kvm);
 void kvm_make_mclock_inprogress_request(struct kvm *kvm);
 void kvm_make_scan_ioapic_request(struct kvm *kvm);
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 5fb63fabc903..9ad135b7a356 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1845,11 +1845,13 @@ struct task_struct {
 		pid_t origin_pid;
 	};
 
-	bool is_worker;			/* kernel thread that manages the process*/
+	bool is_worker;			/* kernel thread that manages the process */
 	bool at_remote;			/* Is executing on behalf of another node? */
 
+#ifndef CONFIG_POPCORN_DSHM
 	volatile void *remote_work;
 	struct completion remote_work_pended;
+#endif
 
 	int migration_target_nid;
 	int backoff_weight;
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 163ced4dc0f1..d084dff04a57 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -895,5 +895,8 @@ asmlinkage long sys_popcorn_migrate(int nid, void __user *uregs);
 asmlinkage long sys_popcorn_propose_migration(pid_t pid, int nid);
 asmlinkage long sys_popcorn_get_thread_status(struct popcorn_thread_status __user *status);
 asmlinkage long sys_popcorn_get_node_info(int * __user _my_nid, struct popcorn_node_info __user *info);
-
+asmlinkage long sys_pcn_dshm_mmap(unsigned long addr, unsigned long len,
+			unsigned long prot, unsigned long flags,
+			unsigned long fd, unsigned long pgoff);
+asmlinkage long sys_pcn_dshm_munmap(unsigned long addr, size_t len);
 #endif
diff --git a/include/popcorn/debug.h b/include/popcorn/debug.h
index 857b9bf13b5f..c360c05fa78d 100644
--- a/include/popcorn/debug.h
+++ b/include/popcorn/debug.h
@@ -4,6 +4,59 @@
 #define PCNPRINTK(...) printk(KERN_INFO "popcorn: " __VA_ARGS__)
 #define PCNPRINTK_ERR(...) printk(KERN_ERR "popcorn: " __VA_ARGS__)
 
+#define PERF_EXP 1 /* Get performance #s */
+#define POPHYPE_KVM 1 /* pophype kvm support */
+#define POPHYPE_ARCH_2NDTLB_FLUSH 0 /* x86 and arm flush at different places */
+
+
+#if PERF_EXP
+#define POP_CLEAN 1
+#define POP_DBG_DSHM 0
+#define POP_DBG_SHMEM 0
+#else
+#define POP_CLEAN 0
+#define POP_DBG_DSHM 1
+#define POP_DBG_SHMEM 1
+#endif
+
+#if POP_CLEAN
+#define POPPK_DSHM(...) ;
+#define POPPK_GDSHM(...) ; // printk(KERN_INFO "[dshm]" __VA_ARGS__);
+#define POPPK_GDSHMV(...) ;
+#define POPPK_SHMEM(...) ;
+#define POPPK_DSHMV(...) ;
+#define POPPK_DSHMVV(...) ;
+#define POPPK_MMAP(...) ; // special case for POPPK_DSHMVV
+#define KVMPK(...) ;
+#define KVMPKV(...) ;
+#else
+#define POPPK_DSHM(...) printk(KERN_INFO "[dshm]" __VA_ARGS__); /* anon SHM (host) */
+#define POPPK_GDSHM(...) ;
+//#define POPPK_GDSHM(...) printk(KERN_INFO "[dshm]" __VA_ARGS__); /* DSHM for guest/caused by guest*/
+#define POPPK_GDSHMV(...) ;
+#define POPPK_DSHMV(...) ; // anon SHM verbose
+//#define POPPK_DSHMV(...) printk(KERN_INFO "[dshm]" __VA_ARGS__); // anon SHM verbose
+//#define POPPK_SHMEM(...) printk(KERN_INFO "[shmem]" __VA_ARGS__); // File-backed SHM
+#define POPPK_DSHMVV(...) ; // happends occasionally
+#define POPPK_MMAP(...) ; // special case for POPPK_DSHMVV
+#define POPPK_SHMEM(...) ;
+#define KVMPK(...) ;
+//#define KVMPK(...) printk(KERN_INFO "[kvm]" __VA_ARGS__); // casued by kvm (especially interrupts like apic and gic, like IPI forwarding)
+#define KVMPKV(...) ;
+#endif
+
+
+#if POPHYPE_KVM
+/* CROSS IPI - Match guest # */
+#define CROSS_ARCH_IPI_X86_64_VECTOR 0xdd /* X86 */
+#define IPI_CROSS_ARCH 7 /* ARM =7 (emu) */
+#endif
+
+
+// TODO INTERESTING_SHM_PID
+// TODO INTERESTING_FIRST_SHM_CNT
+#define DEBUG_SKIP_SHM_CNT 100
+
 /*
  * Function macros
  */
diff --git a/include/popcorn/hype_kvm.h b/include/popcorn/hype_kvm.h
new file mode 100644
index 000000000000..7b2d2d09c630
--- /dev/null
+++ b/include/popcorn/hype_kvm.h
@@ -0,0 +1,17 @@
+/*
+ * hype_kvm.h
+ * Copyright (C) 2022 jackchuang <jackchuang@mir>
+ *
+ * Distributed under terms of the MIT license.
+ */
+
+#ifndef HYPE_KVM_H
+#define HYPE_KVM_H
+
+int popcorn_cross_ipi_send(int dst_nid);
+//int popcorn_send_ipi(struct kvm_vcpu *dst_vcpu, struct kvm_lapic_irq *irq, unsigned long *dest_map);
+//#if defined(CONFIG_X86_64)
+//#elif defined(CONFIG_ARM64)
+//#endif
+
+#endif /* !HYPE_KVM_H */
diff --git a/include/popcorn/pcn_kmsg.h b/include/popcorn/pcn_kmsg.h
index a00cd2f9a1bd..8cb5ed17cd67 100644
--- a/include/popcorn/pcn_kmsg.h
+++ b/include/popcorn/pcn_kmsg.h
@@ -10,6 +10,20 @@
 
 #include <linux/types.h>
 
+#define VM_TESTING 1
+#define MULTI_CONN_PER_NODE 0
+
+#if VM_TESTING
+#define X86_THREADS 8
+#define ARM_THREADS 8
+#else
+#define X86_THREADS 16
+#define ARM_THREADS 96
+//#define X86_THREADS 24
+//#define ARM_THREADS 144
+#endif
+#define MAX_POPCORN_THREADS ARM_THREADS
+
 /* Enumerate message types */
 enum pcn_kmsg_type {
 	/* Thread migration */
@@ -47,6 +61,8 @@ enum pcn_kmsg_type {
 	PCN_KMSG_TYPE_TEST_RESPONSE,
 	PCN_KMSG_TYPE_TEST_RDMA_REQUEST,
 	PCN_KMSG_TYPE_TEST_RDMA_RESPONSE,
+	PCN_KMSG_TYPE_TEST_RDMA_DSMRR_REQUEST,
+	PCN_KMSG_TYPE_TEST_RDMA_DSMRR_RESPONSE,
 
 	/* Provide the single system image */
 	PCN_KMSG_TYPE_REMOTE_PROC_CPUINFO_REQUEST,
@@ -56,6 +72,23 @@ enum pcn_kmsg_type {
 	PCN_KMSG_TYPE_REMOTE_PROC_PS_REQUEST,
 	PCN_KMSG_TYPE_REMOTE_PROC_PS_RESPONSE,
 
+#ifdef CONFIG_POPCORN_DSHM
+	/* Popcorn DSHM */
+	PCN_KMSG_TYPE_DSHM_JOIN_REQUEST,
+	PCN_KMSG_TYPE_DSHM_JOIN_RESPONSE,
+	PCN_KMSG_TYPE_DSHM_JOIN_UPDATE_REQUEST,
+	PCN_KMSG_TYPE_DSHM_JOIN_UPDATE_RESPONSE,
+	PCN_KMSG_TYPE_UPDATE_REMOTE_WORKER_PID,
+	PCN_KMSG_TYPE_UPDATE_ORIGIN_WORKER_PID,
+	PCN_KMSG_TYPE_GLOBAL_BARRIER_REQUEST,
+	PCN_KMSG_TYPE_GLOBAL_BARRIER_RESPONSE,
+	PCN_KMSG_TYPE_STOP_WORKER,
+
+	/* KVM IPI forwarding */
+	PCN_KMSG_TYPE_IPI_REQUEST,
+	PCN_KMSG_TYPE_IPI_RESPONSE,
+#endif
+
 	/* Schedule server */
 	PCN_KMSG_TYPE_SCHED_PERIODIC,		/* XXX sched requires help!! */
 
@@ -71,9 +104,12 @@ enum pcn_kmsg_prio {
 
 /* Message header */
 struct pcn_kmsg_hdr {
-	int from_nid			:6;
+	int from_nid			:6;	///* max node = 15 */
 	enum pcn_kmsg_prio prio	:2;
-	enum pcn_kmsg_type type	:8;
+	enum pcn_kmsg_type type	:8; ///* max type = 127 */
+#if MULTI_CONN_PER_NODE
+	unsigned int channel;		///* max node = */
+#endif
 	size_t size;
 } __attribute__((packed));
 
@@ -81,7 +117,7 @@ struct pcn_kmsg_hdr {
 	(((struct pcn_kmsg_message *)x)->header.from_nid)
 #define PCN_KMSG_SIZE(x) (sizeof(struct pcn_kmsg_hdr) + x)
 
-#define PCN_KMSG_MAX_SIZE (64UL << 10)
+#define PCN_KMSG_MAX_SIZE (32UL << 10)
 #define PCN_KMSG_MAX_PAYLOAD_SIZE \
 	(PCN_KMSG_MAX_SIZE - sizeof(struct pcn_kmsg_hdr))
 
diff --git a/include/popcorn/process_server.h b/include/popcorn/process_server.h
index 2f2d3ca46993..6f71561f5b0b 100644
--- a/include/popcorn/process_server.h
+++ b/include/popcorn/process_server.h
@@ -1,6 +1,36 @@
 #ifndef __POPCORN_PROCESS_SERVER_H
 #define __POPCORN_PROCESS_SERVER_H
 
+
+#define __lock_remote_contexts(index) \
+    spin_lock(remote_contexts_lock + index)
+#define __lock_remote_contexts_in(nid) \
+    __lock_remote_contexts(INDEX_INBOUND)
+#define __lock_remote_contexts_out(nid) \
+    __lock_remote_contexts(INDEX_OUTBOUND)
+
+#define __unlock_remote_contexts(index) \
+    spin_unlock(remote_contexts_lock + index)
+#define __unlock_remote_contexts_in(nid) \
+    __unlock_remote_contexts(INDEX_INBOUND)
+#define __unlock_remote_contexts_out(nid) \
+    __unlock_remote_contexts(INDEX_OUTBOUND)
+
+#define __remote_contexts_in() remote_contexts[INDEX_INBOUND]
+#define __remote_contexts_out() remote_contexts[INDEX_OUTBOUND]
+
+
+extern struct list_head remote_contexts[2];
+extern spinlock_t remote_contexts_lock[2];
+
+enum {
+    INDEX_OUTBOUND = 0,
+    INDEX_INBOUND = 1,
+};
+
+struct remote_context *__lookup_remote_contexts_in(int nid, int tgid);
+
+
 struct task_struct;
 
 int process_server_do_migration(struct task_struct* tsk, unsigned int dst_nid, void __user *uregs);
@@ -14,4 +44,14 @@ long process_server_do_futex_at_remote(u32 __user *uaddr, int op, u32 val,
 struct remote_context;
 void free_remote_context(struct remote_context *);
 
+struct remote_context *alloc_remote_context(int nid, int tgid, bool remote);
+
+struct remote_context *get_task_remote(struct task_struct *tsk);
+
+void process_remote_works(struct task_struct *tsk);
+#ifdef CONFIG_POPCORN_DSHM
+void pcn_dshm_join(pid_t ttid, unsigned long addr, unsigned long len);
+int popcorn_distributed_barrier(unsigned long addr, unsigned long len);
+#endif
+
 #endif /* __POPCORN_PROCESS_SERVER_H */
diff --git a/include/popcorn/sync.h b/include/popcorn/sync.h
new file mode 100644
index 000000000000..8b9a60be98ae
--- /dev/null
+++ b/include/popcorn/sync.h
@@ -0,0 +1,42 @@
+/*
+ * sync.h
+ * Copyright (C) 2018 Ho-Ren(Jack) Chuang <horenc@vt.edu>
+ *
+ * Distributed under terms of the MIT license.
+ */
+
+#ifndef SYNC_H
+#define SYNC_H
+#include <popcorn/pcn_kmsg.h>
+
+#define MAX_OMP_REGIONS 100
+
+/* Depends on (PCN_KMSG_MAX_SIZE - 1) pages for msg head + metadata */
+#define LIMIT_PER_INV_ADDR_SIZE_FACTOR (PCN_KMSG_MAX_SIZE / (32UL << 10)) // for seting inv cnt as the same as when size = 32k
+#define MAX_WRITE_INV_BUFFERS ((long unsigned int)((PCN_KMSG_MAX_PAYLOAD_SIZE / LIMIT_PER_INV_ADDR_SIZE_FACTOR) / sizeof(unsigned long)) - 8) // (-8) since page_merge_request_t has 8 element each has 8 bytes (worst case)
+
+/* IS-D, BT-D use more: 2000 => 2000 * 96(max threads)
+ * For not IS-D: 1500
+ * 2500 will crash since cannot allocate sys_region
+ */
+#define MAX_READ_BUFFERS 2000
+#define MAX_WRITE_NOPAGE_BUFFERS 2000
+
+#define MAX_PF_MSG (ARM_THREADS * 10 * 2) // = (1000msg * 31pg per msg) pages is enough except sp (2000)
+
+/* 1 end spot for sorting */
+#define MAX_ALIVE_THREADS (X86_THREADS + ARM_THREADS + 1)
+/*
+#if VM_TESTING
+#define MAX_ALIVE_THREADS (16 + 1)
+#else
+//#define MAX_ALIVE_THREADS (112 + 1)
+//#define MAX_ALIVE_THREADS (168 + 1)
+#define MAX_ALIVE_THREADS (X86_THREADS + ARM_THREADS + 1)
+// try X86_THREADS + ARM_THREADS
+#endif
+*/
+
+
+
+#endif /* !SYNC_H */
diff --git a/include/popcorn/types.h b/include/popcorn/types.h
index 48ccb12578da..642409999704 100644
--- a/include/popcorn/types.h
+++ b/include/popcorn/types.h
@@ -12,6 +12,47 @@
 
 #include <linux/sched.h>
 
+#include <popcorn/bundle.h>
+
+#define FAULTS_HASH 31
+
+/**
+ * Remote execution context
+ */
+struct remote_context {
+    struct list_head list;
+    atomic_t count;
+    struct mm_struct *mm;
+
+    int tgid;
+    bool for_remote;
+
+    /* Tracking page status */
+    struct radix_tree_root pages;
+
+    /* For page replication protocol */
+    spinlock_t faults_lock[FAULTS_HASH];
+    struct hlist_head faults[FAULTS_HASH];
+
+    /* For VMA management */
+    spinlock_t vmas_lock;
+    struct list_head vmas;
+
+    /* Remote worker */
+    bool stop_remote_worker;
+
+    struct task_struct *remote_worker;
+    struct completion remote_works_ready; // retmote worker
+#ifdef CONFIG_POPCORN_DSHM
+	volatile void *remote_work; // origin worker's work
+    struct completion remote_work_pended; // origin worker
+#endif
+    spinlock_t remote_works_lock;
+    struct list_head remote_works;
+
+    pid_t remote_tgids[MAX_POPCORN_NODES];
+};
+
 static inline bool distributed_process(struct task_struct *tsk)
 {
 	if (!tsk->mm) return false;
diff --git a/include/uapi/asm-generic/unistd.h b/include/uapi/asm-generic/unistd.h
index 921efb9419c4..ca842f925afd 100644
--- a/include/uapi/asm-generic/unistd.h
+++ b/include/uapi/asm-generic/unistd.h
@@ -723,9 +723,13 @@ __SYSCALL(__NR_popcorn_propose_migration, sys_popcorn_propose_migration)
 __SYSCALL(__NR_popcorn_get_thread_status, sys_popcorn_get_thread_status)
 #define __NR_popcorn_get_node_info 288
 __SYSCALL(__NR_popcorn_get_node_info, sys_popcorn_get_node_info)
+#define __NR_pcn_dshm_mmap 289
+__SYSCALL(__NR_pcn_dshm_mmap, sys_pcn_dshm_mmap)
+#define __NR_pcn_dshm_munmap 290
+__SYSCALL(__NR_pcn_dshm_munmap, sys_pcn_dshm_munmap)
 
 #undef __NR_syscalls
-#define __NR_syscalls 289
+#define __NR_syscalls 291
 
 /*
  * All syscalls below here should go away really,
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index 376d0ab5b9f2..ab4de53e2fed 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -184,6 +184,7 @@ struct kvm_s390_skeys {
 #define KVM_EXIT_SYSTEM_EVENT     24
 #define KVM_EXIT_S390_STSI        25
 #define KVM_EXIT_IOAPIC_EOI       26
+#define KVM_EXIT_CROSS_IPI		  100
 
 /* For KVM_EXIT_INTERNAL_ERROR */
 /* Emulate instruction failed. */
diff --git a/ipc/shm.c b/ipc/shm.c
index 32974cfe5947..b0bd0593b3bf 100644
--- a/ipc/shm.c
+++ b/ipc/shm.c
@@ -47,6 +47,10 @@
 
 #include "util.h"
 
+#if defined(CONFIG_POPCORN_DSHM)
+#include <popcorn/debug.h>
+#endif
+
 struct shm_file_data {
 	int id;
 	struct ipc_namespace *ns;
@@ -193,6 +197,14 @@ static int __shm_open(struct vm_area_struct *vma)
 	struct shm_file_data *sfd = shm_file_data(file);
 	struct shmid_kernel *shp;
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	POPPK_SHMEM("[sysv][%d] %s(): %lx - %lx\n",
+				current->pid, __func__,
+				vma ? vma->vm_start : 0,
+				vma ? vma->vm_end : 0)
+	//dump_stack();
+#endif
+
 	shp = shm_lock(sfd->ns, sfd->id);
 
 	if (IS_ERR(shp))
@@ -419,6 +431,14 @@ static int shm_mmap(struct file *file, struct vm_area_struct *vma)
 	struct shm_file_data *sfd = shm_file_data(file);
 	int ret;
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	POPPK_SHMEM("[sysv][%d] %s(): %lx - %lx\n",
+				current->pid, __func__,
+				vma ? vma->vm_start : 0,
+				vma ? vma->vm_end : 0)
+	//dump_stack();
+#endif
+
 	/*
 	 * In case of remap_file_pages() emulation, the file can represent an
 	 * IPC ID that was removed, and possibly even reused by another shm
@@ -535,6 +555,12 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)
 	int id;
 	vm_flags_t acctflag = 0;
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	POPPK_SHMEM("[sysv][%d] %s(): creats struct shmid_kernel *shp "
+				"calls shmem_kernel_file_setup() "
+				"to create a file\n",
+				current->pid, __func__);
+#endif
 	if (size < SHMMIN || size > ns->shm_ctlmax)
 		return -EINVAL;
 
@@ -549,6 +575,10 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)
 	if (!shp)
 		return -ENOMEM;
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	POPPK_SHMEM("[sysv][%d] %s(): creats struct shmid_kernel *shp %p\n",
+				current->pid, __func__, shp);
+#endif
 	shp->shm_perm.key = key;
 	shp->shm_perm.mode = (shmflg & S_IRWXUGO);
 	shp->mlock_user = NULL;
@@ -592,6 +622,13 @@ static int newseg(struct ipc_namespace *ns, struct ipc_params *params)
 	if (IS_ERR(file))
 		goto no_file;
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	// shmem_kernel_file_setup -> __shmem_file_setup
+	//	-> alloc_file (and dentry, inode)
+	POPPK_SHMEM("[sysv][%d] %s(): shmem_kernel_file_setup() "
+				"creates file %p for shm\n",
+				current->pid, __func__, file);
+#endif
 	shp->shm_cprid = task_tgid_vnr(current);
 	shp->shm_lprid = 0;
 	shp->shm_atim = shp->shm_dtim = 0;
@@ -1108,6 +1145,10 @@ long do_shmat(int shmid, char __user *shmaddr, int shmflg,
 	fmode_t f_mode;
 	unsigned long populate = 0;
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	POPPK_SHMEM("[sysv][%d] %s(): START\n",
+				current->pid, __func__);
+#endif
 	err = -EINVAL;
 	if (shmid < 0)
 		goto out;
@@ -1162,6 +1203,12 @@ long do_shmat(int shmid, char __user *shmaddr, int shmflg,
 		err = PTR_ERR(shp);
 		goto out_unlock;
 	}
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	POPPK_SHMEM("[sysv][%d] %s(): shm_obtain_object_check() uses "
+				"shmid %d to find shp %p (struct shmid_kernel*) "
+				"(filt* shm_file)\n",
+				current->pid, __func__, shmid, shp);
+#endif
 
 	err = -EACCES;
 	if (ipcperms(ns, &shp->shm_perm, acc_mode))
@@ -1205,6 +1252,11 @@ long do_shmat(int shmid, char __user *shmaddr, int shmflg,
 		goto out_nattch;
 	}
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	POPPK_SHMEM("[sysv][%d] %s(): creates sfd %p (struct shm_file_data) "
+			"links to private_data and load it from shp (kdata)\n",
+			current->pid, __func__, sfd);
+#endif
 	file->private_data = sfd;
 	file->f_mapping = shp->shm_file->f_mapping;
 	sfd->id = shp->shm_perm.id;
@@ -1257,6 +1309,10 @@ out_nattch:
 	else
 		shm_unlock(shp);
 	up_write(&shm_ids(ns).rwsem);
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	POPPK_SHMEM("[sysv][%d] %s(): do_mmap_pgoff() %p\n",
+				current->pid, __func__, raddr);
+#endif
 	return err;
 
 out_unlock:
diff --git a/kernel/Kconfig.popcorn b/kernel/Kconfig.popcorn
index c6f67896cc30..d6440cbdab1e 100644
--- a/kernel/Kconfig.popcorn
+++ b/kernel/Kconfig.popcorn
@@ -13,6 +13,12 @@ config POPCORN
 
 if POPCORN
 
+config POPCORN_DSHM
+    bool "Popcorn DSHM for muti-process from muti-node"
+    default y
+    help
+        Support distributed shared memory (DSHM) for processes from different nodes
+
 config POPCORN_DEBUG
 	bool "Log debug messages for Popcorn"
 	default n
@@ -66,6 +72,10 @@ config POPCORN_STAT_PGFAULTS
 	depends on POPCORN_STAT
 	default n
 
+config POPCORN_STAT_MSG
+	bool "MSG handling"
+	depends on POPCORN_STAT
+	default n
 
 comment "Popcorn is not currently supported on this architecture"
 	depends on !ARCH_SUPPORTS_POPCORN
diff --git a/kernel/fork.c b/kernel/fork.c
index 5b4a83a1b71d..9b2a5997471e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -92,6 +92,7 @@
 #ifdef CONFIG_POPCORN
 #include <popcorn/types.h>
 #include <popcorn/process_server.h>
+#include <popcorn/debug.h>
 #endif
 
 /*
@@ -401,8 +402,24 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	 * Reset variables for tracking remote execution
 	 */
 	tsk->remote = NULL;
+
+#if defined(CONFIG_POPCORN_DSHM)
+	// also check other code in this CONFIG_POPCORN region
+	if (distributed_process(orig)) {
+		POPPK_DSHMVV("[fork][%d] -> new[%d]\n", // not many and the same
+						orig->pid, tsk->pid);
+		tsk->origin_nid = orig->origin_nid;
+		tsk->remote_nid = orig->remote_nid;
+		tsk->remote_pid = orig->remote_pid;
+		tsk->origin_pid = orig->origin_pid;
+	} else {
+		tsk->remote_nid = tsk->origin_nid = -1;
+		tsk->remote_pid = tsk->origin_pid = -1;
+	}
+#else
 	tsk->remote_nid = tsk->origin_nid = -1;
 	tsk->remote_pid = tsk->origin_pid = -1;
+#endif
 
 	tsk->is_worker = false;
 
@@ -415,9 +432,21 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (orig->tgid != tsk->tgid) {
 		tsk->at_remote = false;
 	}
+#ifdef CONFIG_POPCORN_DSHM
+#if defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (distributed_process(orig)) {
+		if (my_nid) { // remote
+			BUG_ON(!tsk->at_remote);
+		} else { // origin
+			BUG_ON(tsk->at_remote);
+		}
+	}
+#endif
+#else
 
 	tsk->remote_work = NULL;
 	init_completion(&tsk->remote_work_pended);
+#endif
 
 	tsk->migration_target_nid = -1;
 	tsk->backoff_weight = 0;
@@ -756,6 +785,12 @@ void mmput(struct mm_struct *mm)
 {
 	might_sleep();
 
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	if (mm->remote) {
+		POPPK_DSHMV("[%d] %s(): mm->mm_users cnt %d\n",
+			current->pid, __func__, atomic_read(&mm->mm_users));
+	}
+#endif
 	if (atomic_dec_and_test(&mm->mm_users)) {
 		uprobe_clear_state(mm);
 		exit_aio(mm);
diff --git a/kernel/futex.c b/kernel/futex.c
index 407aab2792dc..835e650cb8a1 100644
--- a/kernel/futex.c
+++ b/kernel/futex.c
@@ -3236,7 +3236,8 @@ long do_futex(u32 __user *uaddr, int op, u32 val, ktime_t *timeout,
 			return -ENOSYS;
 	}
 
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	if (distributed_process(current)) {
 		// printk("  [%d] F %x %x %x %p\n", current->pid, op, flags, val, uaddr);
 		WARN_ON(cmd != FUTEX_WAIT &&
@@ -3310,7 +3311,8 @@ SYSCALL_DEFINE6(futex, u32 __user *, uaddr, int, op, u32, val,
 	    cmd == FUTEX_CMP_REQUEUE_PI || cmd == FUTEX_WAKE_OP)
 		val2 = (u32) (unsigned long) utime;
 
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	if (distributed_remote_process(current)) {
 		return process_server_do_futex_at_remote(
 				uaddr, op, val, tp ? true : false, &ts, uaddr2, val2, val3);
diff --git a/kernel/popcorn/Makefile b/kernel/popcorn/Makefile
index 674c4c575f8b..c14cc3c66853 100644
--- a/kernel/popcorn/Makefile
+++ b/kernel/popcorn/Makefile
@@ -6,6 +6,7 @@ obj-$(CONFIG_POPCORN)	+= bundle.o
 obj-$(CONFIG_POPCORN)	+= remote_info.o
 obj-$(CONFIG_POPCORN)	+= pcn_kmsg.o
 obj-$(CONFIG_POPCORN)	+= stat.o
+obj-$(CONFIG_POPCORN)	+= hype_kvm.o
 
 obj-$(CONFIG_POPCORN_POWER_SENSOR_X86) += power_sensor_x86.o
 obj-$(CONFIG_POPCORN_POWER_SENSOR_ARM) += power_sensor_arm.o
diff --git a/kernel/popcorn/hype_kvm.c b/kernel/popcorn/hype_kvm.c
new file mode 100644
index 000000000000..b35a5d044ee1
--- /dev/null
+++ b/kernel/popcorn/hype_kvm.c
@@ -0,0 +1,216 @@
+/**
+ * @file hype_kvm.c
+ *
+ *
+ * @author Ho-Ren (Jack) Chuang, SSRG Virginia Tech 2019
+ *
+ * Distributed under terms of the MIT license.
+ */
+
+#include <popcorn/hype_kvm.h>
+#include <popcorn/debug.h>
+#include "types.h"
+#include "wait_station.h"
+
+#if defined(CONFIG_X86_64)
+#include "../arch/x86/kvm/lapic.h" // Jack arch
+#include "../arch/x86/kvm/x86.h" // Jack arch
+#elif defined(CONFIG_ARM64) // TODO
+#include <kvm/arm_vgic.h>
+#include <asm/kvm_emulate.h>
+
+#include <linux/irqchip/arm-gic-v3.h>
+#include "../virt/kvm/arm/vgic.h"
+//./arch/arm64/include/asm/kvm_psci.h
+//#include <asm/kvm_psci.h> // Jack arch
+
+#define SGI_AFFINITY_LEVEL(reg, level) \
+    ((((reg) & ICC_SGI1R_AFFINITY_## level ##_MASK) \
+    >> ICC_SGI1R_AFFINITY_## level ##_SHIFT) << MPIDR_LEVEL_SHIFT(level))
+#endif
+
+extern struct kvm_vcpu *first_vcpu;
+
+//struct kvm_vcpu *dst_vcpu, struct kvm_lapic_irq *irq, unsigned long *dest_map) {
+int popcorn_cross_ipi_send(int dst_nid) {
+    //struct remote_context *rc = current->mm->remote;
+    struct wait_station *ws;
+    int r;
+    ipi_response_t *res; /* kvm_ipi_req/res */
+    ipi_request_t *req;
+
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+    BUG_ON(my_nid == dst_nid);
+#endif
+
+    req = kmalloc(sizeof(*req), GFP_KERNEL);
+    ws = get_wait_station(current);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+    BUG_ON(!req);
+    //BUG_ON(!rc);
+#endif
+    //req->remote_pid = rc->remote_tgids[dst_nid];
+    req->from_pid = current->pid;
+    req->ws = ws->id;
+
+	KVMPK("%s %s(): cross ipi req =>\n",
+						__FILE__, __func__);
+	pcn_kmsg_send(PCN_KMSG_TYPE_IPI_REQUEST, dst_nid, req, sizeof(*req));
+    res = wait_at_station(ws);
+    r = res->ret;
+    pcn_kmsg_done(res);
+
+    kfree(req); // can be optimized by puting forward
+	return r; /* not supported yet */
+}
+
+
+static void process_ipi_request(struct work_struct *work)
+{
+    START_KMSG_WORK(ipi_request_t, req, work);
+    ipi_response_t *res = pcn_kmsg_get(sizeof(*res));
+    int from_nid = PCN_KMSG_FROM_NID(req);
+	//struct task_struct *tsk = __get_task_struct(req->remote_pid);
+	int ret = 0;
+
+	/* Inject ipi */
+	KVMPK("  => %s %s(): pophype inject ipi\n",
+						__FILE__, __func__);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	BUG_ON(!first_vcpu);
+#endif
+#if defined(CONFIG_X86_64)
+	{
+		struct kvm_lapic_irq irq = {
+			.vector = CROSS_ARCH_IPI_X86_64_VECTOR,
+			.delivery_mode = APIC_DM_FIXED, // kh: // many <0> doing APIC_ICR and <1>(remote) receives this
+			.dest_mode = APIC_DEST_PHYSICAL,
+			.level = 1, // guess 1 because __apic_accept_irq()
+			.trig_mode = 1, // guess set / clear
+			.shorthand = APIC_DEST_SELF, // guess APIC_DEST_PHYSICAL 0x0 // or APIC_DEST_SELF 0x40000
+			//.shorthand = 0x0, // guess APIC_DEST_PHYSICAL 0x0 // or APIC_DEST_SELF 0x40000
+			.dest_id = 0, // pophyp hardcode for 2node case
+			.msi_redir_hint = false
+			// u32 vector; u16 delivery_mode; u16 dest_mode; bool level;
+			// u16 trig_mode; u32 shorthand; u32 dest_id; bool msi_redir_hint;
+		};
+		int r = 0;
+		//ret = kvm_irq_delivery_to_apic_fast(first_vcpu->kvm,
+		//				first_vcpu->arch.apic, &irq, &r, NULL);
+		//if (!ret) {
+		//	printk("ERROR: cross_ipi re-injection failed\n");
+		//	BUG();
+		//}
+
+		r = kvm_apic_set_irq(first_vcpu, &irq, NULL);
+	}
+#elif defined(CONFIG_ARM64)
+	////local_irq_enable();
+	vgic_v3_dispatch_sgi(first_vcpu, 0x7000001);
+	////kvm_skip_instr(first_vcpu, kvm_vcpu_trap_il_is32bit(first_vcpu)); // testing
+
+	if (0) { // to avoid lock - vgic_v3_dispatch_sgi() no lock version
+		u64 reg = 0x7000001;
+		u64 mpidr;
+		u16 target_cpus;
+		//struct kvm_vcpu *c_vcpu;
+		int sgi;
+
+		sgi = (reg & ICC_SGI1R_SGI_ID_MASK) >> ICC_SGI1R_SGI_ID_SHIFT;
+		//bool broadcast;
+		//broadcast = reg & BIT(ICC_SGI1R_IRQ_ROUTING_MODE_BIT);
+		target_cpus = (reg & ICC_SGI1R_TARGET_LIST_MASK) >> ICC_SGI1R_TARGET_LIST_SHIFT;
+		mpidr = SGI_AFFINITY_LEVEL(reg, 3);
+		mpidr |= SGI_AFFINITY_LEVEL(reg, 2);
+		mpidr |= SGI_AFFINITY_LEVEL(reg, 1);
+
+		//level0 = match_mpidr(mpidr, target_cpus, c_vcpu);
+		// need MPIDR_AFFINITY_LEVEL, kvm_vcpu_get_mpidr_aff()
+		unsigned long affinity;
+		int level0;
+		affinity = kvm_vcpu_get_mpidr_aff(first_vcpu);
+		level0 = MPIDR_AFFINITY_LEVEL(affinity, 0);
+		affinity &= ~MPIDR_LEVEL_MASK;
+
+		u64 sgi_aff = mpidr;
+		u16 sgi_cpu_mask = target_cpus;
+		if (sgi_aff != affinity) { // mpidr
+			level0 = -1; /* bail out if the upper three levels don't match */
+			KVMPK("bail out if the upper three levels don't match\n");
+		}
+		if (!(sgi_cpu_mask & BIT(level0))) { // target_cpus
+			level0 = -1; /* Is this VCPU's bit set in the mask ? */
+			KVMPK("Is this VCPU's bit set in the mask ?\n");
+		}
+		if (level0 == -1) {
+			KVMPK("%s %s(): level0 == -1 continue %c\n",
+				__FILE__, __func__,
+				level0 == -1 ? 'O' : 'X');
+		} else {
+			target_cpus &= ~BIT(level0);
+		}
+
+		//vgic_dist_irq_set_pending
+		KVMPK("     %s %s(): sgi %d -1\n", __FILE__, __func__, sgi);
+		struct vgic_dist *dist = &first_vcpu->kvm->arch.vgic;
+		struct vgic_bitmap *x = &dist->irq_pending;
+		// struct vgic_dist at ./include/kvm/arm_vgic.h (doesn't see irq_pending
+		KVMPK("     %s %s(): dist %p sgi %d -1 x %p debug x %p priv %p <%d>\n",
+				__FILE__, __func__, dist, sgi, x,
+				&first_vcpu->kvm->arch.vgic.irq_pending,
+				first_vcpu->kvm->arch.vgic.irq_pending.private,
+				first_vcpu->vcpu_id);
+		int cpuid = first_vcpu->vcpu_id;
+		unsigned long *reg2 = x->private + cpuid;
+		KVMPK("     %s %s(): sgi %d -1 x->private %p reg2 %p\n",
+						__FILE__, __func__, sgi, x->private, reg2);
+		// sgi = int#
+		vgic_dist_irq_set_pending(first_vcpu, sgi);
+
+		KVMPK("     %s %s(): sgi %d -2\n", __FILE__, __func__, sgi);
+		vgic_update_state(first_vcpu->kvm);
+		KVMPK("     %s %s(): sgi %d -3\n", __FILE__, __func__, sgi);
+		vgic_kick_vcpus(first_vcpu->kvm);
+		KVMPK("     %s %s(): sgi %d -4\n", __FILE__, __func__, sgi);
+	}
+#endif
+
+	KVMPK("  %s %s(): inject ipi done reply ->\n",
+								__FILE__, __func__);
+	res->ret = ret;
+    res->from_pid = req->from_pid;
+    res->ws = req->ws;
+    pcn_kmsg_post(PCN_KMSG_TYPE_IPI_RESPONSE,
+                    from_nid, res, sizeof(*res));
+    END_KMSG_WORK(req);
+}
+
+static int handle_ipi_response(struct pcn_kmsg_message *msg)
+{
+    ipi_response_t *res = (ipi_response_t *)msg;
+    struct wait_station *ws = wait_station(res->ws);
+
+    // THIS IS FOR DEBUGGIN PLZ KILL (also in types.h)
+    //struct kvm_lapic_irq *irq = &res->irq;
+    //static int cnt = 0;
+    //cnt++;
+	KVMPK("  ~> %s %s(): cross ipi req done\n",
+						__FILE__, __func__);
+    ws->private = res;
+    complete(&ws->pendings);
+
+    return res->ret;
+//#if defined(CONFIG_X86_64) // kvm_lapic_irq
+//#elif defined(CONFIG_ARM64)
+//    BUG_ON("TODO ARCH");
+//#endif
+}
+
+DEFINE_KMSG_WQ_HANDLER(ipi_request);
+int __init popcorn_hype_kvm_init(void) {
+	KVMPK("%s %s(): callback %d %d registered\n", __FILE__, __func__,
+			PCN_KMSG_TYPE_IPI_REQUEST, PCN_KMSG_TYPE_IPI_RESPONSE);
+	REGISTER_KMSG_WQ_HANDLER(PCN_KMSG_TYPE_IPI_REQUEST, ipi_request);
+    REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_IPI_RESPONSE, ipi_response);
+	return 0;
+}
diff --git a/kernel/popcorn/init.c b/kernel/popcorn/init.c
index b68ab3779ea4..66a49de25f88 100644
--- a/kernel/popcorn/init.c
+++ b/kernel/popcorn/init.c
@@ -20,6 +20,7 @@ struct workqueue_struct *popcorn_ordered_wq;
 EXPORT_SYMBOL(popcorn_wq);
 EXPORT_SYMBOL(popcorn_ordered_wq);
 
+extern int popcorn_hype_kvm_init(void);
 extern int pcn_kmsg_init(void);
 extern int popcorn_nodes_init(void);
 extern int sched_server_init(void);
@@ -51,6 +52,9 @@ static int __init popcorn_init(void)
 
 	remote_info_init();
 	statistics_init();
+
+	popcorn_hype_kvm_init();
+
 	return 0;
 }
 late_initcall(popcorn_init);
diff --git a/kernel/popcorn/page_server.c b/kernel/popcorn/page_server.c
index ed4ca90efe99..eeeefa7eae21 100644
--- a/kernel/popcorn/page_server.c
+++ b/kernel/popcorn/page_server.c
@@ -37,10 +37,323 @@
 
 #include "trace_events.h"
 
+#include <linux/kvm_host.h>
+#include <asm/kvm_host.h>
+#if defined(__aarch64__)
+#include <asm/kvm_mmu.h>
+#endif
+extern struct kvm_vcpu *first_vcpu;
+
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+#define MICROSECOND 1000000
+atomic64_t mm_cnt = ATOMIC64_INIT(0);
+atomic64_t mm_time_ns = ATOMIC64_INIT(0);
+
+/* local origin & it has to bring from remote (RW)*/
+//atomic64_t ptef_ns = ATOMIC64_INIT(0);
+//atomic64_t ptef_cnt = ATOMIC64_INIT(0);
+/* local_origin & __claim_remote_page(1)(!pg_mine)(RW) */
+atomic64_t clr_ns = ATOMIC64_INIT(0);
+atomic64_t clr_cnt = ATOMIC64_INIT(0);
+
+/* local_origin & !pg_mine & !send_revoke_msg & is_page */
+atomic64_t fp_ns = ATOMIC64_INIT(0);
+atomic64_t fp_cnt = ATOMIC64_INIT(0);
+
+/* local_origin & !pg_mine & !send_revoke_msg & is_page */
+atomic64_t fpin_ns = ATOMIC64_INIT(0);
+atomic64_t fpin_cnt = ATOMIC64_INIT(0);
+atomic64_t fpinh_ns = ATOMIC64_INIT(0);
+atomic64_t fpinh_cnt = ATOMIC64_INIT(0);
+
+/* __claim_local_page(pg_mine) & origin */
+atomic64_t inv_ns = ATOMIC64_INIT(0);
+atomic64_t inv_cnt = ATOMIC64_INIT(0);
+
+/* process_page_invalidate_request */
+atomic64_t invh_ns = ATOMIC64_INIT(0);
+atomic64_t invh_cnt = ATOMIC64_INIT(0);
+/* full rr fault time */
+atomic64_t fph_ns = ATOMIC64_INIT(0);
+atomic64_t fph_cnt = ATOMIC64_INIT(0);
+#endif
+
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+bool is_debug(unsigned long addr) {
+	/* HVA 0x7ffd24000000 = GPA 0X0 */
+	if (addr == 0x7ffdc49e4000 || // for other apps
+		addr == 0x7ffd64000000 || // sync_page = shared_mem + 0x40000000
+		((addr >= 0x7ffd64000000) && addr <= 0x7ffd64001000) || 	// X86 tx GPA 0x40000000
+		((addr >= 0x7ffd74000000) && addr <= 0x7ffd74001000) || 	// X86 rx GPA 0X50000000
+		((addr >= 0x7ffd64000000) && addr <= (0x7ffd64000000 + 0x00020000)) || 	// X86 tx ~20slots
+		((addr >= 0x7ffd74000000) && addr <= (0x7ffd74000000 + 0x00020000))		// x86 rx ~20slots
+		//((addr >= 0x7ffd64000000) && addr <= (0x7ffd64000000 + 0x10000000)) || 	// all X86 tx (init many)
+		//((addr >= 0x7ffd74000000) && addr <= (0x7ffd74000000 + 0x10000000))		// all x86 rx (init many)
+		// X86 rx: shared_mem + 0x50000000 = 0x7ffd74000000
+		// ARM GPA 0x80000000
+		) {
+		/* Can include the entire msg_layer shared region 0x80000000 & 0x90000000*/
+		return true;
+	}
+	return false;
+}
+#endif
+
+/*
+	pte_present is a flage
+	pte_none is all 0
+
+ */
+void invalidate_kvm_mmu(struct mm_struct *mm, unsigned long addr, pte_t *h_pte) {
+#if defined(__x86_64__)
+	/* kvm_mmu_notifier_invalidate_page */
+	mmu_notifier_invalidate_page(mm, addr);
+	return;
+#else /* __arch64__  */
+#ifndef CONFIG_POPCORN_CHECK_SANITY || PERF_EXP
+	mmu_notifier_invalidate_page(mm, addr); // prepare to get numbers
+	return;
+#endif
+/* 03/30
+SEE IF TRANSLATION SOLVE THE BUG FOR UNKNOWN REASONS
+*/
+	unsigned long ipa_ofs = 0x7ffd64000000 - 0x80000000;
+	unsigned long ipa = addr - ipa_ofs;
+	pte_t *pte; // guest
+	/* Good to know: stage2_pte = pfn_pte(pte_pfn(pte), PAGE_S2); at kvm_set_spte_hva() */
+
+	if (!is_debug(addr) || !first_vcpu) {
+		return;
+	}
+
+	pte = gpa_to_guest_pte(ipa); // guest
+	pfn_t pfn;
+	if (!pte) {
+		POPPK_GDSHM("%s(): 0%lx fail\n", __func__, addr);
+		return;
+	}
+
+	pfn = pte_pfn(*pte);
+
+	/* Mimic successful case */
+	//printk("  MY mimic_abort_behav() - 1\n");
+	//printk("  [dbg] 0x%lx - 0 pte_present %s pte_none %s\n",
+	//		addr, pte_present(*pte) ? "O***" : "X",
+	//		pte_none(*pte) ? "O***" : "X");
+	//mimic_abort_behav(ipa);
+	//printk("  MY mimic_abort_behav() - 2\n");
+
+	/* Invalidate */
+	//POPPK_GDSHM("  MY mmu_notifier_invalidate_page(mm %p, 0x%lx) - 1\n",
+	//		mm, addr);
+	POPPK_GDSHM("  [dbg]%s(): 0x%lx - 1 pte_present %s pte_none %s mm %p\n",
+			__func__, addr, pte_present(*pte) ? "O***" : "X",
+			pte_none(*pte) ? "O***" : "X", mm);
+	mmu_notifier_invalidate_page(mm, addr); // at include/linux/mmu_notifier.h Jack: this is implemented by injecting req into VM
+	//kvm_vcpu_kick(first_vcpu); // no use
+	// check pte_present
+	POPPK_GDSHM("  [dbg]%s(): 0x%lx - 2 pte_present %s pte_none %s\n",
+			__func__, addr, pte_present(*pte) ? "O***" : "X",
+			pte_none(*pte) ? "O***" : "X");
+	// check if present
+	//POPPK_GDSHM("  MY mmu_notifier_invalidate_page(mm %p, 0x%lx) - 2\n",
+	//		mm, addr);
+//TODO try put_page....fully free the page in stage2
+// put_page(virt_to_page(pte)); // GPA(IPA)
+//咁 直接加到mmu_notifier_invalidate_page啦
+
+	// -> __mmu_notifier_invalidate_page at mm/mmu_notifier.c (general)
+	// 		if mn->ops->invalidate_page()
+	//		-> kvm_mmu_notifier_invalidate_page at virt/kvm/kvm_main.c (general)
+	// kvm_mmu_notifier_invalidate_page() at ./virt/kvm/kvm_main.c (genral)
+	// -> kvm_unmap_hva (must do) (arch)
+	// -> if (need_tlb_flush) kvm_flush_remote_tlbs(kvm); (arch)
+	// -> kvm_arch_mmu_notifier_invalidate_page	(arch)
+	// X86: arch/x86/kvm/x86.c
+	// ARM: ./arch/arm64/include/asm/kvm_host.h
+	//		will not flush becuase kvm_unmap_hva() always return 0.
+	//		Even if I force to flush tlbs, it still doesn't work.
+
+	// kvm_unmap_hva()
+	// x86: ./arch/x86/kvm/mmu.c
+	//	-> kvm_handle_hva(kvm, hva, 0, kvm_unmap_rmapp);
+	// ARM: ./arch/arm/kvm/mmu.c
+	//	First HOST VA to GUEST PA !!!!!!!!
+	//	-> handle_hva_to_gpa(kvm, hva, end, &kvm_unmap_hva_handler, NULL);
+	// 		-> kvm_unmap_hva_handler(kvm, gpa, NULL);
+	//			check kvm->arch.pgd need > 0
+	//			-> unmap_stage2_range(kvm, gpa, PAGE_SIZE);
+	//				-> unmap_range(kvm, kvm->arch.pgd, start, size);
+	//					-> unmap_puds() -> unmap_pmds() -> unmap_ptes()
+	//						= kvm_set_pte(0) + kvm_tlb_flush_vmid_ipa() +
+	//								kvm_flush_dcache_pte() if !device_pfn
+	//
+	// kvm_tlb_flush_vmid_ipa at arch/arm64/kvm/hyp.S
+
+	// According to "phys_addr_t addr = memslot->base_gfn << PAGE_SHIFT;" in "stage2_flush_memslot()", the addr here is on host but the function name is for stage2
+	// stage2_flush_*() -> stage2_flush_ptes() -> kvm_flush_dcache_pte(*pte) -> __kvm_flush_dcache_pte(pte) ->
+	//					-> kvm_flush_dcache_to_poc(pte->page->addr) at ./arch/arm64/include/asm/kvm_mmu.h
+	// 						-> __flush_dcache_area() at ./arch/arm64/mm/cache.S Ensure that the data held in the page kaddr is written back to the page in question. // clean & invalidate D line / unified line
+	// __flush_dcache_area最終還是一般的asm指令 只是對象會是 guest那邊走下來的的address
+	// 這個pte是guest的....stage2_flush_ptes
+	// 但是怎拿guest pte?除了慢慢爬?
+
+
+	// e.g. stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);
+#if defined(__aarch64__) // Above is not enough for ARM
+	// Does ARM call mmu_notifier_invalidate_page() correctly?
+	// ARM
+	// -> __mmu_notifier_invalidate_page
+	//	-> invalidate_page
+
+	//mm->mmu_notifier_mm);
+    // struct mmu_notifier *mn;
+	// mn->ops->invalidate_page(mn, mm, address);
+
+		if (first_vcpu && is_debug(addr)) {
+			// way 2 - try again
+			/* stage 1 (in gust) -> stage 2 (IPA(GPA) to HPA)
+				https://kernelgo.org/armv8-virt-guide.html */
+			//BUG_ON(!first_vcpu);
+			//printk("hcr 0x%lx\n", vcpu_get_hcr(first_vcpu));
+			//kvm_set_way_flush(first_vcpu); // flsuh all stage2 mmu (perf degradation) at ./arch/arm/kvm/mmu.c // THIS ONE HAS A IF CONDITION to call stage2_flush_vm() // second time will not work
+			// So => directly call it
+			// arch/arm/mm/mmu.c?? arm init (X) TODO try it again.
+			/* This is wrong because you cannot invalidate all owned pages */
+			//printk("    [dbg]tage2_flush_vm() 0x%lx - 1\n", addr);
+			//stage2_flush_vm(first_vcpu->kvm); // not crash and not working
+			//printk("    [dbg]tage2_flush_vm() 0x%lx - 2\n", addr);
+			// tage2_flush_vm() -> stage2_flush_memslot() -> stage2_flush_puds() -> stage2_flush_pmds() -> stage2_flush_ptes() -> kvm_flush_dcache_pte()
+			// 		Along with, it kvm_flush_dcache_pmd() & kvm_flush_dcache_pud() all levels' cache
+
+
+#if 1 // change to 0
+			// way 3 (~=way10) - Crash - Before kowning correct *pte, don't enable
+			//kvm_flush_dcache_pte(pte); // X
+			//kvm_flush_dcache_pte(*pte); // Crash in this function
+			// try again
+			POPPK_GDSHMV("  [test] %s(): "
+				"0x%lx !pte_none(*pte) %d && !!pfn_valid(pfn) %d %d ??? "
+				"pfn 0x%lx (see if my way to calculate pfn is correct)\n",
+				__func__, addr, !pte_none(*pte),
+				!!pfn_valid(pfn), pfn_valid(pfn), pfn);
+			if (!pte_none(*pte) && !!pfn_valid(pfn)) { // avoid crash - never enter...... (always 0 is the *host* pte correct? or should be *guest* pte?)
+			//	printk("  [peek]  [dbg]kvm_flush_dcache_pte(*pte) do xxx - 1\n");
+			//	kvm_flush_dcache_pte(*pte); // Crash in this function
+			//	printk("    [dbg]kvm_flush_dcache_pte(*pte) do xxx - 2\n");
+			}
+#endif
+
+			// how guest abort fixed s2pte -
+			// kvm_handle_guest_abort() -> user_mem_abort() ->
+			//		(kvm_set_s2pte_writable()) +
+			//		coherent_cache_guest_page() +
+			//		stage2_set_pte()(kvm_set_pte(0)+kvm_tlb_flush_vmid_ipa()) +
+			//		其中的stage2_set_pte()做的事情很像unmap_ptes()做的
+			//pgprot_t mem_type = PAGE_S2;
+			//pte_t new_pte = pfn_pte(pfn, mem_type);
+			//kvm_set_s2pte_writable(&new_pte);
+
+			// Since one pte will crash I think multiple or all (all would fail) will crash too
+#if 0
+			//// ./arch/arm/kvm/mmu.c stage2(IPA(GPA)->HPA)
+			//// kvm_handle_guest_abort(gfn) -> user_mem_abort() ???
+			////
+			//// way 4 IPA - flush only 1 ipa - arm init(X)
+			////unsigned long ofs = 0x7ffd64000000 - 0x80000000;
+			////kvm_tlb_flush_vmid_ipa(first_vcpu->kvm, addr - ofs); // doesn't work // redundant
+#endif
+			// way 5 - brutal - flush all tlb - arm init (X)(X)(X)
+			//printk("    [dbg]kvm_flush_remote_tlbs() 0x%lx - 1\n", addr);
+			//kvm_flush_remote_tlbs(first_vcpu->kvm); // doesn't work...
+			//printk("    [dbg]kvm_flush_remote_tlbs() 0x%lx - 2\n", addr);
+			// 或許可以哦...... 聽說v7沒support __kvm_tlb_flush_vmid_ipa
+			// 所以v7一次全刷掉
+			// https://patchwork.kernel.org/project/linux-arm-kernel/patch/1362451403-23460-29-git-send-email-marc.zyngier@arm.com/
+
+#if 0
+			// way 6 - arm init (X).
+			//if (is_debug(addr)) {
+			//	spin_lock(&first_vcpu->kvm->mmu_lock);
+			// unmap_stage2_range -- Clear stage2 page table entries to unmap a range
+			//	unmap_stage2_range(first_vcpu->kvm, 0x80000000, PAGE_SIZE); // 1 page more sense // -> unmap_range() -> unmap_puds()
+			//		- kvm_tlb_flush_vmid_ipa
+			//		- kvm_flush_dcache_pud
+			//	spin_unlock(&first_vcpu->kvm->mmu_lock);
+			//}
+			//if (is_debug(addr)) { // CRASH
+			//	stage2_unmap_vm(first_vcpu->kvm); // NULL pointer // highly likely it will crash....
+			//}
+#endif
+
+			/*
+				static inline struct kvm *mmu_notifier_to_kvm(struct mmu_notifier *mn)
+				=> return container_of(mn, struct kvm, mmu_notifier);
+			*/
+
+			// mm/pgtable-generic.c ????
+			//printk("    [dbg]first_vcpu->kvm->arch.pgd 0x%p (O)\n",
+			//			first_vcpu->kvm->arch.pgd);
+
+
+			// way 7
+			//printk("    [dbg]kvm_mmu_free_memory_caches\n");
+			//kvm_mmu_free_memory_caches(first_vcpu); // arm init (X)
+			//printk("    [dbg]kvm_mmu_free_memory_caches done\n");
+
+			// way 8 ....dangerous called upon vm_destory -> crash NULL ptr
+			//struct mmu_notifier *mn;
+			//spin_lock(&mm->mmu_notifier_mm->lock);
+			//hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
+			////while (unlikely(!hlist_empty(&mm->mmu_notifier_mm->list))) {
+			////	mn = hlist_entry(mm->mmu_notifier_mm->list.first,
+			////			 struct mmu_notifier,
+			////			 hlist);
+			//	printk("    [dbg]kvm_mmu_notifier_release() mn %p\n", mn);
+			//	kvm_mmu_notifier_release(mn, mm); // crash - NULL ptr
+			//	//break;
+			//}
+			//spin_unlock(&mm->mmu_notifier_mm->lock);
+			// mn = container_of(mn, struct kvm, mmu_notifier); // X
+
+			// way 9
+			// trying to use
+			// 			or kvm_set_pfn_dirty(pfn)
+			//			or kvm_release_page_clean(page)
+			//				->kvm_release_pfn_clean(pfn) // looks like host pfn...
+			//					->put_page() // cnt-- .....
+
+			// way10 (~=way3) - crash - Unable to handle kernel paging request at virtual address ffff7fffff200000
+			// try coherent_cache_guest_page at user_mem_abort() at ./arch/arm/kvm/mmu.c
+			//	__coherent_cache_guest_page() -> kvm_flush_dcache_to_poc() at ./arch/arm64/include/asm/kvm_mmu.h
+			// pfn is from gfn
+			//printk("    [dbg]kvm_flush_dcache_pte(*pte) - 1\n");
+			//pfn_t pfn = pte_pfn(*pte);
+			//bool ipa_uncached = false; /* read only TODO try false again*/
+			//coherent_cache_guest_page(first_vcpu, pfn, PAGE_SIZE, ipa_uncached); // at ./arch/arm/include/asm/kvm_mmu.h
+			//printk("    [dbg]kvm_flush_dcache_pte(*pte) - 2\n");
+
+			// 目前希望abord的coherent_cache_guest_page()因為這根cache tlb有關係
+			// 其實coherent_cache_guest_page主要幹的就是__kvm_flush_dcache_pte + inv icache...
+		}
+#endif
+#endif
+}
+
+#ifdef CONFIG_POPCORN_DSHM
+extern unsigned long dshm_addr;
+extern unsigned long dshm_len;
+#endif
 inline void page_server_start_mm_fault(unsigned long address)
 {
 #ifdef CONFIG_POPCORN_STAT_PGFAULTS
 	if (!distributed_process(current)) return;
+#ifdef CONFIG_POPCORN_DSHM
+	if (!dshm_addr || !dshm_len ||
+		!(address >= dshm_addr && address < (dshm_addr + dshm_len))) {
+		return;
+	}
+#endif
 	if (current->fault_address == 0 ||
 			current->fault_address != address) {
 		current->fault_address = address;
@@ -55,22 +368,130 @@ inline int page_server_end_mm_fault(int ret)
 {
 #ifdef CONFIG_POPCORN_STAT_PGFAULTS
 	if (!distributed_process(current)) return ret;
+#ifdef CONFIG_POPCORN_DSHM
+	//if (!dshm_addr || !dshm_len ||
+	//	!(address >= dshm_addr && address < (dshm_addr + dshm_len))) {
+	//	return;
+	//} // instead, leverage fault_address to do the job
 
-	if (ret & VM_FAULT_RETRY) {
-		current->fault_retry++;
-	} else if (!(ret & VM_FAULT_ERROR)) {
-		ktime_t dt, fault_end = ktime_get();
-
-		dt = ktime_sub(fault_end, current->fault_start);
-		trace_pgfault_stat(instruction_pointer(current_pt_regs()),
-				current->fault_address, ret,
-				current->fault_retry, ktime_to_ns(dt));
-		current->fault_address = 0;
+	if (current->fault_address) {
+#endif
+		if (ret & VM_FAULT_RETRY) {
+			current->fault_retry++;
+		} else if (!(ret & VM_FAULT_ERROR)) {
+			ktime_t dt, fault_end = ktime_get();
+
+			dt = ktime_sub(fault_end, current->fault_start);
+			trace_pgfault_stat(instruction_pointer(current_pt_regs()),
+					current->fault_address, ret,
+					current->fault_retry, ktime_to_ns(dt));
+			current->fault_address = 0;
+			if (ktime_to_ns(dt) < 1000 * MICROSECOND) {
+				atomic64_add(ktime_to_ns(dt), &mm_time_ns);
+				atomic64_inc(&mm_cnt);
+			}
+		}
+#ifdef CONFIG_POPCORN_DSHM
 	}
 #endif
+#endif
 	return ret;
 }
 
+void pf_time_stat(struct seq_file *seq, void *v)
+{
+#ifdef CONFIG_POPCORN_STAT
+	if (seq) {
+		seq_printf(seq, "%4s  %10ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+					"mm", (atomic64_read(&mm_time_ns) / 1000) / MICROSECOND,
+							(atomic64_read(&mm_time_ns) / 1000)  % MICROSECOND,
+					"cnt", atomic64_read(&mm_cnt),
+					"per", atomic64_read(&mm_cnt) ?
+					 atomic64_read(&mm_time_ns)/atomic64_read(&mm_cnt)/1000 : 0);
+
+		//seq_printf(seq, "%4s  %10ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+		//			"ptef", (atomic64_read(&ptef_ns) / 1000) / MICROSECOND,
+		//					(atomic64_read(&ptef_ns) / 1000)  % MICROSECOND,
+		//			"cnt", atomic64_read(&ptef_cnt),
+		//			"per", atomic64_read(&ptef_cnt) ?
+		//			 atomic64_read(&ptef_ns)/atomic64_read(&ptef_cnt)/1000 : 0);
+
+		seq_printf(seq, "%4s  %10ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+					"clr", (atomic64_read(&clr_ns) / 1000) / MICROSECOND,
+							(atomic64_read(&clr_ns) / 1000)  % MICROSECOND,
+					"cnt", atomic64_read(&clr_cnt),
+					"per", atomic64_read(&clr_cnt) ?
+					 atomic64_read(&clr_ns)/atomic64_read(&clr_cnt)/1000 : 0);
+
+		/* R: only page (R+!pg_mine) */
+		seq_printf(seq, "%4s  %10ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+			"fp", (atomic64_read(&fp_ns) / 1000) / MICROSECOND,
+					(atomic64_read(&fp_ns) / 1000)  % MICROSECOND,
+			"cnt", atomic64_read(&fp_cnt),
+			"per", atomic64_read(&fp_cnt) ?
+			 atomic64_read(&fp_ns)/atomic64_read(&fp_cnt)/1000 : 0);
+
+		seq_printf(seq, "%4s  %10ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+			"fph", (atomic64_read(&fph_ns) / 1000) / MICROSECOND,
+					(atomic64_read(&fph_ns) / 1000)  % MICROSECOND,
+			"cnt", atomic64_read(&fph_cnt),
+			"per", atomic64_read(&fph_cnt) ?
+			 atomic64_read(&fph_ns)/atomic64_read(&fph_cnt)/1000 : 0);
+
+		/* W: only inv */
+		seq_printf(seq, "%4s  %10ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+			"inv", (atomic64_read(&inv_ns) / 1000) / MICROSECOND,
+					(atomic64_read(&inv_ns) / 1000)  % MICROSECOND,
+			"cnt", atomic64_read(&inv_cnt),
+			"per", atomic64_read(&inv_cnt) ?
+			 atomic64_read(&inv_ns)/atomic64_read(&inv_cnt)/1000 : 0);
+
+		seq_printf(seq, "%4s  %10ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+			"invh", (atomic64_read(&invh_ns) / 1000) / MICROSECOND,
+					(atomic64_read(&invh_ns) / 1000)  % MICROSECOND,
+			"cnt", atomic64_read(&invh_cnt),
+			"per", atomic64_read(&invh_cnt) ?
+			 atomic64_read(&invh_ns)/atomic64_read(&invh_cnt)/1000 : 0);
+
+		/* W: page + inv */
+		seq_printf(seq, "%4s  %10ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+			"fpiv", (atomic64_read(&fpin_ns) / 1000) / MICROSECOND,
+					(atomic64_read(&fpin_ns) / 1000)  % MICROSECOND,
+			"cnt", atomic64_read(&fpin_cnt),
+			"per", atomic64_read(&fpin_cnt) ?
+			 atomic64_read(&fpin_ns)/atomic64_read(&fpin_cnt)/1000 : 0);
+		seq_printf(seq, "%5s  %9ld.%06ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+			"fpivh", (atomic64_read(&fpinh_ns) / 1000) / MICROSECOND,
+					(atomic64_read(&fpinh_ns) / 1000)  % MICROSECOND,
+			"cnt", atomic64_read(&fpinh_cnt),
+			"per", atomic64_read(&fpinh_cnt) ?
+			 atomic64_read(&fpinh_ns)/atomic64_read(&fpinh_cnt)/1000 : 0);
+	} else {
+        atomic64_set(&mm_cnt, 0);
+        atomic64_set(&mm_time_ns, 0);
+
+		//atomic64_set(&ptef_cnt, 0);
+		//atomic64_set(&ptef_ns, 0);
+		atomic64_set(&clr_cnt, 0);
+		atomic64_set(&clr_ns, 0);
+		atomic64_set(&fp_ns, 0);
+		atomic64_set(&fp_cnt, 0);
+		atomic64_set(&fph_ns, 0);
+		atomic64_set(&fph_cnt, 0);
+
+		atomic64_set(&inv_cnt, 0);
+		atomic64_set(&inv_ns, 0);
+		atomic64_set(&invh_cnt, 0);
+		atomic64_set(&invh_ns, 0);
+
+		atomic64_set(&fpin_ns, 0);
+		atomic64_set(&fpin_cnt, 0);
+		atomic64_set(&fpinh_ns, 0);
+		atomic64_set(&fpinh_cnt, 0);
+	}
+#endif
+}
+
 static inline int __fault_hash_key(unsigned long address)
 {
 	return (address >> PAGE_SHIFT) % FAULTS_HASH;
@@ -845,10 +1266,39 @@ static void __do_invalidate_page(struct task_struct *tsk, page_invalidate_reques
 	BUG_ON(!pte_present(*pte));
 	entry = ptep_clear_flush(vma, addr, pte);
 	entry = pte_make_invalid(entry);
+#if POPHYPE_ARCH_2NDTLB_FLUSH
+#if defined(CONFIG_POPCORN_DSHM) && !defined(__aarch64__)
+    /* kvm_mmu_notifier_invalidate_page */
+    //mmu_notifier_invalidate_page(mm, addr);
+	invalidate_kvm_mmu(mm, addr, pte);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	//{
+	//	static int cnt = 0;
+	//	if (cnt == 0) {
+	//		printk("[dbg][ONCE]has "
+	//			"mmu_notifier_mm/mmu_notifier_invalidate_page? %p\n",
+	//			mm->mmu_notifier_mm);
+	//		cnt = 1;
+	//	}
+	//}
+#endif
+#endif
+#endif
 
 	set_pte_at_notify(mm, addr, pte, entry);
 	update_mmu_cache(vma, addr, pte);
 
+	/* According to wp_page_copy() at mm/memory.c
+		mmu_notifier_invalidate*() is after set_pte_at_notify() */
+#if POPHYPE_ARCH_2NDTLB_FLUSH
+#if defined(CONFIG_POPCORN_DSHM) && defined(__aarch64__)
+    /* kvm_mmu_notifier_invalidate_page */
+    //mmu_notifier_invalidate_page(mm, addr);
+	invalidate_kvm_mmu(mm, addr, pte);
+#endif
+#else // X86 & ARM flush at the same place
+	invalidate_kvm_mmu(mm, addr, pte);
+#endif
 	__finish_invalidation(fh);
 	pte_unmap_unlock(pte, ptl);
 
@@ -859,6 +1309,9 @@ out:
 
 static void process_page_invalidate_request(struct work_struct *work)
 {
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	ktime_t dt, invh_end, invh_start = ktime_get();
+#endif
 	START_KMSG_WORK(page_invalidate_request_t, req, work);
 	page_invalidate_response_t *res;
 	struct task_struct *tsk;
@@ -877,8 +1330,21 @@ static void process_page_invalidate_request(struct work_struct *work)
 		goto out_free;
 	}
 
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	if (is_debug(req->addr)) {
+		POPPK_GDSHM("\nREMOTE_INV_REQUEST xxx> 0x%lx xxx\n",
+				req->addr);
+	}
+#endif
 	__do_invalidate_page(tsk, req);
 
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	if (is_debug(req->addr)) {
+		POPPK_GDSHM("  >x>[%d] -x>[%d/%d] >xxx> 0x%lx >xxx>\n",
+				req->remote_pid, res->origin_pid,
+				PCN_KMSG_FROM_NID(req), req->addr);
+	}
+#endif
 	PGPRINTK(">>[%d] ->[%d/%d]\n", req->remote_pid, res->origin_pid,
 			PCN_KMSG_FROM_NID(req));
 	pcn_kmsg_post(PCN_KMSG_TYPE_PAGE_INVALIDATE_RESPONSE,
@@ -886,6 +1352,13 @@ static void process_page_invalidate_request(struct work_struct *work)
 
 	put_task_struct(tsk);
 
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	invh_end = ktime_get();
+	dt = ktime_sub(invh_end, invh_start);
+	atomic64_add(ktime_to_ns(dt), &invh_ns);
+	atomic64_inc(&invh_cnt);
+#endif
+
 out_free:
 	END_KMSG_WORK(req);
 }
@@ -914,6 +1387,12 @@ static void __revoke_page_ownership(struct task_struct *tsk, int nid, pid_t pid,
 	req->origin_ws = ws_id;
 	req->remote_pid = pid;
 
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	if (is_debug(addr)) {
+		POPPK_GDSHM("  [%d] revoke %lx [%d/%d] -x>\n",
+				tsk->pid, addr, pid, nid);
+	}
+#endif
 	PGPRINTK("  [%d] revoke %lx [%d/%d]\n", tsk->pid, addr, pid, nid);
 	pcn_kmsg_post(PCN_KMSG_TYPE_PAGE_INVALIDATE_REQUEST, nid, req, sizeof(*req));
 }
@@ -968,6 +1447,9 @@ int page_server_release_page_ownership(struct vm_area_struct *vma, unsigned long
 	clear_page_owner(my_nid, mm, addr);
 	pte_val = ptep_clear_flush(vma, addr, pte);
 	pte_val = pte_make_invalid(pte_val);
+#ifdef CONFIG_POPCORN_DSHM
+	BUG_ON("NEVER CALLED");
+#endif
 
 	set_pte_at_notify(mm, addr, pte, pte_val);
 	update_mmu_cache(vma, addr, pte);
@@ -1000,6 +1482,59 @@ static int handle_remote_page_response(struct pcn_kmsg_message *msg)
 static int __request_remote_page(struct task_struct *tsk, int from_nid, pid_t from_pid, unsigned long addr, unsigned long fault_flags, int ws_id, struct pcn_kmsg_rdma_handle **rh)
 {
 	remote_page_request_t *req;
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	//static int interested_pid = -1; // moveed to outside
+#endif
+#if 0
+	/* This is too late!! */
+	/* THIS IS FOR VCPU
+		which doesn't inherite Pophype main thread's task_struct metadata */
+	//if (addr == 0x7ffdc49e4000 && tsk->at_remote) {
+	if ((from_nid < 0 || from_pid < 0)) {
+	//if ((from_nid < 0 || from_pid < 0) && tsk->at_remote) {
+		printk("  THIS IS HARCODED. IF THIS WORKS. FIND A CORRECT SOLUTION\n");
+		printk("  [dbg] current->pid %d =? tsk->pid %d ??? "
+				"(at_remote ? yes : no)\n",
+				current->pid, tsk->pid);
+		printk("  [dbg][%d] rc->remote_tgids[0] = [origin's %d] ok??? \n",
+				current->pid, tsk->mm->remote->remote_tgids[0]);
+		printk("  [dbg][%d] _nid %d _pid %d (old)\n",
+				current->pid, from_nid, from_pid);
+		if (my_nid) {
+			from_nid = tsk->origin_nid = 0;
+			//from_pid = tsk->origin_pid = rc->remote_tgids[PCN_KMSG_FROM_NID(msg)];
+			from_pid = tsk->origin_pid = tsk->mm->remote->remote_tgids[from_nid];
+		} else {
+			from_nid = tsk->remote_nid = 1;
+			from_pid = tsk->remote_pid = tsk->mm->remote->remote_tgids[from_nid];
+		}
+		printk("  [dbg][%d] _nid %d _pid %d (new)\n",
+				current->pid, from_nid, from_pid);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		//interested_pid = current->pid;
+		//printk("  [dbg][%d] interested_pid updated to [%d]\n",
+		//		current->pid, current->pid);
+		//} // moveed to outside
+#endif
+	}
+#endif
+
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	//if (interested_pid == current->pid) { // This pid memset and fail
+	//	printk("  [dbg][%d] *** %lx ***\n",
+	//			current->pid, addr);
+	//} // moveed to outside
+
+	/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+	//if (addr == 0x7ffdc49e4000) {
+	if ((from_nid < 0 || from_pid < 0) || is_debug(addr)) {
+		POPPK_DSHM("  [dbg][%d] ->[%d/%d] ***** %lx ***** inst %lx "
+				"target[%d] my at_remote %d =>\n", tsk->pid,
+				from_pid, from_nid, addr,
+				instruction_pointer(current_pt_regs()), tsk->origin_nid,
+				tsk->at_remote);
+	}
+#endif
 
 	*rh = NULL;
 
@@ -1053,6 +1588,21 @@ static remote_page_response_t *__fetch_page_from_origin(struct task_struct *tsk,
 		} else {
 			copy_to_user_page(vma, page, addr, paddr, rp->page, PAGE_SIZE);
 		}
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		if (is_debug(addr)) {
+			POPPK_GDSHM("  %lx peek first u64 %lu - "
+					"%x %x %x %x %x %x %x %x\n",
+					addr, *(unsigned long*)paddr,
+					*(unsigned char*)(paddr + 7),
+					*(unsigned char*)(paddr + 6),
+					*(unsigned char*)(paddr + 5),
+					*(unsigned char*)(paddr + 4),
+					*(unsigned char*)(paddr + 3),
+					*(unsigned char*)(paddr + 2),
+					*(unsigned char*)(paddr + 1),
+					*(unsigned char*)(paddr + 0));
+		}
+#endif
 		kunmap(page);
 		flush_dcache_page(page);
 		__SetPageUptodate(page);
@@ -1063,7 +1613,7 @@ static remote_page_response_t *__fetch_page_from_origin(struct task_struct *tsk,
 	return rp;
 }
 
-static int __claim_remote_page(struct task_struct *tsk, struct mm_struct *mm, struct vm_area_struct *vma, unsigned long addr, unsigned long fault_flags, struct page *page)
+static int __claim_remote_page(struct task_struct *tsk, struct mm_struct *mm, struct vm_area_struct *vma, unsigned long addr, unsigned long fault_flags, struct page *page, int local_origin)
 {
 	int peers;
 	unsigned int random = prandom_u32();
@@ -1077,6 +1627,13 @@ static int __claim_remote_page(struct task_struct *tsk, struct mm_struct *mm, st
 	unsigned long offset;
 	struct page *pip = __get_page_info_page(mm, addr, &offset);
 	unsigned long *pi = (unsigned long *)kmap(pip) + offset;
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	//int revoke = 0;
+	int page_trans = 0;
+	ktime_t fp_start;
+	if (local_origin) /* aka !pg_mine */
+		fp_start = ktime_get();
+#endif
 	BUG_ON(!pip);
 
 	peers = bitmap_weight(pi, MAX_POPCORN_NODES);
@@ -1106,6 +1663,11 @@ static int __claim_remote_page(struct task_struct *tsk, struct mm_struct *mm, st
 			if (fault_for_write(fault_flags)) {
 				clear_bit(nid, pi);
 				__revoke_page_ownership(tsk, nid, pid, addr, ws->id);
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+				//revoke = 1;
+				//BUG_ON(revoke && "Two nodes shouldn't send stand along inv");
+				BUG_ON("Two nodes shouldn't send stand along inv");
+#endif
 			}
 		}
 		if (--peers == 0) break;
@@ -1127,12 +1689,38 @@ static int __claim_remote_page(struct task_struct *tsk, struct mm_struct *mm, st
 		kunmap(page);
 		flush_dcache_page(page);
 		__SetPageUptodate(page);
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+		page_trans = 1;
+#endif
 	}
 	pcn_kmsg_done(rp);
 
 	if (rh) pcn_kmsg_unpin_rdma_buffer(rh);
 	__put_task_remote(rc);
 	kunmap(pip);
+
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	//if (!my_nid && local_origin && !revoke && page_trans) {
+	//if (!my_nid && local_origin && page_trans) {
+	if (!my_nid && local_origin) {
+		if (fault_for_write(fault_flags)) { /* page + inv */
+			ktime_t dt, fp_end = ktime_get();
+			dt = ktime_sub(fp_end, fp_start);
+			atomic64_add(ktime_to_ns(dt), &fpin_ns);
+			atomic64_inc(&fpin_cnt);
+		} else { /* page + !inv  */
+		//if (page_trans) {
+			ktime_t dt, fp_end = ktime_get();
+			dt = ktime_sub(fp_end, fp_start);
+			atomic64_add(ktime_to_ns(dt), &fp_ns);
+			atomic64_inc(&fp_cnt);
+		//}
+		}
+
+		if (!page_trans)
+			BUG_ON("!pg_mine must transfer page");
+	}
+#endif
 	return 0;
 }
 
@@ -1144,6 +1732,10 @@ static void __claim_local_page(struct task_struct *tsk, unsigned long addr, int
 	struct page *pip = __get_page_info_page(mm, addr, &offset);
 	unsigned long *pi;
 	int peers;
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	int is_inv = 0;
+	ktime_t dt, inv_end, inv_start;
+#endif
 
 	if (!pip) return; /* skip claiming non-distributed page */
 	pi = (unsigned long *)kmap(pip) + offset;
@@ -1158,6 +1750,10 @@ static void __claim_local_page(struct task_struct *tsk, unsigned long addr, int
 
 	if (test_bit(my_nid, pi) && except_nid != my_nid) peers--;
 
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	inv_start = ktime_get();
+#endif
+
 	if (peers > 0) {
 		int nid;
 		struct remote_context *rc = get_task_remote(tsk);
@@ -1169,11 +1765,24 @@ static void __claim_local_page(struct task_struct *tsk, unsigned long addr, int
 
 			clear_bit(nid, pi);
 			__revoke_page_ownership(tsk, nid, pid, addr, ws->id);
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+			is_inv = 1;
+#endif
 		}
 		put_task_remote(tsk);
 
 		wait_at_station(ws);
 	}
+
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	if (is_inv) {
+		inv_end = ktime_get();
+		dt = ktime_sub(inv_end, inv_start);
+		atomic64_add(ktime_to_ns(dt), &inv_ns);
+		atomic64_inc(&inv_cnt);
+	}
+#endif
+
 	kunmap(pip);
 }
 
@@ -1266,12 +1875,52 @@ static int __handle_remotefault_at_remote(struct task_struct *tsk, struct mm_str
 	if (fault_for_write(fault_flags)) {
 		clear_page_owner(my_nid, mm, addr);
 		entry = pte_make_invalid(entry);
-	} else {
+#if POPHYPE_ARCH_2NDTLB_FLUSH
+#if defined(CONFIG_POPCORN_DSHM) && !defined(__aarch64__)
+		/* kvm_mmu_notifier_invalidate_page */
+		invalidate_kvm_mmu(mm, addr, pte);
+		//mmu_notifier_invalidate_page(mm, addr); // rr
+#endif
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		if (is_debug(addr)) {
+			POPPK_GDSHM("  rr W XXX %lx XXX\n", addr);
+		}
+#endif
+#endif
+	} else { // share-share (ss)
 		entry = pte_wrprotect(entry);
+#ifdef CONFIG_POPCORN_DSHM
+		/* TODO should I change ARM's stage 2 table?*/
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		if (is_debug(addr)) {
+			//invalidate_kvm_mmu(mm, addr, pte);
+			// stage2_wp_ptes -> kvm_set_s2pte_readonly(pte);
+			POPPK_GDSHM("  rr Rss XXX %lx XXX\n", addr);
+		}
+#endif
+#endif
 	}
 
 	set_pte_at_notify(mm, addr, pte, entry);
 	update_mmu_cache(vma, addr, pte);
+
+#if POPHYPE_ARCH_2NDTLB_FLUSH
+#if defined(CONFIG_POPCORN_DSHM) && defined(__aarch64__)
+	//if (fault_for_write(fault_flags)) {
+		/* kvm_mmu_notifier_invalidate_page */
+		invalidate_kvm_mmu(mm, addr, pte);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		if (is_debug(addr)) {
+			POPPK_GDSHM("  remotefault_at_origin XXX %lx %s XXX\n",
+				addr, fault_for_write(fault_flags) ? "W" : "Rss");
+		}
+#endif
+	//}
+#endif
+#else // X86 & ARM flush at the same place
+	invalidate_kvm_mmu(mm, addr, pte);
+#endif
+
 	pte_unmap_unlock(pte, ptl);
 
 	page = vm_normal_page(vma, addr, *pte);
@@ -1279,6 +1928,22 @@ static int __handle_remotefault_at_remote(struct task_struct *tsk, struct mm_str
 	flush_cache_page(vma, addr, page_to_pfn(page));
 	if (TRANSFER_PAGE_WITH_RDMA) {
 		paddr = kmap(page);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		if (is_debug(req->addr)) { // don't peek after invalidation...?
+			POPPK_GDSHM("  -- 0x%lx don't peek after inv\n", addr);
+			//printk("  -- %lx peek first u64 %lu - "
+			//		"%x %x %x %x %x %x %x %x\n",
+			//		addr, *(unsigned long*)paddr,
+			//		*(unsigned char*)(paddr + 7),
+			//		*(unsigned char*)(paddr + 6),
+			//		*(unsigned char*)(paddr + 5),
+			//		*(unsigned char*)(paddr + 4),
+			//		*(unsigned char*)(paddr + 3),
+			//		*(unsigned char*)(paddr + 2),
+			//		*(unsigned char*)(paddr + 1),
+			//		*(unsigned char*)(paddr + 0));
+		}
+#endif
 		pcn_kmsg_rdma_write(PCN_KMSG_FROM_NID(req),
 				req->rdma_addr, paddr, PAGE_SIZE, req->rdma_key);
 		kunmap(page);
@@ -1364,9 +2029,13 @@ again:
 			BUG_ON(fault_for_read(fault_flags) && "Read fault from owner??");
 			__claim_local_page(tsk, addr, from_nid);
 			grant = true;
+#if defined(CONFIG_POPCORN_DSHM)
+			/* grant: remote has had the page, just need to be granted
+						exclusive write */
+#endif
 		} else {
 			if (!page_is_mine(mm, addr)) {
-				__claim_remote_page(tsk, mm, vma, addr, fault_flags, page);
+				__claim_remote_page(tsk, mm, vma, addr, fault_flags, page, 0); // > 2node
 			} else {
 				if (fault_for_write(fault_flags))
 					__claim_local_page(tsk, addr, my_nid);
@@ -1381,6 +2050,29 @@ again:
 		if (fault_for_write(fault_flags)) {
 			clear_page_owner(my_nid, mm, addr);
 			entry = pte_make_invalid(entry);
+#if POPHYPE_ARCH_2NDTLB_FLUSH
+#if defined(CONFIG_POPCORN_DSHM) && !defined(__aarch64__)
+			/* kvm_mmu_notifier_invalidate_page */
+			//mmu_notifier_invalidate_page(mm, addr); // remotefault at origin
+			invalidate_kvm_mmu(mm, addr, pte);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+			if (is_debug(addr)) {
+				POPPK_GDSHM("  remotefault_at_origin XXX %lx XXX\n", addr);
+			}
+#endif
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+			{
+				static int cnt = 0;
+				if (cnt == 0) {
+					printk("[dbg][ONCE]has "
+						"mmu_notifier_mm/mmu_notifier_invalidate_page? %p\n",
+						mm->mmu_notifier_mm);
+					cnt = 1;
+				}
+			}
+#endif
+#endif
+#endif
 		} else {
 			entry = pte_make_valid(entry); /* For remote-claimed case */
 			entry = pte_wrprotect(entry);
@@ -1388,6 +2080,23 @@ again:
 		}
 		set_pte_at_notify(mm, addr, pte, entry);
 		update_mmu_cache(vma, addr, pte);
+#if POPHYPE_ARCH_2NDTLB_FLUSH
+#if defined(CONFIG_POPCORN_DSHM) && defined(__aarch64__)
+//		if (fault_for_write(fault_flags)) {
+			/* kvm_mmu_notifier_invalidate_page */
+			//mmu_notifier_invalidate_page(mm, addr); // remotefault at origin
+			invalidate_kvm_mmu(mm, addr, pte);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+			if (is_debug(addr)) {
+				POPPK_GDSHM("  remotefault_at_origin XXX %lx %s XXX\n",
+					addr, fault_for_write(fault_flags) ? "W" : "R");
+			}
+#endif
+//		}
+#endif
+#else // X86 & ARM flush at the same place
+		invalidate_kvm_mmu(mm, addr, pte);
+#endif
 
 		spin_unlock(ptl);
 	}
@@ -1397,6 +2106,21 @@ again:
 		flush_cache_page(vma, addr, page_to_pfn(page));
 		if (TRANSFER_PAGE_WITH_RDMA) {
 			paddr = kmap(page);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+			if (is_debug(req->addr)) {
+				POPPK_GDSHM("  -- %lx peek first u64 %lu - "
+						"%x %x %x %x %x %x %x %x\n",
+						addr, *(unsigned long*)paddr,
+						*(unsigned char*)(paddr + 7),
+						*(unsigned char*)(paddr + 6),
+						*(unsigned char*)(paddr + 5),
+						*(unsigned char*)(paddr + 4),
+						*(unsigned char*)(paddr + 3),
+						*(unsigned char*)(paddr + 2),
+						*(unsigned char*)(paddr + 1),
+						*(unsigned char*)(paddr + 0));
+			}
+#endif
 			pcn_kmsg_rdma_write(PCN_KMSG_FROM_NID(req),
 					req->rdma_addr, paddr, PAGE_SIZE, req->rdma_key);
 			kunmap(page);
@@ -1430,6 +2154,22 @@ static void process_remote_page_request(struct work_struct *work)
 	int res_size;
 	enum pcn_kmsg_type res_type;
 	int down_read_retry = 0;
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	int rr = 0;
+	ktime_t fph_start = ktime_get();
+#endif
+
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+	/* Doesn't know about this req's address. Need to hardcode by running first and getting the addr, then put here */
+	if (is_debug(req->addr)) {
+		POPPK_GDSHM("  =>[dbg][I'm %d] *** %lx %c *** got req - start "
+				"(req->remote_pid %d current->is_worker %d)\n",
+				current->pid, req->addr,
+				fault_for_write(req->fault_flags) ? 'W' : 'R',
+				req->remote_pid, current->is_worker);
+	}
+#endif
 
 	if (TRANSFER_PAGE_WITH_RDMA) {
 		res = pcn_kmsg_get(sizeof(remote_page_response_short_t));
@@ -1441,15 +2181,40 @@ again:
 	tsk = __get_task_struct(req->remote_pid);
 	if (!tsk) {
 		res->result = VM_FAULT_SIGBUS;
-		PGPRINTK("  [%d] not found\n", req->remote_pid);
+		printk(KERN_ERR "%s():  [%d] not found\n", __func__, req->remote_pid);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+		if (is_debug(req->addr)) {
+			POPPK_GDSHM("  [deb][origin %d] \n", req->remote_pid); // -1
+			POPPK_GDSHM("  [origin %d] not found\n", req->remote_pid); // -1
+		}
+#endif
 		goto out;
 	}
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+	if (is_debug(req->addr)) {
+		POPPK_GDSHM("  -[dbg][%d] %lx tsk %p tsk->is_worker %d\n",
+				req->remote_pid, req->addr, tsk, tsk->is_worker);
+	}
+#endif
 	mm = get_task_mm(tsk);
 
 	PGPRINTK("\nREMOTE_PAGE_REQUEST [%d] %lx %c %lx from [%d/%d]\n",
 			req->remote_pid, req->addr,
 			fault_for_write(req->fault_flags) ? 'W' : 'R',
 			req->instr_addr, req->origin_pid, from_nid);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+	if (is_debug(req->addr)) {
+		POPPK_GDSHM("REMOTE_PAGE_REQUEST -[%d] %lx %c inst %lx from [%d/%d]\n",
+				//"([dbg] mm %p)\n",
+				req->remote_pid, req->addr,
+				fault_for_write(req->fault_flags) ? 'W' : 'R',
+				req->instr_addr, req->origin_pid, from_nid);
+				//, mm);
+	}
+#endif
 
 	while (!down_read_trylock(&mm->mmap_sem)) {
 		if (!tsk->at_remote && down_read_retry++ > 4) {
@@ -1460,6 +2225,16 @@ again:
 	}
 	vma = find_vma(mm, req->addr);
 	if (!vma || vma->vm_start > req->addr) {
+		printk(KERN_ERR "\n%s: req->remote_pid[%d] VMA not found %lx\n\n",
+					__func__, req->remote_pid, req->addr);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+		if (is_debug(req->addr)) {
+			POPPK_GDSHM("\n%s: req->remote_pid[%d] VMA not found *** %lx *** "
+					"ret = VM_FAULT_SIGBUS %d\n\n",
+					__func__, req->remote_pid, req->addr, VM_FAULT_SIGBUS);
+		}
+#endif
 		res->result = VM_FAULT_SIGBUS;
 		goto out_up;
 	}
@@ -1470,6 +2245,10 @@ again:
 
 	if (tsk->at_remote) {
 		res->result = __handle_remotefault_at_remote(tsk, mm, vma, req, res);
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+		if (res->result == 0)
+			rr = 1;
+#endif
 	} else {
 		res->result = __handle_remotefault_at_origin(tsk, mm, vma, req, res);
 	}
@@ -1499,6 +2278,29 @@ out:
 	res->origin_pid = req->origin_pid;
 	res->origin_ws = req->origin_ws;
 
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+//	/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+	if (is_debug(req->addr)) { // cannot put here since RDMA......
+		if (res->result != VM_FAULT_CONTINUE) {
+//			POPPK_GDSHM("  - %lx peek first u64 %lu - "
+//					"%x %x %x %x %x %x %x %x\n",
+//					*(unsigned long*)res->page,
+//					res->page[0], res->page[1], res->page[2], res->page[3],
+//					res->page[4], res->page[5], res->page[6], res->page[7]);
+		} else {
+			POPPK_GDSHM("  --[dbg][%d] %lx "
+					"cannot peek first u64 since it's the same\n",
+					current->pid, req->addr);
+		}
+//		POPPK_GDSHM("  [%d] ->[%d/%d] %x process_req done=>\n", req->remote_pid,
+//				res->origin_pid, from_nid, res->result);
+	}
+#endif
+#if defined(CONFIG_POPCORN_DSHM)
+	/* 'grant = 1' leads to VM_FAULT_CONTINUE/
+		This means no need to transfer page data
+		but only the permission is changed */
+#endif
 	PGPRINTK("  [%d] ->[%d/%d] %x\n", req->remote_pid,
 			res->origin_pid, from_nid, res->result);
 
@@ -1509,6 +2311,14 @@ out:
 	pcn_kmsg_post(res_type, from_nid, res, res_size);
 
 	END_KMSG_WORK(req);
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	if (rr) {
+		ktime_t dt, fph_end = ktime_get();
+		dt = ktime_sub(fph_end, fph_start);
+		atomic64_add(ktime_to_ns(dt), &fph_ns);
+		atomic64_inc(&fph_cnt);
+	}
+#endif
 }
 
 
@@ -1565,7 +2375,7 @@ retry:
 
 	if (leader && !page_is_mine(mm, addr)) {
 		struct page *page = get_normal_page(vma, addr, pte);
-		__claim_remote_page(current, mm, vma, addr, fault_flags, page);
+		__claim_remote_page(current, mm, vma, addr, fault_flags, page, 0);
 
 		spin_lock(ptl);
 		__make_pte_valid(mm, vma, addr, fault_flags, pte);
@@ -1605,7 +2415,35 @@ static int __handle_localfault_at_remote(struct mm_struct *mm,
 	struct fault_handle *fh;
 	bool leader;
 	remote_page_response_t *rp;
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	ktime_t fp_start, fpin_start;
+	ktime_t dt, inv_end, inv_start;
+#endif
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	static int interested_pid = -1; // This pid (vcpu1 at remote) memset and fail
+#endif
 
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	/* THIS IS FOR VCPU
+		which doesn't inherite Pophype main thread's task_struct metadata */
+	/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+	//if (addr == 0x7ffdc49e4000) {
+	if ((current->origin_nid < 0 || current->origin_pid < 0)) {
+		printk("  [dbg][%d] ****** %lx ******** start (is_worker %d)\n",
+				current->pid, addr, current->is_worker);
+
+		printk("  [dbg][%d] interested_pid updated to [%d]\n",
+				current->pid, current->pid);
+		interested_pid = current->pid;
+	}
+
+//	//if (interested_pid == current->pid) { // This pid memset and fail
+//	if (is_debug(addr)) {
+//		POPPK_GDSHM("  [dbg][%d] ***** %lx %c ***** start\n",
+//				current->pid, addr,
+//				fault_for_write(fault_flags) ? 'W' : 'R');
+//	}
+#endif
 	if (anon_vma_prepare(vma)) {
 		BUG_ON("Cannot prepare vma for anonymous page");
 		pte_unmap(pte);
@@ -1632,6 +2470,16 @@ static int __handle_localfault_at_remote(struct mm_struct *mm,
 		pte_unmap(pte);
 		ret = fh->ret;
 		if (ret) up_read(&mm->mmap_sem);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+		//if (addr == 0x7ffdc49e4000) {
+		if ((current->origin_nid < 0 || current->origin_pid < 0) ||
+			is_debug(addr)
+			) {
+			POPPK_GDSHM("  [dbg][%d] *** %lx *** follower ret %d\n",
+					current->pid, addr, ret);
+		}
+#endif
 		goto out_follower;
 	}
 
@@ -1646,14 +2494,61 @@ static int __handle_localfault_at_remote(struct mm_struct *mm,
 	}
 	get_page(page);
 
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	fp_start = fpin_start = inv_start = ktime_get();
+#endif
+
 	rp = __fetch_page_from_origin(current, vma, addr, fault_flags, page);
 
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+		if (page_is_mine(mm, addr)) {
+			if (fault_for_write(fault_flags)) {
+				if (rp->result == VM_FAULT_CONTINUE) { /* W: inv lat */
+					inv_end = ktime_get();
+					dt = ktime_sub(inv_end, inv_start);
+					atomic64_add(ktime_to_ns(dt), &inv_ns);
+					atomic64_inc(&inv_cnt);
+				} else if (!rp->result) { /* W: inv + page transferred */
+					// X -> W
+					ktime_t dt, fpin_end = ktime_get();
+					dt = ktime_sub(fpin_end, fpin_start);
+					atomic64_add(ktime_to_ns(dt), &fpin_ns);
+					atomic64_inc(&fpin_cnt);
+				}
+			}
+		} else { /* fp only page */
+			if (fault_for_read(fault_flags)) {
+				ktime_t dt, fp_end = ktime_get();;
+				dt = ktime_sub(fp_end, fp_start);
+				atomic64_add(ktime_to_ns(dt), &fp_ns);
+				atomic64_inc(&fp_cnt);
+			}
+			if (fault_for_write(fault_flags)) { /* W: inv + page transferred */
+					ktime_t dt, fpin_end = ktime_get();
+					dt = ktime_sub(fpin_end, fpin_start);
+					atomic64_add(ktime_to_ns(dt), &fpin_ns);
+					atomic64_inc(&fpin_cnt);
+		}
+	}
+#endif
+
 	if (rp->result && rp->result != VM_FAULT_CONTINUE) {
-		if (rp->result != VM_FAULT_RETRY)
+		if (rp->result != VM_FAULT_RETRY) {
 			PGPRINTK("  [%d] failed 0x%x\n", current->pid, rp->result);
+		}
 		ret = rp->result;
 		pte_unmap(pte);
 		up_read(&mm->mmap_sem);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+		if (is_debug(addr)) {
+		//if ((current->origin_nid < 0 || current->origin_pid < 0) &&
+		//		tsk->at_remote) {
+			POPPK_GDSHM("  [dbg][%d] *** %lx *** "
+					"rp->result = ret %d (check origin)\n",
+					current->pid, addr, ret);
+		}
+#endif
 		goto out_free;
 	}
 
@@ -1700,6 +2595,19 @@ out_free:
 	fh->ret = ret;
 
 out_follower:
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	//if (interested_pid == current->pid) { // This pid memset and fail
+//	if (is_debug(addr)) {
+//		POPPK_GDSHM("  [dbg][%d] ***** %lx %c ***** exit ret %d\n",
+//				current->pid, addr,
+//				fault_for_write(fault_flags) ? 'W' : 'R', ret);
+//	}
+//	/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+//	if (is_debug(addr) || (ret && ret != VM_FAULT_RETRY)) {
+//		POPPK_GDSHM("  [dbg][%d] ***** %lx ***** end ret %d (is_worker %d)\n",
+//				current->pid, addr, ret, current->is_worker);
+//	}
+#endif
 	__finish_fault_handling(fh);
 	return ret;
 }
@@ -1710,9 +2618,40 @@ static bool __handle_copy_on_write(struct mm_struct *mm,
 		struct vm_area_struct *vma, unsigned long addr,
 		pte_t *pte, pte_t *pte_val, unsigned int fault_flags)
 {
-	if (vma_is_anonymous(vma) || fault_for_read(fault_flags)) return false;
-	BUG_ON(vma->vm_flags & VM_SHARED);
+	if (vma_is_anonymous(vma) || fault_for_read(fault_flags)) {
+#ifndef CONFIG_POPCORN_DSHM
+		POPPK_DSHM("%s(): addr %lx is_anonymous %d fault_4_read %d "
+				"vm_flag 0x%lx\n",
+				__func__, addr, !vma->vm_ops,
+				!!!(fault_flags & FAULT_FLAG_WRITE), vma->vm_flags);
+#endif
+		return false;
+	}
 
+#ifndef CONFIG_POPCORN_DSHM
+	BUG_ON(vma->vm_flags & VM_SHARED);
+#else
+	/* cow_file_at_origin is doing COW. We must skip it */
+	POPPK_DSHMV("%s(): addr %lx W " // many
+			"[WARN] pcn-DSHM needs VM_SHARED flag 0x00000008 (vm_flags 0x%lx) "
+			"do nothing, leave. pte_dirty(is_written) %c is_cowed %c\n",
+			__func__, addr, vma->vm_flags,
+			pte_dirty(*pte_val) ? 'O' : 'X', // written (O) so far so good
+			PageCowed(mm, addr) ? 'O' : 'X'); // is_cowd (O) so far so good
+	if(vma->vm_flags & VM_SHARED)
+		return false;
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	// Only interested add
+	else { // Since the former printk is too many, we need to catch coner cases to better catch problems
+		printk(KERN_ERR "\n\n\n\n%s(): addr %lx W (vm_flags 0x%lx) "
+			"pte_dirty(is_written) %c is_cowed %c\n\n\n\n\n",
+			__func__, addr, vma->vm_flags,
+			pte_dirty(*pte_val) ? 'O' : 'X', // written (O) so far so good
+			PageCowed(mm, addr) ? 'O' : 'X'); // is_cowd (O) so far so good
+		BUG();
+	}
+#endif
+#endif
 	/**
 	 * We need to determine whether the page is already cowed or not to
 	 * avoid unnecessary cows. But there is no explicit data structure that
@@ -1748,6 +2687,10 @@ static int __handle_localfault_at_origin(struct mm_struct *mm,
 
 	struct fault_handle *fh;
 	bool leader;
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	bool remote_fault = false;
+	//ktime_t ptef_start = ktime_get();
+#endif
 
 	ptl = pte_lockptr(mm, pmd);
 	spin_lock(ptl);
@@ -1811,9 +2754,23 @@ static int __handle_localfault_at_origin(struct mm_struct *mm,
 		}
 	} else {
 		struct page *page = vm_normal_page(vma, addr, pte_val);
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+		ktime_t dt, clr_end, clr_start = ktime_get();
+#endif
 		BUG_ON(!page);
 
-		__claim_remote_page(current, mm, vma, addr, fault_flags, page);
+#ifdef CONFIG_POPCORN_HYPE
+		/* In CONFIG_POPCORN_HYPE */
+		//__claim_remote_page may fail....
+#endif
+		__claim_remote_page(current, mm, vma, addr, fault_flags, page, 1);
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+		clr_end = ktime_get();
+		dt = ktime_sub(clr_end, clr_start);
+		atomic64_add(ktime_to_ns(dt), &clr_ns);
+		atomic64_inc(&clr_cnt);
+		remote_fault = true;
+#endif
 
 		spin_lock(ptl);
 		__make_pte_valid(mm, vma, addr, fault_flags, pte);
@@ -1826,6 +2783,15 @@ static int __handle_localfault_at_origin(struct mm_struct *mm,
 out_wakeup:
 	__finish_fault_handling(fh);
 
+#ifdef CONFIG_POPCORN_STAT_PGFAULTS
+	if (remote_fault) {
+		//ktime_t dt, ptef_end = ktime_get();
+		//dt = ktime_sub(ptef_end, ptef_start);
+		//atomic64_add(ktime_to_ns(dt), &ptef_ns);
+		//atomic64_inc(&ptef_cnt);
+	}
+#endif
+
 	return 0;
 }
 
@@ -1854,6 +2820,9 @@ int page_server_handle_pte_fault(
 {
 	unsigned long addr = address & PAGE_MASK;
 	int ret = 0;
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	static int interested_pid = -1; // This pid (vcpu1 at remote) memset and fail
+#endif
 
 	might_sleep();
 
@@ -1862,6 +2831,60 @@ int page_server_handle_pte_fault(
 			fault_for_write(fault_flags) ? 'W' : 'R',
 			instruction_pointer(current_pt_regs()),
 			fault_flags, pte_flags(pte_val));
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	if ((current->origin_nid < 0 || current->origin_pid < 0)) {
+	//if ((origin_nid < 0 || origin_pid < 0) && current->at_remote) {
+		printk("  THIS IS HARCODED. IF THIS WORKS. FIND A CORRECT SOLUTION\n");
+		printk("  [dbg] current->pid %d =? current->pid %d ??? "
+				"(at_remote ? yes : no - %d)\n",
+				current->pid, current->pid, current->at_remote);
+		printk("  [dbg][%d] rc->remote_tgids[0] = [origin's %d] ok??? \n",
+				current->pid, current->mm->remote->remote_tgids[0]);
+		printk("  [dbg][%d] _nid %d _pid %d (old)\n",
+				current->pid, current->origin_nid, current->origin_pid);
+		if (my_nid) {
+			current->origin_nid = 0;
+			//current->origin_pid = rc->remote_tgids[PCN_KMSG_FROM_NID(msg)];
+			current->origin_pid =
+					current->mm->remote->remote_tgids[current->origin_nid];
+		} else {
+			current->remote_nid = 1;
+			current->remote_pid =
+					current->mm->remote->remote_tgids[current->remote_nid];
+			/* This will get remote worker's pid */
+		}
+		printk("  [dbg][%d] _nid %d _pid [[[%d]]] (new)\n",
+				current->pid, current->origin_nid, current->origin_pid);
+	}
+
+	/* THIS IS FOR VCPU
+		which doesn't inherite Pophype main thread's task_struct metadata */
+	/* [dbg] for remote vcpu dies at the begining of guest kernel starts */
+	//if (addr == 0x7ffdc49e4000) {
+#if 0
+	if (!my_nid) {
+		if ((current->origin_nid < 0 || current->origin_pid < 0)) {
+			printk("  [dbg][%d] ****** %lx ******** start (is_worker %d)\n",
+					current->pid, addr, current->is_worker);
+
+			printk("  [dbg][%d] interested_pid updated to [%d]\n",
+					current->pid, current->pid);
+			interested_pid = current->pid;
+		}
+
+		//if (interested_pid == current->pid) { // This pid memset and fail
+		if (is_debug(addr)) {
+			POPPK_GDSHM("  [dbg][%d] *** %lx *** start\n",
+					current->pid, addr);
+		}
+	}
+#endif
+	if (is_debug(addr)) {
+		POPPK_GDSHM("  [dbg][%d] *** %lx %c *** start\n",
+				current->pid, addr,
+				fault_for_write(fault_flags) ? 'W' : 'R');
+	}
+#endif
 
 	/**
 	 * Thread at the origin
@@ -1882,6 +2905,8 @@ int page_server_handle_pte_fault(
 	if (pte_none(pte_val)) {
 		/* Can we handle the fault locally? */
 		if (vma->vm_flags & VM_EXEC) {
+			printk(KERN_ERR "\n\n\nDSHM shouldn't be a VM_EXEC region. "
+							"This addr/its vma is wrong.\n\n\n");
 			PGPRINTK("  [%d] VM_EXEC. continue\n", current->pid);
 			ret = VM_FAULT_CONTINUE;
 			goto out;
@@ -1915,6 +2940,14 @@ int page_server_handle_pte_fault(
 	ret = 0;
 
 out:
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	POPPK_DSHMV("[%d] #PAGEFAULT %lx ret %d\n", current->pid, address, ret);
+	if (is_debug(addr)) {
+		POPPK_GDSHM("  [dbg][%d] *** %lx %c *** exit ret %x\n",
+				current->pid, addr,
+				fault_for_write(fault_flags) ? 'W' : 'R', ret);
+	}
+#endif
 	trace_pgfault(my_nid, current->pid,
 			fault_for_write(fault_flags) ? 'W' : 'R',
 			instruction_pointer(current_pt_regs()), addr, ret);
diff --git a/kernel/popcorn/pcn_kmsg.c b/kernel/popcorn/pcn_kmsg.c
index 56fba5383890..13a18a81e851 100644
--- a/kernel/popcorn/pcn_kmsg.c
+++ b/kernel/popcorn/pcn_kmsg.c
@@ -46,29 +46,77 @@ EXPORT_SYMBOL(pcn_kmsg_unregister_callback);
 static atomic_t __nr_outstanding_requests[PCN_KMSG_TYPE_MAX] = { ATOMIC_INIT(0) };
 #endif
 
+#define ITERS 1000002
+#define ITER 1000000
 void pcn_kmsg_process(struct pcn_kmsg_message *msg)
 {
 	pcn_kmsg_cbftn ftn;
-
+//	static int cnt = 0;
+#ifdef CONFIG_POPCORN_STAT
+	//ktime_t dt1, t1e, t1s;
+	ktime_t t2e, t2s;
+	ktime_t t3e, t3s;
+	ktime_t t4e, t4s;
+	static long long t2 = 0, t3 = 0, t4 = 0;
+
+	t2s = ktime_get();
+#endif
 #ifdef CONFIG_POPCORN_CHECK_SANITY
 	BUG_ON(msg->header.type < 0 || msg->header.type >= PCN_KMSG_TYPE_MAX);
 	BUG_ON(msg->header.size < 0 || msg->header.size > PCN_KMSG_MAX_SIZE);
-	if (atomic_inc_return(__nr_outstanding_requests + msg->header.type) > 64) {
+	if (atomic_inc_return(__nr_outstanding_requests + msg->header.type) > 96) {
 		if (WARN_ON_ONCE("leaking received messages, ")) {
-			printk("type %d\n", msg->header.type);
+			//printk("type %d\n", msg->header.type);
 		}
 	}
 #endif
 	account_pcn_message_recv(msg);
+#ifdef CONFIG_POPCORN_STAT
+	t2e = ktime_get();
+	t2 += ktime_to_ns(ktime_sub(t2e, t2s));
 
+	t3s = ktime_get();
+#endif
 	ftn = pcn_kmsg_cbftns[msg->header.type];
+#ifdef CONFIG_POPCORN_STAT
+	t3e = ktime_get();
+	t3 += ktime_to_ns(ktime_sub(t3e, t3s));
 
+	t4s = ktime_get();
+#endif
 	if (ftn != NULL) {
 		ftn(msg);
 	} else {
 		printk(KERN_ERR"No callback registered for %d\n", msg->header.type);
 		pcn_kmsg_done(msg);
 	}
+#ifdef CONFIG_POPCORN_STAT
+	t4e = ktime_get();
+	t4 += ktime_to_ns(ktime_sub(t4e, t4s));
+#if 0
+	if (cnt <= 2 ) {
+		t2 = 0; t3 = 0; t4 = 0;
+	}
+
+	if (cnt >= ITERS) {
+		//printk("%s(): %d\n", __func__, cnt);
+		printk("%s(): t2 %lld ns %lld us!!!\n",
+						__func__,
+						t2 / ITER,
+						t2 / ITER / 1000);
+		printk("%s(): t3 %lld ns %lld us!!!\n",
+						__func__,
+						t3 / ITER,
+						t3 / ITER / 1000);
+		printk("%s(): t4 %lld ns %lld us!!!\n",
+						__func__,
+						t4 / ITER,
+						t4 / ITER / 1000);
+		// TODO jack
+	}
+#endif
+#endif
+
 }
 EXPORT_SYMBOL(pcn_kmsg_process);
 
@@ -76,6 +124,10 @@ EXPORT_SYMBOL(pcn_kmsg_process);
 static inline int __build_and_check_msg(enum pcn_kmsg_type type, int to, struct pcn_kmsg_message *msg, size_t size)
 {
 #ifdef CONFIG_POPCORN_CHECK_SANITY
+	if (to < 0 || to >= MAX_POPCORN_NODES) {
+		printk("\n\n\nCRASH: to = %d\n\n\n", to);
+		dump_stack();
+	}
 	BUG_ON(type < 0 || type >= PCN_KMSG_TYPE_MAX);
 	BUG_ON(size > PCN_KMSG_MAX_SIZE);
 	BUG_ON(to < 0 || to >= MAX_POPCORN_NODES);
diff --git a/kernel/popcorn/process_server.c b/kernel/popcorn/process_server.c
index 679cc3530b05..2590face83d0 100644
--- a/kernel/popcorn/process_server.c
+++ b/kernel/popcorn/process_server.c
@@ -2,7 +2,7 @@
  * @file process_server.c
  *
  * Popcorn Linux thread migration implementation
- * This work was an extension of David Katz MS Thesis, but totally rewritten 
+ * This work was an extension of David Katz MS Thesis, but totally rewritten
  * by Sang-Hoon to support multithread environment.
  *
  * @author Sang-Hoon Kim, SSRG Virginia Tech 2017
@@ -27,6 +27,8 @@
 #include <popcorn/types.h>
 #include <popcorn/bundle.h>
 #include <popcorn/cpuinfo.h>
+#include <popcorn/process_server.h>
+#include <popcorn/vma_server.h>
 
 #include "types.h"
 #include "process_server.h"
@@ -35,16 +37,11 @@
 #include "wait_station.h"
 #include "util.h"
 
-static struct list_head remote_contexts[2];
-static spinlock_t remote_contexts_lock[2];
-
-enum {
-	INDEX_OUTBOUND = 0,
-	INDEX_INBOUND = 1,
-};
+struct list_head remote_contexts[2];
+spinlock_t remote_contexts_lock[2];
 
 /* Hold the correnponding remote_contexts_lock */
-static struct remote_context *__lookup_remote_contexts_in(int nid, int tgid)
+struct remote_context *__lookup_remote_contexts_in(int nid, int tgid)
 {
 	struct remote_context *rc;
 
@@ -56,41 +53,34 @@ static struct remote_context *__lookup_remote_contexts_in(int nid, int tgid)
 	return NULL;
 }
 
-#define __lock_remote_contexts(index) \
-	spin_lock(remote_contexts_lock + index)
-#define __lock_remote_contexts_in(nid) \
-	__lock_remote_contexts(INDEX_INBOUND)
-#define __lock_remote_contexts_out(nid) \
-	__lock_remote_contexts(INDEX_OUTBOUND)
-
-#define __unlock_remote_contexts(index) \
-	spin_unlock(remote_contexts_lock + index)
-#define __unlock_remote_contexts_in(nid) \
-	__unlock_remote_contexts(INDEX_INBOUND)
-#define __unlock_remote_contexts_out(nid) \
-	__unlock_remote_contexts(INDEX_OUTBOUND)
-
-#define __remote_contexts_in() remote_contexts[INDEX_INBOUND]
-#define __remote_contexts_out() remote_contexts[INDEX_OUTBOUND]
-
-
 inline struct remote_context *__get_mm_remote(struct mm_struct *mm)
 {
 	struct remote_context *rc = mm->remote;
 	atomic_inc(&rc->count);
+//#ifdef CONFIG_POPCORN_CHECK_SANITY
+	POPPK_DSHMV("[%d] %s(): rc cnt %d (+1)\n",
+				current->pid, __func__, atomic_read(&rc->count));
+//#endif
 	return rc;
 }
 
-inline struct remote_context *get_task_remote(struct task_struct *tsk)
+struct remote_context *get_task_remote(struct task_struct *tsk)
 {
 	return __get_mm_remote(tsk->mm);
 }
 
 inline bool __put_task_remote(struct remote_context *rc)
 {
+	POPPK_DSHMV("[%d] %s(): rc cnt %d -1\n",
+				current->pid, __func__, atomic_read(&rc->count));
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	BUG_ON(!rc);
+#endif
 	if (!atomic_dec_and_test(&rc->count)) return false;
 
 	__lock_remote_contexts(rc->for_remote);
+	POPPK_DSHMV("[%d] %s(): rc cnt %d (-1)\n",
+				current->pid, __func__, atomic_read(&rc->count));
 #ifdef CONFIG_POPCORN_CHECK_SANITY
 	BUG_ON(atomic_read(&rc->count));
 #endif
@@ -109,13 +99,16 @@ inline bool put_task_remote(struct task_struct *tsk)
 
 void free_remote_context(struct remote_context *rc)
 {
+	/* This is from mmput(), so it's before rc release. */
+	POPPK_DSHMV("[%d] %s(): rc cnt %d (need=1/2)\n",
+				current->pid, __func__, atomic_read(&rc->count));
 #ifdef CONFIG_POPCORN_CHECK_SANITY
 	BUG_ON(atomic_read(&rc->count) != 1 && atomic_read(&rc->count) != 2);
 #endif
 	__put_task_remote(rc);
 }
 
-static struct remote_context *__alloc_remote_context(int nid, int tgid, bool remote)
+struct remote_context *alloc_remote_context(int nid, int tgid, bool remote)
 {
 	struct remote_context *rc = kmalloc(sizeof(*rc), GFP_KERNEL);
 	int i;
@@ -291,8 +284,17 @@ static int __exit_origin_task(struct task_struct *tsk)
 	 * Trigger peer termination if this is the last user thread
 	 * referring to this mm.
 	 */
+	POPPK_DSHM("%s(): origin exit [%d] => origin worker [%d]\n",
+							__func__, tsk->pid, tsk->origin_nid);
 	if (atomic_read(&tsk->mm->mm_users) == 1) {
+		POPPK_DSHM("[%d] last one. Terminate remotes\n",
+					tsk->pid);
+#ifdef CONFIG_POPCORN_DSHM
+		POPPK_DSHM("[%d] last one. Skip terminating remotes.\n",
+					tsk->pid);
+#else
 		__terminate_remotes(rc);
+#endif
 	}
 
 	return 0;
@@ -310,12 +312,28 @@ static int __exit_remote_task(struct task_struct *tsk)
 				.remote_pid = tsk->pid,
 				.exit_code = tsk->exit_code,
 			};
+
+			POPPK_DSHM("%s(): remote exit [%d] => origin worker [%d]\n",
+									__func__, tsk->pid, tsk->origin_nid);
+#ifdef CONFIG_POPCORN_DSHM
+			POPPK_DSHM("[%d] Skip notifying/terminating origin.\n",
+						tsk->pid);
+#else
 			pcn_kmsg_send(PCN_KMSG_TYPE_TASK_EXIT_REMOTE,
 					tsk->origin_nid, &req, sizeof(req));
+#endif
 		}
+#ifdef CONFIG_POPCORN_DSHM
+		POPPK_DSHM("[%d] %s(): put_task_remote 1 for msg send\n",
+					tsk->pid, __func__);
+#endif
 		put_task_remote(tsk);
 	}
 
+#ifdef CONFIG_POPCORN_DSHM
+	POPPK_DSHM("[%d] %s(): put_task_remote 2\n",
+				tsk->pid, __func__);
+#endif
 	put_task_remote(tsk);
 	tsk->remote = NULL;
 	tsk->origin_nid = tsk->origin_pid = -1;
@@ -323,20 +341,67 @@ static int __exit_remote_task(struct task_struct *tsk)
 	return 0;
 }
 
+/* From exit_mm() ./kernel/exit.c */
 int process_server_task_exit(struct task_struct *tsk)
 {
 	WARN_ON(tsk != current);
 
 	if (!distributed_process(tsk)) return -ESRCH;
 
+#if defined(CONFIG_POPCORN_DSHM)
+#if defined(CONFIG_POPCORN_CHECK_SANITY)
+	//dump_stack();
+    POPPK_DSHMVV("[%d] %s(): pophype EXITED only exit locally "
+			"(comm \"%s\" ins %lx) current->_nid %d _pid %d "
+			"at_remote %d is_worker %d rc %p exit_code [0x%x]\n",
+			tsk->pid, __func__,
+			tsk->comm, instruction_pointer(current_pt_regs()),
+			tsk->origin_nid, tsk->origin_pid,
+			tsk->at_remote, tsk->is_worker,
+			tsk->mm ? tsk->mm->remote : NULL,
+			tsk->exit_code);
+			//); // == remote_nid/pid
+	{
+		struct task_struct *p;
+		POPPK_DSHMVV("[%d]: show thread list "
+					"(change task_struct's popcorn meta data) ->\n",
+					current->pid);
+		rcu_read_lock();
+		//for_each_process_thread(g, p) {
+		for_each_thread(current, p) {
+			POPPK_DSHMVV("\t\t[%d] p->_nid %d _pid %d\n",
+					p->pid, p->origin_nid, p->origin_pid);
+			/*
+			if (!my_nid) {
+				p->at_remote = false;
+			} else {
+				p->at_remote = true;
+			}*/
+		}
+		rcu_read_unlock();
+	}
+#endif
+	/* ################## EXIT ############################ */
+	return 0; // return to local exit
+#endif
+
 	PSPRINTK("EXITED [%d] %s%s / 0x%x\n", tsk->pid,
-			tsk->at_remote ? "remote" : "local",
-			tsk->is_worker ? " worker": "",
+			tsk->at_remote ? "[remote]" : "[origin]",
+			tsk->is_worker ? " [worker]": " [user]",
 			tsk->exit_code);
 
 	// show_regs(task_pt_regs(tsk));
 
-	if (tsk->is_worker) return 0;
+#if !defined(CONFIG_POPCORN_DSHM)
+	if (tsk->is_worker) {
+		return 0;
+	}
+#else
+	POPPK_DSHM("\n\n%s(): [%d] EXITED EXITED EXITED EXITED "
+				"is_worker %d\n\n\n",
+				__func__, tsk->pid, tsk->is_worker);
+	//dump_stack();
+#endif
 
 	if (tsk->at_remote) {
 		return __exit_remote_task(tsk);
@@ -354,6 +419,21 @@ static void process_remote_task_exit(remote_task_exit_t *req)
 	struct task_struct *tsk = current;
 	int exit_code = req->exit_code;
 
+#ifdef CONFIG_POPCORN_DSHM
+	POPPK_DSHM("%s(): I'm a worker I need to tell real app to exit\n",
+						__func__);
+
+	POPPK_DSHM("%s(): do tsk = user's tgid\n",
+									__func__);
+
+	POPPK_DSHM("%s(): check how remote_worker let remote process to do "
+				" do_exit()\n",
+				__func__);
+
+	POPPK_DSHM("%s(): DSHM problem1: remote process can exit\n",
+														__func__);
+#endif
+
 	if (tsk->remote_pid != req->remote_pid) {
 		printk(KERN_INFO"%s: pid mismatch %d != %d\n", __func__,
 				tsk->remote_pid, req->remote_pid);
@@ -629,8 +709,11 @@ static void __terminate_remote_threads(struct remote_context *rc)
 	rcu_read_unlock();
 }
 
-static void __run_remote_worker(struct remote_context *rc)
+void run_remote_worker(struct remote_context *rc)
 {
+	POPPK_DSHM("%s(): TODO remote kworker need your attention\n", __func__);
+	BUG_ON(!current->at_remote);
+
 	while (!rc->stop_remote_worker) {
 		struct work_struct *work = NULL;
 		struct pcn_kmsg_message *msg;
@@ -651,17 +734,45 @@ static void __run_remote_worker(struct remote_context *rc)
 		if (!work) continue;
 
 		msg = ((struct pcn_kmsg_work *)work)->msg;
+		POPPK_DSHM("  [%d] %s(): remote worker got a tsk 0x%x, waken up %p\n",
+			current->pid, __func__, msg->header.type, &rc->remote_works_ready);
 
 		switch (msg->header.type) {
 		case PCN_KMSG_TYPE_TASK_MIGRATE:
 			__fork_remote_thread((clone_request_t *)msg);
 			break;
 		case PCN_KMSG_TYPE_VMA_OP_REQUEST:
-			process_vma_op_request((vma_op_request_t *)msg);
+			POPPK_DSHM("%s(): PCN_KMSG_TYPE_VMA_OP_REQUEST: "
+					"TODO current needs your attention\n", __func__);
+			process_vma_op_request((vma_op_request_t *)msg, current);
 			break;
 		case PCN_KMSG_TYPE_TASK_EXIT_ORIGIN:
 			process_origin_task_exit(rc, (origin_task_exit_t *)msg);
 			break;
+#ifdef CONFIG_POPCORN_DSHM
+		case PCN_KMSG_TYPE_UPDATE_ORIGIN_WORKER_PID:
+			/* at remote */
+			POPPK_DSHM("%s(): PCN_KMSG_TYPE_UPDATE_ORIGIN_WORKER_PID\n", __func__);
+			current->origin_pid = ((update_origin_worker_pid_t *)msg)->ttid;
+			rc->remote_tgids[PCN_KMSG_FROM_NID(msg)] =
+							((update_origin_worker_pid_t *)msg)->ttid;
+			// rc->remote_tgids[PCN_KMSG_FROM_NID(msg)] = ((update_origin_worker_pid_t *)msg)->ttid;
+			current->tgid = rc->remote_tgids[current->origin_nid];
+			rc->tgid = rc->remote_tgids[current->origin_nid];
+			POPPK_DSHM("\n!!!!!![dbg] %s(): current->remote %p (hack) rc %p\n\n",
+						__func__, current->remote, rc);
+			POPPK_DSHM("\n%s(): node[%d]'s worker's pid updated to [[[[%d]]]]\n\n",
+						__func__, PCN_KMSG_FROM_NID(msg),
+						((update_origin_worker_pid_t *)msg)->ttid);
+			pcn_kmsg_done(msg);
+			break;
+		case PCN_KMSG_TYPE_STOP_WORKER:
+			POPPK_DSHM("%s(): PCN_KMSG_TYPE_STOP_WORKER\n", __func__);
+			rc->stop_remote_worker = true;
+			kfree(msg);
+			//pcn_kmsg_done(msg);
+			break;
+#endif
 		default:
 			printk("Unknown remote work type %d\n", msg->header.type);
 			break;
@@ -718,8 +829,8 @@ static int remote_worker_main(void *data)
 
 	get_task_remote(current);
 	rc->tgid = current->tgid;
-	
-	__run_remote_worker(rc);
+
+	run_remote_worker(rc);
 
 	__terminate_remote_threads(rc);
 
@@ -751,7 +862,7 @@ static void clone_remote_thread(struct work_struct *_work)
 	int tgid_from = req->origin_tgid;
 	struct remote_context *rc;
 	struct remote_context *rc_new =
-			__alloc_remote_context(nid_from, tgid_from, true);
+			alloc_remote_context(nid_from, tgid_from, true);
 
 	BUG_ON(!rc_new);
 
@@ -806,10 +917,19 @@ int request_remote_work(pid_t pid, struct pcn_kmsg_message *req)
 {
 	struct task_struct *tsk = __get_task_struct(pid);
 	int ret = -ESRCH;
-	if (!tsk) {
+	if (unlikely(!tsk)) {
+		int i = 0;
 		printk(KERN_INFO"%s: invalid origin task %d for remote work %d\n",
 				__func__, pid, req->header.type);
-		goto out_err;
+		WARN_ON("trying to fix");
+		while (!tsk) {
+			if (++i > 100) BUG();
+			tsk = __get_task_struct(pid);
+			io_schedule();
+		}
+		printk(KERN_INFO"%s: fixed origin task %d for remote work %d\n",
+										__func__, pid, req->header.type);
+		//goto out_err;
 	}
 
 	/**
@@ -821,43 +941,73 @@ int request_remote_work(pid_t pid, struct pcn_kmsg_message *req)
 	if (tsk->at_remote) {
 		struct remote_context *rc = get_task_remote(tsk);
 		struct pcn_kmsg_work *work = kmalloc(sizeof(*work), GFP_ATOMIC);
-
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		BUG_ON(!rc);
+		BUG_ON(!work);
+#endif
+#ifdef CONFIG_POPCORN_DSHM
+		POPPK_DSHM("\n\n => remote worker got a req (DEFINE_KMSG_RW_HANDLER) "
+					"e.g. EXIT "
+					//"Popcorn_DSHM doesn't create kthread main worker at remote. "
+					//"So DO NOTHING.\n"
+					"tsk %p [%d] rc %p"
+					"\n\n", tsk, tsk->pid, rc);
+
+		POPPK_DSHM("Popcorn DSHM leverage process's ttid not kthead's tid. "
+					"This breaks the Popcorn's original design.\n");
+
+		POPPK_DSHM(" [dbg][using tsk %d] %s(): tsk->is_worker %d (1: good)\n",
+					tsk->pid, __func__, tsk->is_worker);
+#else
 		BUG_ON(!tsk->is_worker);
+#endif
 		work->msg = req;
 
 		__schedule_remote_work(rc, work);
 
 		__put_task_remote(rc);
 	} else {
-		BUG_ON(tsk->remote_work);
-		tsk->remote_work = req;
-		complete(&tsk->remote_work_pended); /* implicit memory barrier */
+		WARN_ON(tsk->remote->remote_work); // something going on
+		while (tsk->remote->remote_work) // Jack DEX BUG FIX
+			; // Jack DEX BUG FIX
+		tsk->remote->remote_work = req;
+		POPPK_DSHM("%s(): origin worker got a tsk 0x%x, complete %p (tsk %p rc %p)\n",
+					__func__, req->header.type, &tsk->remote->remote_work_pended,
+					tsk, tsk->remote);
+		complete(&tsk->remote->remote_work_pended); /* implicit memory barrier */
 	}
 
 	put_task_struct(tsk);
 	return 0;
 
-out_err:
+//out_err:
+	/* Handlers should do this */
 	pcn_kmsg_done(req);
 	return ret;
 }
 
-static void __process_remote_works(void)
+void process_remote_works(struct task_struct *tsk)
 {
 	bool run = true;
-	BUG_ON(current->at_remote);
+	BUG_ON(tsk->at_remote);
 
 	while (run) {
 		struct pcn_kmsg_message *req;
 		long ret;
+		/* DSHM moves remot_work(_pended) to rc */
+		//POPPK_DSHM("  [%d][dbg] %s(): origin worker waits at %p (tsk %p rc %p)\n", // many
+		//			tsk->pid, __func__, &tsk->remote->remote_work_pended,
+		//			tsk, tsk->remote); // nonblocking
 		ret = wait_for_completion_interruptible_timeout(
-				&current->remote_work_pended, HZ);
+				&tsk->remote->remote_work_pended, HZ);
 		if (ret == 0) continue; /* timeout */
 
-		req = (struct pcn_kmsg_message *)current->remote_work;
-		current->remote_work = NULL;
+		req = (struct pcn_kmsg_message *)tsk->remote->remote_work;
+		tsk->remote->remote_work = NULL;
 		smp_wmb();
 
+		POPPK_DSHM("  [%d] %s(): origin worker got a tsk 0x%x, waken up %p\n",
+			tsk->pid, __func__, req->header.type, &tsk->remote->remote_work_pended);
 		if (!req) continue;
 
 		switch (req->header.type) {
@@ -865,22 +1015,60 @@ static void __process_remote_works(void)
 			WARN_ON_ONCE("Not implemented yet!");
 			break;
 		case PCN_KMSG_TYPE_VMA_OP_REQUEST:
-			process_vma_op_request((vma_op_request_t *)req);
+			POPPK_DSHM("%s(): PCN_KMSG_TYPE_VMA_OP_REQUEST\n", __func__);
+			process_vma_op_request((vma_op_request_t *)req, tsk);
 			break;
 		case PCN_KMSG_TYPE_VMA_INFO_REQUEST:
-			process_vma_info_request((vma_info_request_t *)req);
+			POPPK_DSHM("%s(): PCN_KMSG_TYPE_VMA_INFO_REQUEST\n", __func__);
+			process_vma_info_request((vma_info_request_t *)req, tsk);
 			break;
 		case PCN_KMSG_TYPE_FUTEX_REQUEST:
+#ifdef CONFIG_POPCORN_DSHM
+			POPPK_DSHM("%s(): WARNING check current->remote_nid %d\n",
+						__func__, current->remote_nid);
+#endif
 			process_remote_futex_request((remote_futex_request *)req);
 			break;
 		case PCN_KMSG_TYPE_TASK_EXIT_REMOTE:
+#ifdef CONFIG_POPCORN_DSHM
+			POPPK_DSHM("%s(): WARNING use current: may sig term to current\n",
+						__func__);
+			/* at origin got EXIT request */
+#endif
 			process_remote_task_exit((remote_task_exit_t *)req);
 			run = false;
 			break;
 		case PCN_KMSG_TYPE_TASK_MIGRATE_BACK:
+#ifdef CONFIG_POPCORN_DSHM
+			BUG();
+#endif
 			process_back_migration((back_migration_request_t *)req);
 			run = false;
 			break;
+#ifdef CONFIG_POPCORN_DSHM
+		case PCN_KMSG_TYPE_UPDATE_REMOTE_WORKER_PID:
+			/* at origin */
+			/* Should I use current instead of tsk? Trying now
+				caller is current, current's parent is tsk (testing version: this is not true caller use current only) */
+			POPPK_DSHM("%s(): PCN_KMSG_TYPE_UPDATE_REMOTE_WORKER_PID\n", __func__);
+			tsk->remote_pid = ((update_remote_worker_pid_t *)req)->ttid;
+			tsk->remote->remote_tgids[PCN_KMSG_FROM_NID(req)] = ((update_remote_worker_pid_t *)req)->ttid;
+			current->remote_pid = ((update_remote_worker_pid_t *)req)->ttid; // hack
+			//current->remote->remote_tgids[PCN_KMSG_FROM_NID(req)] = ((update_remote_worker_pid_t *)req)->ttid; // hack
+			POPPK_DSHM("[dbg] %s(): current->remote_pid %d tsk->remote_pid %d\n",
+						__func__, current->remote_pid, tsk->remote_pid);
+			POPPK_DSHM("\n%s(): node[%d]'s worker's pid updated to [[[[%d]]]]\n\n",
+						__func__, PCN_KMSG_FROM_NID(req),
+						((update_remote_worker_pid_t *)req)->ttid);
+			pcn_kmsg_done(req);
+			break;
+		case PCN_KMSG_TYPE_STOP_WORKER:
+			POPPK_DSHM("%s(): PCN_KMSG_TYPE_STOP_WORKER\n", __func__);
+			run = false;
+			kfree(req);
+			//pcn_kmsg_done(req);
+			break;
+#endif
 		default:
 			if (WARN_ON("Received unsupported remote work")) {
 				printk("  type: %d\n", req->header.type);
@@ -964,7 +1152,7 @@ static int __do_migration(struct task_struct *tsk, int dst_nid, void __user *ure
 	struct remote_context *rc;
 
 	/* Won't to allocate this object in a spinlock-ed area */
-	rc = __alloc_remote_context(my_nid, tsk->tgid, false);
+	rc = alloc_remote_context(my_nid, tsk->tgid, false);
 	if (IS_ERR(rc)) return PTR_ERR(rc);
 
 	if (cmpxchg(&tsk->mm->remote, 0, rc)) {
@@ -991,7 +1179,7 @@ static int __do_migration(struct task_struct *tsk, int dst_nid, void __user *ure
 	ret = __request_clone_remote(dst_nid, tsk, uregs);
 	if (ret) return ret;
 
-	__process_remote_works();
+	process_remote_works(current);
 	return 0;
 }
 
@@ -1022,10 +1210,484 @@ int process_server_do_migration(struct task_struct *tsk, unsigned int dst_nid, v
 }
 
 
+#ifdef CONFIG_POPCORN_DSHM
+//struct task_struct *get_task_struct(pid_t pid) {
+//	return __get_task_struct(pid);
+//}
+/* Ask remote to tell me its ttid.
+	Each node installs its ttid when calling the same function.
+
+	Another way is to let me send my from_pid and remote will remember it.
+	This assumes the invoker will quit last.
+*/
+
+/* Caution: Support only 1 shared memory region.
+	To improve, pcn_mmap_join needs to have a seq# (auto increment/user define) */
+pid_t gttid = 0;
+unsigned long dshm_addr = 0;
+unsigned long dshm_len = 0;
+void pcn_dshm_join(pid_t ttid, unsigned long addr, unsigned long len)
+{
+	// remote_kvm_create_request_t -> dshm_join_request_t
+	dshm_join_response_t *res;
+	dshm_join_request_t *req = kmalloc(sizeof(*req), GFP_KERNEL);
+	struct wait_station *ws = get_wait_station(current);
+	int dst_nid = my_nid ? 0 : 1; // 2node now
+	//struct task_struct *tsk; // for testing debuggin
+	struct remote_context *rc = current->mm->remote;
+	BUG_ON(!rc || !req);
+
+
+	/* Create worker thread and rc. Make it as a Popcorn distributed thread */
+	POPPK_DSHM("\n[%d] %s(): Create worker thread and rc. "
+				"Make it as a Popcorn distributed thread\n",
+								current->pid, __func__);
+
+	POPPK_DSHM("[%d] MSG rc %p\n", current->pid, rc);
+
+	// TODO hand shake set up remote_tgids[0]
+	//printk("TODO rc->remote_tgids[0]\n");
+
+	req->ws = ws->id;
+	req->from_pid = current->pid;
+
+	// only 2nd entry knows
+	//if (my_nid)
+	//	req->target_pid = rc->remote_tgids[0]; // lookup the pid on target node
+	gttid = ttid;
+	POPPK_DSHM("[%d] MSG req->target_pid = rc->remote_tgids[0] %d\n",
+								current->pid, rc->remote_tgids[0]);
+	POPPK_DSHM("[%d] MSG req->target_pid = rc->remote_tgids[1] %d\n",
+								current->pid, rc->remote_tgids[1]);
+
+	// 0		1...
+	// save at global (and aslo for sync)
+	//    -> who?
+	//	who? <=
+	//		 =>
+	//			wake_up()
+	//				rc->remote_tgids[0] = ttid
+	//    <-
+	//	wake_up()
+	// rc->remote_tgids[from_nid] = ttid
+
+	/* Customized */
+	req->addr = addr; // notused
+	//req->ttid = ttid;
+	POPPK_DSHM("[%d] my addr 0x%lx len 0x%lx ttid=gttid [[[%d]]]\n",
+				current->pid, addr, len, ttid);
+	dshm_addr = addr;
+	dshm_len = len;
+	POPPK_DSHM("[%d] my dshm_addr 0x%lx dshm_len 0x%lx\n",
+				current->pid, dshm_addr, dshm_len);
+
+
+	// TODO
+	// test can ttid get the original current/current->mm ???
+	// test can ttid get the original current/current->mm ???
+	// test can ttid get the original current/current->mm ???
+	//POPPK_DSHM("[%d] self testing addr 0x%lx ttid %d\n", current->pid, addr, ttid);
+	//tsk = __get_task_struct(ttid); // future, sender need to send me this ttid
+	//BUG_ON(!tsk);
+	//POPPK_DSHM("[%d] current %p tsk %p\n", current->pid, current, tsk);
+	//POPPK_DSHM("[%d] current->mm %p tsk-> %p\n", current->pid, current->mm, tsk->mm);
+
+
+	//POPPK_DSHM("[%d] #working# [[kvm_create]]- delegating to origin "
+	//					"req->type %lu\n", current->pid, req->type);
+	pcn_kmsg_send(PCN_KMSG_TYPE_DSHM_JOIN_REQUEST,
+								dst_nid, req, sizeof(*req));
+	res = wait_at_station(ws);
+
+	// load it
+	POPPK_DSHM("[%d] Got remote ttid [[[%d]]]\n", current->pid, res->ttid);
+	rc->remote_tgids[dst_nid] = res->ttid;
+	if (!my_nid) {
+		current->remote_pid = res->ttid; // remote's pid e.g. used at exit
+	} else {
+		current->origin_pid = res->ttid; // remote's pid e.g. used at exit
+	}
+
+	kfree(req);
+	pcn_kmsg_done(res);
+	return;
+}
+
+static void process_dshm_join_request(struct work_struct *work)
+{
+	// remote_kvm_create_request_t -> dshm_join_request_t
+	// remote_kvm_create_response_t -> dshm_join_response_t
+    START_KMSG_WORK(dshm_join_request_t, req, work);
+    dshm_join_response_t *res = pcn_kmsg_get(sizeof(*res));
+    int from_nid = PCN_KMSG_FROM_NID(req);
+	// only 2nd entry knows
+    //struct task_struct *tsk = __get_task_struct(req->target_pid); // TODO BUG
+
+    //unsigned long addr = req->addr; // not used // current join implementation doesn't take addr into consideration
+	//pid_t ttid = req->ttid;
+
+    POPPK_DSHM(" => [from %d]\n", req->from_pid);
+    //POPPK_DSHM("[from%d/origin%d]#working# [[kvm_create]] at origin\n",
+    //                                    req->from_pid, req->target_pid);
+
+    //BUG_ON(!tsk && "No task exist");
+    //BUG_ON(tsk->at_remote);
+
+    //POPPK_DSHM("[from %d / origin %d]\n", req->from_pid, tsk->pid);
+
+    //POPPK_DSHM(" => got ttid %d iderationddr 0x%lx\n", ttid, addr);
+    /* remote is trying to create a vm */
+    //res->fd = replay_kvm_dev_ioctl_create_vm_tsk(tsk, type);
+    POPPK_DSHM(" => [from %d] wating for my node to join\n", req->from_pid);
+	while(!gttid) {
+		schedule();
+	}
+    POPPK_DSHM(" => [from %d] My node has joined. My ttid = %d\n",
+											req->from_pid, gttid);
+	res->ttid = gttid;
+
+	// res
+    //res->from_pid = req->from_pid;
+    res->ws = req->ws;
+
+    //POPPK_DSHM("#working# [[kvm_create]] at origin DONE fd %d ->\n\n",
+    //                                            res->fd);
+    pcn_kmsg_post(PCN_KMSG_TYPE_DSHM_JOIN_RESPONSE,
+                            from_nid, res, sizeof(*res));
+    END_KMSG_WORK(req);
+}
+
+static int handle_dshm_join_response(struct pcn_kmsg_message *msg)
+{
+    dshm_join_response_t *res = (dshm_join_response_t *)msg;
+    struct wait_station *ws = wait_station(res->ws);
+
+    ws->private = res;
+
+    complete(&ws->pendings);
+    return 0;
+}
+
+void pcn_dshm_join_update(pid_t ttid, unsigned long addr, unsigned long len)
+{
+	// remote_kvm_create_request_t -> dshm_join_update_request_t
+	dshm_join_update_response_t *res;
+	dshm_join_update_request_t *req = kmalloc(sizeof(*req), GFP_KERNEL);
+	struct wait_station *ws = get_wait_station(current);
+	int dst_nid = my_nid ? 0 : 1; // 2node now
+	//struct task_struct *tsk; // for testing debuggin
+	struct remote_context *rc = current->mm->remote;
+	BUG_ON(!rc || !req);
+
+
+	/* Create worker thread and rc. Make it as a Popcorn distributed thread */
+	POPPK_DSHM("\n[%d] %s(): Create worker thread and rc. "
+				"Make it as a Popcorn distributed thread\n",
+								current->pid, __func__);
+
+	POPPK_DSHM("[%d] MSG rc %p\n", current->pid, rc);
+
+	// TODO hand shake set up remote_tgids[0]
+	//printk("TODO rc->remote_tgids[0]\n");
+
+	req->ws = ws->id;
+	req->from_pid = current->pid;
+
+	// only 2nd entry knows
+	//if (my_nid)
+	//	req->target_pid = rc->remote_tgids[0]; // lookup the pid on target node
+	gttid = ttid;
+	POPPK_DSHM("[%d] MSG req->target_pid = rc->remote_tgids[0] %d\n",
+								current->pid, rc->remote_tgids[0]);
+	POPPK_DSHM("[%d] MSG req->target_pid = rc->remote_tgids[1] %d\n",
+								current->pid, rc->remote_tgids[1]);
+
+
+
+
+	// 0		1...
+	// save at global (and aslo for sync)
+	//    -> who?
+	//	who? <=
+	//		 =>
+	//			wake_up()
+	//				rc->remote_tgids[0] = ttid
+	//    <-
+	//	wake_up()
+	// rc->remote_tgids[from_nid] = ttid
+
+	/* Customized */
+    req->addr = addr; // notused
+	//req->ttid = ttid;
+	POPPK_DSHM("[%d] my addr 0x%lx len 0x%lx ttid=gttid [[[%d]]]\n",
+				current->pid, addr, len, ttid);
+	dshm_addr = addr;
+	dshm_len = len;
+	POPPK_DSHM("[%d] my dshm_addr 0x%lx dshm_len 0x%lx\n",
+				current->pid, dshm_addr, dshm_len);
+
+
+	// TODO
+	// test can ttid get the original current/current->mm ???
+	// test can ttid get the original current/current->mm ???
+	// test can ttid get the original current/current->mm ???
+	//POPPK_DSHM("[%d] self testing addr 0x%lx ttid %d\n", current->pid, addr, ttid);
+	//tsk = __get_task_struct(ttid); // future, sender need to send me this ttid
+	//BUG_ON(!tsk);
+	//POPPK_DSHM("[%d] current %p tsk %p\n", current->pid, current, tsk);
+	//POPPK_DSHM("[%d] current->mm %p tsk-> %p\n", current->pid, current->mm, tsk->mm);
+
+
+	//POPPK_DSHM("[%d] #working# [[kvm_create]]- delegating to origin "
+	//					"req->type %lu\n", current->pid, req->type);
+	pcn_kmsg_send(PCN_KMSG_TYPE_DSHM_JOIN_REQUEST,
+								dst_nid, req, sizeof(*req));
+	res = wait_at_station(ws);
+
+	// load it
+	POPPK_DSHM("[%d] Got remote ttid [[[%d]]]\n", current->pid, res->ttid);
+	rc->remote_tgids[dst_nid] = res->ttid;
+	current->remote_pid = res->ttid; // remote's pid e.g. used at exit
+
+	kfree(req);
+	pcn_kmsg_done(res);
+	return;
+}
+
+static void process_dshm_join_update_request(struct work_struct *work)
+{
+	// remote_kvm_create_request_t -> dshm_join_update_request_t
+	// remote_kvm_create_response_t -> dshm_join_update_response_t
+    START_KMSG_WORK(dshm_join_update_request_t, req, work);
+    dshm_join_update_response_t *res = pcn_kmsg_get(sizeof(*res));
+    int from_nid = PCN_KMSG_FROM_NID(req);
+	// only 2nd entry knows
+    //struct task_struct *tsk = __get_task_struct(req->target_pid); // TODO BUG
+
+    //unsigned long addr = req->addr; // not used // current join implementation doesn't take addr into consideration
+	//pid_t ttid = req->ttid;
+
+    POPPK_DSHM(" => [from %d]\n", req->from_pid);
+    //POPPK_DSHM("[from %d / origin %d]#working# [[kvm_create]] at origin\n",
+    //                                    req->from_pid, req->target_pid);
+
+    //BUG_ON(!tsk && "No task exist");
+    //BUG_ON(tsk->at_remote);
+
+    //POPPK_DSHM("[from %d / origin %d]\n", req->from_pid, tsk->pid);
+
+    //POPPK_DSHM(" => got ttid %d iderationddr 0x%lx\n", ttid, addr);
+    /* remote is trying to create a vm */
+    //res->fd = replay_kvm_dev_ioctl_create_vm_tsk(tsk, type);
+    POPPK_DSHM(" => [from %d] wating for my node to join\n", req->from_pid);
+	while(!gttid) {
+		schedule();
+	}
+    POPPK_DSHM(" => [from %d] My node has joined. My ttid = %d\n",
+											req->from_pid, gttid);
+	res->ttid = gttid;
+
+	// res
+    //res->from_pid = req->from_pid;
+    res->ws = req->ws;
+
+    //POPPK_DSHM("#working# [[kvm_create]] at origin DONE fd %d ->\n\n",
+    //                                            res->fd);
+    pcn_kmsg_post(PCN_KMSG_TYPE_DSHM_JOIN_RESPONSE,
+                            from_nid, res, sizeof(*res));
+    END_KMSG_WORK(req);
+}
+
+static int handle_dshm_join_update_response(struct pcn_kmsg_message *msg)
+{
+    dshm_join_response_t *res = (dshm_join_response_t *)msg;
+    struct wait_station *ws = wait_station(res->ws);
+
+    ws->private = res;
+
+    complete(&ws->pendings);
+    return 0;
+}
+
+
+/* Popcorn global barrier */
+atomic64_t popcorn_global_local_cnt = ATOMIC64_INIT(0);
+//int barrier_unmaping = 1; /* Do atomic and use cnt to support more concurrency */
+int popcorn_distributed_barrier(unsigned long addr, unsigned long len)
+{
+	int ret = -1;
+	POPPK_DSHM(" [munmap][barrier][%d] I'm an user thread\n", current->pid);
+	if (current->at_remote) {
+		/* wait at origin */
+		popcorn_barrier_response_t *res;
+		popcorn_barrier_request_t *req = kmalloc(sizeof(*req), GFP_KERNEL);
+		struct wait_station *ws = get_wait_station(current);
+		int dst_nid = my_nid ? 0 : 1; // 2node now
+		BUG_ON(!req);
+		req->ws = ws->id;
+
+		req->from_pid = current->pid;
+		//req->addr = addr; // notused // input
+		POPPK_DSHM(" [remote] barrier =>\n");
+		pcn_kmsg_send(PCN_KMSG_TYPE_GLOBAL_BARRIER_REQUEST,
+									dst_nid, req, sizeof(*req));
+		res = wait_at_station(ws);
+		POPPK_DSHM(" => [remote] barrier waked up\n");
+		ret = res->ret;
+		//return vma_server_munmap_remote(addr, len);
+
+		/* local munmap */
+		POPPK_DSHM(" [barrier][%d] DSHM munmap local at remote start\n", current->pid);
+		ret |= vm_munmap(addr, len);
+		POPPK_DSHM(" [barrier][%d] DSHM munmap local at remote done\n", current->pid);
+
+		kfree(req);
+		pcn_kmsg_done(res);
+
+		current->mm->remote = NULL; // for some reseaon, it faults after unmmap // disconnect my self from rc
+		/* TODO - Kill worker */
+		POPPK_DSHM(" [barrier][%d] kill kworker at remote\n", current->pid);
+		{
+			//struct task_struct *tsk = __get_task_struct(gttid); // get workers pid
+			struct task_struct *tsk =
+				__get_task_struct(current->remote->remote_worker->pid); // get workers pid
+			clone_request_t *req;
+			struct pcn_kmsg_work *work = kmalloc(sizeof(*work), GFP_KERNEL);
+			req = kmalloc(sizeof(*req), GFP_KERNEL);
+			//req = pcn_kmsg_get(sizeof(*req));
+			BUG_ON(!req);
+			BUG_ON(!work);
+			req->header.type = PCN_KMSG_TYPE_STOP_WORKER;
+			req->header.prio = PCN_KMSG_PRIO_NORMAL;
+			req->header.size = sizeof(*req);
+			req->header.from_nid = my_nid;
+			work->msg = req;
+			__schedule_remote_work(current->remote, work);
+
+			BUG_ON(!tsk);
+			POPPK_DSHM(" [barrier] detach kworker[%d]'s rc\n",
+						tsk->pid);
+			//tsk->remote = NULL; // disconnect user process to be distributed
+			//tsk->origin_nid = -1;
+			//tsk->origin_pid = -1;
+			// tsk->mm->remote = NULL; // for some reseaon, it faults after unmmap // let kworker to release this
+			put_task_struct(tsk);
+		}
+
+		// hint: remote worker. why?
+	} else { /* Origin */
+		//barrier_unmaping = 1;
+		/* Barrier wait() */
+		POPPK_DSHM(" [local] barrier\n");
+		atomic64_inc(&popcorn_global_local_cnt);
+		while (atomic64_read(&popcorn_global_local_cnt) != 2) { /* TODO: hack 2 */
+			/* only single CPU I think I can spin */
+			//CPU_RELAX;
+			schedule();
+		}
+
+		POPPK_DSHM(" [barrier] all arrived\n");
+		/* unmap() */
+		//POPPK_DSHM(" [barrier] DSHM munmap delegate to origin and broadcase\n");
+		//ret = vma_server_munmap_origin(addr, len, my_nid);
+		//POPPK_DSHM(" [barrier] munmap_origin() done\n");
+
+		/* local munmap */
+		POPPK_DSHM(" [barrier][%d] DSHM munmap local at origin start\n", current->pid);
+		ret = vm_munmap(addr, len);
+		POPPK_DSHM(" [barrier][%d] DSHM munmap local at origin done\n", current->pid);
+
+		/* Broadcast barrier response */
+		POPPK_DSHM(" [barrier][%d] remote vm_munmap() =>\n", current->pid);
+		atomic64_set(&popcorn_global_local_cnt, 0);
+		//barrier_unmaping = 0;
+
+		//return vma_server_munmap_origin(addr, len, my_nid);
+
+		/* TODO - Kill worker */
+		POPPK_DSHM(" [barrier][%d] kill kworker at origin\n", current->pid);
+		current->mm->remote = NULL; // for some reseaon, it faults after unmmap // disconnect my self from rc
+
+		//current->remote = NULL; // let worker to kfree()
+		//current->remote_nid = -1;
+		//current->remote_pid = -1;
+		{
+			clone_request_t *req;
+			struct pcn_kmsg_work *work = kmalloc(sizeof(*work), GFP_KERNEL);
+			req = kmalloc(sizeof(*req), GFP_KERNEL);
+			//req = pcn_kmsg_get(sizeof(*req));
+			BUG_ON(!req);
+			BUG_ON(!work);
+			req->header.type = PCN_KMSG_TYPE_STOP_WORKER;
+			req->header.prio = PCN_KMSG_PRIO_NORMAL;
+			req->header.size = sizeof(*req);
+			req->header.from_nid = my_nid;
+			work->msg = req;
+			current->remote->remote_work = req;
+			complete(&current->remote->remote_work_pended);
+
+			POPPK_DSHM(" [barrier] detach user tsk [%d]'s rc\n",
+						current->pid);
+			// current->mm->remote = NULL; // for some reseaon, it faults after unmmap // let kworker to release this
+		}
+		// hint: origin user thread. why?
+	}
+	POPPK_DSHM(" [munmap][barrier][%d] user done ret %d\n", current->pid, ret);
+	return ret;
+}
+
+
+static void process_global_barrier_request(struct work_struct *work)
+{
+    START_KMSG_WORK(popcorn_barrier_request_t, req, work);
+    popcorn_barrier_response_t *res = pcn_kmsg_get(sizeof(*res));
+    int from_nid = PCN_KMSG_FROM_NID(req);
+    res->ws = req->ws;
+	//barrier_unmaping = 1;
+
+	/* Wait until origin unmap it */
+    POPPK_DSHM(" => [from %d / %d] wait at barrier "
+				"(frome remote e.g. pcn_dshm_munmap)\n",
+				from_nid, req->from_pid);
+	atomic64_inc(&popcorn_global_local_cnt);
+
+	while (atomic64_read(&popcorn_global_local_cnt) > 0) {
+	//while (barrier_unmaping) {
+		schedule();
+	}
+	//TODO: last barrier_unmaping = 1;
+
+	res->ret = 0;
+    POPPK_DSHM("    barrier wake up => [%d]\n", from_nid);
+    pcn_kmsg_post(PCN_KMSG_TYPE_GLOBAL_BARRIER_RESPONSE,
+                            from_nid, res, sizeof(*res));
+    END_KMSG_WORK(req);
+}
+
+static int handle_global_barrier_response(struct pcn_kmsg_message *msg)
+{
+    popcorn_barrier_response_t *res = (popcorn_barrier_response_t *)msg;
+    struct wait_station *ws = wait_station(res->ws);
+
+    ws->private = res;
+
+    POPPK_DSHM(" => [remote] wakeup barrier\n");
+    complete(&ws->pendings);
+    return 0;
+}
+#endif
+
 DEFINE_KMSG_RW_HANDLER(origin_task_exit, origin_task_exit_t, remote_pid);
 DEFINE_KMSG_RW_HANDLER(remote_task_exit, remote_task_exit_t, origin_pid);
 DEFINE_KMSG_RW_HANDLER(back_migration, back_migration_request_t, origin_pid);
 DEFINE_KMSG_RW_HANDLER(remote_futex_request, remote_futex_request, origin_pid);
+#ifdef CONFIG_POPCORN_DSHM
+DEFINE_KMSG_WQ_HANDLER(dshm_join_request);
+DEFINE_KMSG_WQ_HANDLER(dshm_join_update_request);
+DEFINE_KMSG_WQ_HANDLER(global_barrier_request);
+DEFINE_KMSG_RW_HANDLER(update_origin_worker_pid, update_origin_worker_pid_t, remote_pid);
+DEFINE_KMSG_RW_HANDLER(update_remote_worker_pid, update_remote_worker_pid_t, origin_pid);
+#endif
 
 /**
  * Initialize the process server.
@@ -1049,5 +1711,24 @@ int __init process_server_init(void)
 	REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_FUTEX_REQUEST, remote_futex_request);
 	REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_FUTEX_RESPONSE, remote_futex_response);
 
+#ifdef CONFIG_POPCORN_DSHM
+	REGISTER_KMSG_WQ_HANDLER(
+			PCN_KMSG_TYPE_DSHM_JOIN_REQUEST, dshm_join_request);
+    REGISTER_KMSG_HANDLER(
+            PCN_KMSG_TYPE_DSHM_JOIN_RESPONSE, dshm_join_response);
+	REGISTER_KMSG_WQ_HANDLER(
+			PCN_KMSG_TYPE_DSHM_JOIN_UPDATE_REQUEST, dshm_join_update_request);
+    REGISTER_KMSG_HANDLER(
+            PCN_KMSG_TYPE_DSHM_JOIN_UPDATE_RESPONSE, dshm_join_update_response);
+	REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_UPDATE_ORIGIN_WORKER_PID, update_origin_worker_pid);
+	//REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_UPDATE_REMOTE_WORKER_PID, update_origin_worker_pid); // testing
+	REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_UPDATE_REMOTE_WORKER_PID, update_remote_worker_pid);
+	//REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_UPDATE_ORIGIN_WORKER_PID, update_remote_worker_pid); // testing
+	REGISTER_KMSG_WQ_HANDLER(
+			PCN_KMSG_TYPE_GLOBAL_BARRIER_REQUEST, global_barrier_request);
+    REGISTER_KMSG_HANDLER(
+            PCN_KMSG_TYPE_GLOBAL_BARRIER_RESPONSE, global_barrier_response);
+#endif
+
 	return 0;
 }
diff --git a/kernel/popcorn/stat.c b/kernel/popcorn/stat.c
index fc7e62f3c644..37a7584b1f62 100644
--- a/kernel/popcorn/stat.c
+++ b/kernel/popcorn/stat.c
@@ -28,6 +28,8 @@ const char *pcn_kmsg_type_name[PCN_KMSG_TYPE_MAX] = {
 	[PCN_KMSG_TYPE_VMA_INFO_REQUEST] = "VMA info",
 	[PCN_KMSG_TYPE_VMA_OP_REQUEST] = "VMA op",
 	[PCN_KMSG_TYPE_REMOTE_PAGE_REQUEST] = "remote page",
+	[PCN_KMSG_TYPE_REMOTE_PAGE_RESPONSE] = "w/ page",
+	[PCN_KMSG_TYPE_REMOTE_PAGE_RESPONSE_SHORT] = "w/o page",
 	[PCN_KMSG_TYPE_PAGE_INVALIDATE_REQUEST] = "invalidate",
 	[PCN_KMSG_TYPE_FUTEX_REQUEST] = "futex",
 };
@@ -61,6 +63,7 @@ void account_pcn_rdma_read(size_t size)
 }
 
 void fh_action_stat(struct seq_file *seq, void *);
+extern void pf_time_stat(struct seq_file *seq, void *v);
 
 static int __show_stats(struct seq_file *seq, void *v)
 {
@@ -113,13 +116,15 @@ static int __show_stats(struct seq_file *seq, void *v)
 
 #ifdef CONFIG_POPCORN_STAT
 	seq_printf(seq, "-----------------------------------------------\n");
-	for (i = PCN_KMSG_TYPE_STAT_START + 1; i < PCN_KMSG_TYPE_STAT_END; i++) {
+	//for (i = PCN_KMSG_TYPE_STAT_START + 1; i < PCN_KMSG_TYPE_STAT_END; i++) {
+	for (i = PCN_KMSG_TYPE_REMOTE_PAGE_REQUEST; i < PCN_KMSG_TYPE_REMOTE_PAGE_FLUSH; i++) {
 		seq_printf(seq, POPCORN_STAT_FMT,
 				sent_stats[i], recv_stats[i], pcn_kmsg_type_name[i] ? : "");
 	}
 	seq_printf(seq, "---------------------------------------------------------------------------\n");
 
-	fh_action_stat(seq, v);
+	//fh_action_stat(seq, v);
+	pf_time_stat(seq, v);
 #endif
 	return 0;
 }
@@ -141,6 +146,10 @@ static ssize_t __write_stats(struct file *file, const char __user *buffer, size_
 	}
 	fh_action_stat(NULL, NULL);
 
+#ifdef CONFIG_POPCORN_STAT
+	pf_time_stat(NULL, NULL);
+#endif
+
 	return size;
 }
 
diff --git a/kernel/popcorn/sync.h b/kernel/popcorn/sync.h
new file mode 100644
index 000000000000..8f95fd80fa3b
--- /dev/null
+++ b/kernel/popcorn/sync.h
@@ -0,0 +1,14 @@
+/*
+ * sync.h
+ * Copyright (C) 2018 jackchuang <jackchuang@mir7>
+ *
+ * Distributed under terms of the MIT license.
+ */
+
+#ifndef _SYNC_H
+#define _SYNC_H
+#include <popcorn/sync.h>
+
+void tso_wr_inc(struct vm_area_struct *vma, unsigned long addr, struct page *page, spinlock_t *ptl);
+
+#endif /* !_SYNC_H */
diff --git a/kernel/popcorn/types.h b/kernel/popcorn/types.h
index 08d26735c7b3..4f7cc1b80cd7 100644
--- a/kernel/popcorn/types.h
+++ b/kernel/popcorn/types.h
@@ -12,47 +12,11 @@
 #include <popcorn/pcn_kmsg.h>
 #include <popcorn/regset.h>
 
-#define FAULTS_HASH 31
-
-/**
- * Remote execution context
- */
-struct remote_context {
-	struct list_head list;
-	atomic_t count;
-	struct mm_struct *mm;
-
-	int tgid;
-	bool for_remote;
-
-	/* Tracking page status */
-	struct radix_tree_root pages;
-
-	/* For page replication protocol */
-	spinlock_t faults_lock[FAULTS_HASH];
-	struct hlist_head faults[FAULTS_HASH];
-
-	/* For VMA management */
-	spinlock_t vmas_lock;
-	struct list_head vmas;
-
-	/* Remote worker */
-	bool stop_remote_worker;
-
-	struct task_struct *remote_worker;
-	struct completion remote_works_ready;
-	spinlock_t remote_works_lock;
-	struct list_head remote_works;
-
-	pid_t remote_tgids[MAX_POPCORN_NODES];
-};
-
 struct remote_context *__get_mm_remote(struct mm_struct *mm);
 struct remote_context *get_task_remote(struct task_struct *tsk);
 bool put_task_remote(struct task_struct *tsk);
 bool __put_task_remote(struct remote_context *rc);
 
-
 /**
  * Process migration
  */
@@ -263,6 +227,62 @@ DEFINE_PCN_KMSG(page_invalidate_request_t, PAGE_INVALIDATE_REQUEST_FIELDS);
 	pid_t remote_pid;
 DEFINE_PCN_KMSG(page_invalidate_response_t, PAGE_INVALIDATE_RESPONSE_FIELDS);
 
+#ifdef CONFIG_POPCORN_DSHM
+// from_pid/origin_pid
+//	int origin_ws; -> ws
+//	pid_t target_pid;// no target because this is first handshaking
+//	pid_t remote_pid; // no use
+//	pid_t ttid;
+// addr currently not used
+#define DSHM_JOIN_REQUEST_FIELDS \
+	pid_t from_pid; \
+	int ws; \
+	unsigned long addr;
+DEFINE_PCN_KMSG(dshm_join_request_t, DSHM_JOIN_REQUEST_FIELDS);
+
+//pid_t from_pid;
+//pid_t origin_pid;
+//pid_t remote_pid;
+#define DSHM_JOIN_UPDATE_RESPONSE_FIELDS \
+	int ws; \
+	pid_t ttid;
+DEFINE_PCN_KMSG(dshm_join_response_t, DSHM_JOIN_UPDATE_RESPONSE_FIELDS);
+
+//pid_t from_pid;
+//pid_t origin_pid;
+//pid_t remote_pid;
+#define DSHM_JOIN_RESPONSE_FIELDS \
+	int ws; \
+	pid_t ttid;
+DEFINE_PCN_KMSG(dshm_join_update_response_t, DSHM_JOIN_RESPONSE_FIELDS);
+
+#define DSHM_JOIN_UPDATE_REQUEST_FIELDS \
+	pid_t from_pid; \
+	int ws; \
+	unsigned long addr;
+DEFINE_PCN_KMSG(dshm_join_update_request_t, DSHM_JOIN_UPDATE_REQUEST_FIELDS);
+
+// origin asks remote to update origin's worker's pid
+// remote_pid: current remote worker's pid recorded at origin
+// ttid: new pid I wanna target node to update
+#define UPDATE_ORIGIN_WORKER_PID_FIELDS \
+	pid_t remote_pid; \
+	pid_t ttid;
+DEFINE_PCN_KMSG(update_origin_worker_pid_t, UPDATE_ORIGIN_WORKER_PID_FIELDS);
+
+#define UPDATE_REMOTE_WORKER_PID_FIELDS \
+	pid_t origin_pid; \
+	pid_t ttid;
+DEFINE_PCN_KMSG(update_remote_worker_pid_t, UPDATE_REMOTE_WORKER_PID_FIELDS);
+
+#define GLOBAL_BARRIER_FIELDS \
+	pid_t from_pid; \
+	int ws; \
+	int ret;
+DEFINE_PCN_KMSG(popcorn_barrier_request_t, GLOBAL_BARRIER_FIELDS);
+DEFINE_PCN_KMSG(popcorn_barrier_response_t, GLOBAL_BARRIER_FIELDS);
+
+#endif
 
 /**
  * Futex
@@ -304,6 +324,27 @@ DEFINE_PCN_KMSG(node_info_t, NODE_INFO_FIELDS);
 DEFINE_PCN_KMSG(sched_periodic_req, SCHED_PERIODIC_FIELDS);
 
 
+#ifdef CONFIG_POPCORN_DSHM
+#define ORIGIN_HYPE_COMMON_FIELDS \
+    pid_t from_pid; \
+    pid_t remote_pid; \
+    int ws; \
+    int fd; \
+    int ret;
+
+#define IPI_REQUEST_FIELDS \
+    ORIGIN_HYPE_COMMON_FIELDS
+DEFINE_PCN_KMSG(ipi_request_t, IPI_REQUEST_FIELDS);
+
+#define IPI_RESPONSE_FIELDS \
+    ORIGIN_HYPE_COMMON_FIELDS
+DEFINE_PCN_KMSG(ipi_response_t, IPI_RESPONSE_FIELDS);
+
+//#if defined(CONFIG_X86_64)
+//#elif defined(CONFIG_ARM64)
+//#endif
+#endif
+
 /**
  * Message routing using work queues
  */
diff --git a/kernel/popcorn/vma_server.c b/kernel/popcorn/vma_server.c
index 354fe98d59ea..450feba5f983 100644
--- a/kernel/popcorn/vma_server.c
+++ b/kernel/popcorn/vma_server.c
@@ -2,7 +2,7 @@
  * @file vma_server.c
  *
  * Popcorn Linux VMA handler implementation
- * This work was an extension of David Katz MS Thesis, but totally rewritten 
+ * This work was an extension of David Katz MS Thesis, but totally rewritten
  * by Sang-Hoon to support multithread environment.
  *
  * @author Sang-Hoon Kim, SSRG Virginia Tech 2016-2017
@@ -25,6 +25,7 @@
 
 #include <popcorn/types.h>
 #include <popcorn/bundle.h>
+#include <popcorn/debug.h>
 
 #include "types.h"
 #include "util.h"
@@ -60,7 +61,7 @@ static unsigned long map_difference(struct mm_struct *mm, struct file *file,
 	/**
 	 * Go through ALL VMAs, looking for overlapping with this space.
 	 */
-	VSPRINTK("  [%d] map+ %lx %lx\n", current->pid, start, end);
+	VSPRINTK("  [%d] map+ %lx %lx (expected)\n", current->pid, start, end);
 	for (vma = current->mm->mmap; start < end; vma = vma->vm_next) {
 		/*
 		VSPRINTK("  [%d] vma  %lx -- %lx\n", current->pid,
@@ -73,6 +74,9 @@ static unsigned long map_difference(struct mm_struct *mm, struct file *file,
 			 */
 			VSPRINTK("  [%d] map0 %lx -- %lx @ %lx, %lx\n", current->pid,
 					start, end, pgoff, prot);
+#if defined(CONFIG_POPCORN_DSHM)
+			POPPK_DSHM("\tmap0 = expected start -- end\n");
+#endif
 			error = do_mmap_pgoff(file, start, end - start,
 					prot, flags, pgoff, &populate);
 			if (error != start) {
@@ -108,8 +112,13 @@ static unsigned long map_difference(struct mm_struct *mm, struct file *file,
 			break;
 		} else if (start <= vma->vm_start && vma->vm_end <= end) {
 			/* VMA is fully within the region of interest */
-			VSPRINTK("  [%d] map2 %lx -- %lx @ %lx\n", current->pid,
+			VSPRINTK("  [%d] map2 %lx -- %lx @ %lx (got)\n", current->pid,
 					start, vma->vm_start, pgoff);
+#if defined(CONFIG_POPCORN_DSHM)
+			POPPK_DSHM("\tend %lx != vma->vm_start(len) %lx "
+					"This is incorrect even before do_mmap()\n",
+							end, vma->vm_start);
+#endif
 			error = do_mmap_pgoff(file, start, vma->vm_start - start,
 					prot, flags, pgoff, &populate);
 			if (error != start) {
@@ -224,6 +233,8 @@ static int handle_vma_op_response(struct pcn_kmsg_message *msg)
 	vma_op_response_t *res = (vma_op_response_t *)msg;
 	struct wait_station *ws = wait_station(res->remote_ws);
 
+	VSPRINTK("=> ## VMA mmap %lx - %lx %ld done\n",
+				res->addr, res->addr + res->len, res->ret);
 	ws->private = res;
 	complete(&ws->pendings);
 
@@ -245,7 +256,7 @@ unsigned long vma_server_mmap_remote(struct file *file,
 	req->pgoff = pgoff;
 	get_file_path(file, req->path, sizeof(req->path));
 
-	VSPRINTK("\n## VMA mmap [%d] %lx - %lx, %lx %lx\n", current->pid,
+	VSPRINTK("\n## VMA mmap [%d] %lx - %lx, %lx %lx =>\n", current->pid,
 			addr, addr + len, prot, flags);
 	if (req->path[0] != '\0') {
 		VSPRINTK("  [%d] %s\n", current->pid, req->path);
@@ -253,7 +264,7 @@ unsigned long vma_server_mmap_remote(struct file *file,
 
 	ret = __delegate_vma_op(req, &res);
 
-	VSPRINTK("  [%d] %ld %lx -- %lx\n", current->pid,
+	VSPRINTK("=>  [%d] %ld %lx -- %lx\n", current->pid,
 			ret, res->addr, res->addr + res->len);
 
 	if (ret) goto out_free;
@@ -278,7 +289,7 @@ int vma_server_munmap_remote(unsigned long start, size_t len)
 	vma_op_request_t *req;
 	vma_op_response_t *res;
 
-	VSPRINTK("\n## VMA munmap [%d] %lx %lx\n", current->pid, start, len);
+	VSPRINTK("\n## VMA munmap [%d] %lx %lx =>\n", current->pid, start, len);
 
 	ret = vm_munmap(start, len);
 	if (ret) return ret;
@@ -289,7 +300,7 @@ int vma_server_munmap_remote(unsigned long start, size_t len)
 
 	ret = __delegate_vma_op(req, &res);
 
-	VSPRINTK("  [%d] %d %lx -- %lx\n", current->pid,
+	VSPRINTK("=>  [%d] %d %lx -- %lx\n", current->pid,
 			ret, res->addr, res->addr + res->len);
 
 	kfree(req);
@@ -388,6 +399,9 @@ int vma_server_munmap_origin(unsigned long start, size_t len, int nid_except)
 	req->start = start;
 	req->len = len;
 
+#ifdef CONFIG_POPCORN_DSHM
+	BUG_ON("No delegation");
+#endif
 	for (nid = 0; nid < MAX_POPCORN_NODES; nid++) {
 		struct wait_station *ws;
 		vma_op_response_t *res;
@@ -398,10 +412,13 @@ int vma_server_munmap_origin(unsigned long start, size_t len, int nid_except)
 
 		ws = get_wait_station(current);
 		req->remote_ws = ws->id;
+		/* remote worker and the process share the same rc.
+			Remote worker spin on rc. */
 		req->origin_pid = rc->remote_tgids[nid];
 
-		VSPRINTK("  [%d] ->munmap [%d/%d] %lx+%lx\n", current->pid,
-				req->origin_pid, nid, start, len);
+		VSPRINTK("  [%d] ->munmap [%d/%d] %lx+%lx (%lx)\n", current->pid,
+				req->origin_pid, nid, start, len,
+				instruction_pointer(current_pt_regs()));
 		pcn_kmsg_send(PCN_KMSG_TYPE_VMA_OP_REQUEST, nid, req, sizeof(*req));
 		res = wait_at_station(ws);
 		pcn_kmsg_done(res);
@@ -464,24 +481,27 @@ static long __process_vma_op_at_remote(vma_op_request_t *req)
 	return ret;
 }
 
-static long __process_vma_op_at_origin(vma_op_request_t *req)
+static long __process_vma_op_at_origin(vma_op_request_t *req, struct task_struct *tsk)
 {
 	long ret = -EPERM;
 	int from_nid = PCN_KMSG_FROM_NID(req);
+	VSPRINTK("\t[%d] req op %x\n",
+			tsk->pid, req->operation);
 
 	switch (req->operation) {
 	case VMA_OP_MMAP: {
 		unsigned long populate = 0;
 		unsigned long raddr;
 		struct file *f = NULL;
-		struct mm_struct *mm = get_task_mm(current);
+		struct mm_struct *mm = get_task_mm(tsk);
 
 		if (req->path[0] != '\0')
 			f = filp_open(req->path, O_RDONLY | O_LARGEFILE, 0);
 
 		if (IS_ERR(f)) {
 			ret = PTR_ERR(f);
-			printk("  [%d] Cannot open %s %ld\n", current->pid, req->path, ret);
+			printk("  [%d] %s(): Cannot open %s %ld\n",
+					tsk->pid, __func__, req->path, ret);
 			mmput(mm);
 			break;
 		}
@@ -489,12 +509,13 @@ static long __process_vma_op_at_origin(vma_op_request_t *req)
 		raddr = do_mmap_pgoff(f, req->addr, req->len, req->prot,
 				req->flags, req->pgoff, &populate);
 		up_write(&mm->mmap_sem);
-		if (populate) mm_populate(raddr, populate);
+		if (populate) mm_populate(raddr, populate); // lock it (don't swap)
 
 		ret = IS_ERR_VALUE(raddr) ? raddr : 0;
 		req->addr = raddr;
-		VSPRINTK("  [%d] %lx %lx -- %lx %lx %lx\n", current->pid,
-				ret, req->addr, req->addr + req->len, req->prot, req->flags);
+		VSPRINTK("  [%d] %lx %lx -- %lx %lx %lx populate %lu\n", tsk->pid,
+				ret, req->addr, req->addr + req->len,
+				req->prot, req->flags, populate);
 
 		if (f) filp_close(f, NULL);
 		mmput(mm);
@@ -531,19 +552,20 @@ static long __process_vma_op_at_origin(vma_op_request_t *req)
 	return ret;
 }
 
-void process_vma_op_request(vma_op_request_t *req)
+void process_vma_op_request(vma_op_request_t *req, struct task_struct *tsk)
 {
 	long ret = 0;
-	VSPRINTK("\nVMA_OP_REQUEST [%d] %s %lx %lx\n", current->pid,
-			vma_op_code_sz[req->operation], req->addr, req->len);
+	VSPRINTK("\n => VMA_OP_REQUEST [%d] %s %lx %lx inst_ptr %lx(dshm !precise)\n",
+			tsk->pid, vma_op_code_sz[req->operation], req->addr, req->len,
+			instruction_pointer(current_pt_regs()));
 
-	if (current->at_remote) {
+	if (tsk->at_remote) {
 		ret = __process_vma_op_at_remote(req);
 	} else {
-		ret = __process_vma_op_at_origin(req);
+		ret = __process_vma_op_at_origin(req, tsk);
 	}
 
-	VSPRINTK("  [%d] ->%s %ld\n", current->pid,
+	VSPRINTK("  [%d] ->%s %ld\n", tsk->pid,
 			vma_op_code_sz[req->operation], ret);
 
 	__reply_vma_op(req, ret);
@@ -589,6 +611,7 @@ static int handle_vma_info_response(struct pcn_kmsg_message *msg)
 	}
 	rc = get_task_remote(tsk);
 
+	POPPK_DSHM("  => [%d] rc %p \n", tsk->pid, rc);
 	spin_lock_irqsave(&rc->vmas_lock, flags);
 	vi = __lookup_pending_vma_request(rc, res->addr);
 	spin_unlock_irqrestore(&rc->vmas_lock, flags);
@@ -599,6 +622,7 @@ static int handle_vma_info_response(struct pcn_kmsg_message *msg)
 		goto out_free;
 	}
 
+	POPPK_DSHM("  => [%d] wake vi %p\n", tsk->pid, vi);
 	vi->response = res;
 	complete(&vi->complete);
 	return 0;
@@ -613,13 +637,15 @@ out_free:
  * Handle VMA info requests at the origin.
  * This is invoked through the remote work delegation.
  */
-void process_vma_info_request(vma_info_request_t *req)
+void process_vma_info_request(vma_info_request_t *req, struct task_struct *tsk)
 {
 	vma_info_response_t *res = NULL;
 	struct mm_struct *mm;
 	struct vm_area_struct *vma;
 	unsigned long addr = req->addr;
 
+	POPPK_DSHM("%s():\n", __func__);
+
 	might_sleep();
 
 	while (!res) {
@@ -627,12 +653,12 @@ void process_vma_info_request(vma_info_request_t *req)
 	}
 	res->addr = addr;
 
-	mm = get_task_mm(current);
+	mm = get_task_mm(tsk);
 	down_read(&mm->mmap_sem);
 
 	vma = find_vma(mm, addr);
 	if (unlikely(!vma)) {
-		printk("vma_info: vma does not exist at %lx\n", addr);
+		printk(KERN_ERR "vma_info: vma does not exist at %lx\n", addr);
 		res->result = -ENOENT;
 		goto out_up;
 	}
@@ -640,7 +666,7 @@ void process_vma_info_request(vma_info_request_t *req)
 		goto good;
 	}
 	if (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {
-		printk("vma_info: vma does not really exist at %lx\n", addr);
+		printk(KERN_ERR "vma_info: vma does not really exist at %lx\n", addr);
 		res->result = -ENOENT;
 		goto out_up;
 	}
@@ -659,13 +685,17 @@ out_up:
 	mmput(mm);
 
 	if (res->result == 0) {
-		VSPRINTK("\n## VMA_INFO [%d] %lx -- %lx %lx\n", current->pid,
+		VSPRINTK("\n## VMA_INFO [%d] %lx -- %lx %lx\n", tsk->pid,
 				res->vm_start, res->vm_end, res->vm_flags);
 		if (!vma_info_anon(res)) {
-			VSPRINTK("  [%d] %s + %lx\n", current->pid,
+			VSPRINTK("  [%d] %s + %lx\n", tsk->pid,
 					res->vm_file_path, res->vm_pgoff);
 		}
 	}
+#ifdef CONFIG_POPCORN_DSHM
+	POPPK_DSHM("Not sure if vm_file_path is a problem "
+				"(remote has set anon but here origin doesn't yet)\n");
+#endif
 
 	res->remote_pid = req->remote_pid;
 	pcn_kmsg_send(PCN_KMSG_TYPE_VMA_INFO_RESPONSE,
@@ -714,26 +744,38 @@ static int __update_vma(struct task_struct *tsk, vma_info_response_t *res)
 	int ret = 0;
 	unsigned long addr = res->addr;
 
+	POPPK_DSHM("  [%d] %s(): %lx - 1\n", tsk->pid, __func__, addr);
 	if (res->result) {
 		down_read(&mm->mmap_sem);
 		return res->result;
 	}
 
+	POPPK_DSHM("  [%d] %s(): %lx - 2\n", tsk->pid, __func__, addr);
 	while (!down_write_trylock(&mm->mmap_sem)) {
 		schedule();
 	}
 	vma = find_vma(mm, addr);
+	POPPK_DSHM("  [%d] %s(): %lx - 3\n", tsk->pid, __func__, addr);
 	VSPRINTK("  [%d] %lx %lx\n", tsk->pid, vma ? vma->vm_start : 0, addr);
 	if (vma && vma->vm_start <= addr) {
 		/* somebody already done for me. */
 		goto out;
 	}
 
+#ifdef CONFIG_POPCORN_DSHM
+	/* Although origin tells us this is not an anon page,
+		we treat it as an anon page. */
+	POPPK_DSHM("  [pophype][%d] %s(): %lx - "
+			"DSHM forces the vma region to be anon pages\n",
+			tsk->pid, __func__, addr);
+	flags |= MAP_ANONYMOUS;
+#else
 	if (vma_info_anon(res)) {
 		flags |= MAP_ANONYMOUS;
 	} else {
 		f = filp_open(res->vm_file_path, O_RDONLY | O_LARGEFILE, 0);
 		if (IS_ERR(f)) {
+			/* DSHM will hit this..... */
 			printk(KERN_ERR"%s: cannot find backing file %s\n",__func__,
 				res->vm_file_path);
 			ret = -EIO;
@@ -747,6 +789,7 @@ static int __update_vma(struct task_struct *tsk, vma_info_response_t *res)
 		VSPRINTK("  [%d] %s + %lx\n", tsk->pid,
 				res->vm_file_path, res->vm_pgoff);
 	}
+#endif
 
 	prot  = ((res->vm_flags & VM_READ) ? PROT_READ : 0)
 			| ((res->vm_flags & VM_WRITE) ? PROT_WRITE : 0)
@@ -757,6 +800,16 @@ static int __update_vma(struct task_struct *tsk, vma_info_response_t *res)
 			| ((res->vm_flags & VM_SHARED) ? MAP_SHARED : MAP_PRIVATE)
 			| ((res->vm_flags & VM_GROWSDOWN) ? MAP_GROWSDOWN : 0);
 
+#ifdef CONFIG_POPCORN_DSHM
+	POPPK_DSHM("  [%d] %s(): %lx - prot 0x%lx (ERW) flags 0x%x ()\n",
+				tsk->pid, __func__, addr, prot, flags);
+	// flags = 0x31
+	// MAP_SHARED	0x01	MAP_PRIVATE	0x02
+	// MAP_TYPE	0x0f
+	// MAP_FIXED	0x10 MAP_ANONYMOUS	0x20
+	POPPK_DSHM("  [%d] %s(): %lx - do map_difference\n",
+				tsk->pid, __func__, addr);
+#endif
 	err = map_difference(mm, f, res->vm_start, res->vm_end,
 				prot, flags, res->vm_pgoff);
 
@@ -816,23 +869,34 @@ int vma_server_fetch_vma(struct task_struct *tsk, unsigned long address)
 	if (req) {
 		spin_unlock_irqrestore(&rc->vmas_lock, flags);
 
-		VSPRINTK("  [%d] %lx ->[%d/%d]\n", current->pid,
-				addr, tsk->origin_pid, tsk->origin_nid);
+		VSPRINTK("  [%d] %lx ->[%d/%d] vi %p\n", current->pid,
+				addr, tsk->origin_pid, tsk->origin_nid, vi);
 		pcn_kmsg_send(PCN_KMSG_TYPE_VMA_INFO_REQUEST,
 				tsk->origin_nid, req, sizeof(*req));
 		wait_for_completion(&vi->complete);
+		POPPK_DSHM("  => [%d] %lx do __update_vma "
+				"(replay locally) res->addr %lx\n",
+				current->pid, addr,
+				((vma_info_response_t *)vi->response)->addr);
 
-		ret = vi->ret =
+		ret = vi->ret = 						/* down_read() */
 			__update_vma(tsk, (vma_info_response_t *)vi->response);
 
+		POPPK_DSHM("  [%d] %lx __update_vma replay locally done ret %d\n",
+					current->pid, addr, ret);
 		spin_lock_irqsave(&rc->vmas_lock, flags);
 		list_del(&vi->list);
 		spin_unlock_irqrestore(&rc->vmas_lock, flags);
 
+		POPPK_DSHM("  [%d] %lx __update_vma replay locally done - "
+					"remove vma_info handle\n",
+					current->pid, addr);
 		pcn_kmsg_done((void *)vi->response);
 		wake_up_all(&vi->pendings_wait);
 
 		kfree(req);
+		VSPRINTK("[%d] %lx ret %d\n", current->pid,
+				addr, ret);
 	} else {
 		VSPRINTK("  [%d] %lx already pended\n", current->pid, addr);
 		atomic_inc(&vi->pendings);
diff --git a/kernel/popcorn/vma_server.h b/kernel/popcorn/vma_server.h
index bc1606be840d..65ff6a1e1863 100644
--- a/kernel/popcorn/vma_server.h
+++ b/kernel/popcorn/vma_server.h
@@ -14,8 +14,8 @@
 
 struct remote_context;
 
-void process_vma_info_request(vma_info_request_t *req);
+void process_vma_info_request(vma_info_request_t *req, struct task_struct *tsk);
 
-void process_vma_op_request(vma_op_request_t *req);
+void process_vma_op_request(vma_op_request_t *req, struct task_struct *tsk);
 
 #endif /* KERNEL_POPCORN_VMA_SERVER_H_ */
diff --git a/mm/gup.c b/mm/gup.c
index ebc9627f8461..5cb99fe4410f 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -604,6 +604,11 @@ int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 	vma = find_extend_vma(mm, address);
 #ifdef CONFIG_POPCORN
 	if (distributed_remote_process(tsk)) {
+#ifdef CONFIG_POPCORN_DSHM
+        //if (interested_vma_range)
+		POPPK_DSHM("[%d][attention] %s %s(): looks like this is for futex\n",
+								current->pid, __FILE__, __func__);
+#endif
 		if (!vma || address < vma->vm_start) {
 			if (vma_server_fetch_vma(tsk, address) == 0) {
 				/* Replace with updated VMA */
diff --git a/mm/madvise.c b/mm/madvise.c
index 4278bb8268a4..fe4af60983f2 100644
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@ -39,7 +39,8 @@ static int madvise_need_mmap_write(int behavior)
 	case MADV_REMOVE:
 	case MADV_WILLNEED:
 	case MADV_DONTNEED:
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	case MADV_RELEASE:
 #endif
 		return 0;
@@ -378,7 +379,8 @@ static int madvise_hwpoison(int bhv, unsigned long start, unsigned long end)
 }
 #endif
 
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 int madvise_release(struct vm_area_struct *vma, unsigned long start, unsigned long end)
 {
 	int nr_pages = 0;
@@ -406,7 +408,8 @@ madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,
 		return madvise_willneed(vma, prev, start, end);
 	case MADV_DONTNEED:
 		return madvise_dontneed(vma, prev, start, end);
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	case MADV_RELEASE:
 		return madvise_release(vma, start, end);
 #endif
@@ -437,7 +440,8 @@ madvise_behavior_valid(int behavior)
 #endif
 	case MADV_DONTDUMP:
 	case MADV_DODUMP:
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	case MADV_RELEASE:
 #endif
 		return true;
@@ -498,7 +502,8 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 	int write;
 	size_t len;
 	struct blk_plug plug;
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	unsigned long start_orig = start;
 	size_t len_orig = len_in;
 #endif
@@ -583,7 +588,8 @@ out:
 	else
 		up_read(&current->mm->mmap_sem);
 
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	if (distributed_remote_process(current)) {
 		error = vma_server_madvise_remote(start_orig, len_orig, behavior);
 		if (error) return error;
diff --git a/mm/memory.c b/mm/memory.c
index 3c02373997f3..7d063f804d9e 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -78,6 +78,10 @@
 #include <popcorn/process_server.h>
 #endif
 
+#if defined(CONFIG_POPCORN_DSHM)
+extern unsigned long pophype_shmem_vma;
+#endif
+
 #if defined(LAST_CPUPID_NOT_IN_PAGE_FLAGS) && !defined(CONFIG_COMPILE_TEST)
 #warning Unfortunate NUMA and NUMA Balancing config, growing page-frame for last_cpupid.
 #endif
@@ -3152,6 +3156,25 @@ static int do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	pgoff_t pgoff = (((address & PAGE_MASK)
 			- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	/* These are file mapped faults */
+	// check if process my process
+	// TODO
+	if (current->pid > 1000 && pophype_shmem_vma &&
+			pophype_shmem_vma == vma->vm_start) {
+		static u64 cnt = 0;
+		cnt++;
+		if (cnt < 20 || !(cnt % DEBUG_SKIP_SHM_CNT)) {
+				POPPK_SHMEM("[%d] %s(): file mapped fault "
+						"err %d r %d cow %d (all0=shm) #%lld\n",
+								current->pid, __func__,
+								!vma->vm_ops->fault,
+								!(flags & FAULT_FLAG_WRITE),
+								!(vma->vm_flags & VM_SHARED), cnt);
+		}
+	}
+#endif
+
 	pte_unmap(page_table);
 	/* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
 	if (!vma->vm_ops->fault)
@@ -3287,6 +3310,10 @@ static int wp_huge_pmd(struct mm_struct *mm, struct vm_area_struct *vma,
 	return VM_FAULT_FALLBACK;
 }
 
+#ifdef CONFIG_POPCORN_DSHM
+extern unsigned long dshm_addr;
+extern unsigned long dshm_len;
+#endif
 /*
  * These routines also need to handle stuff like marking pages dirty
  * and/or accessed for architectures that don't do it in hardware (most
@@ -3322,7 +3349,29 @@ static int handle_pte_fault(struct mm_struct *mm,
 	barrier();
 #ifdef CONFIG_POPCORN
 	if (distributed_process(current)) {
-		int ret = page_server_handle_pte_fault(
+		int ret;
+#ifdef CONFIG_POPCORN_DSHM
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		static u64 cnt = 0;
+#endif
+		if (!dshm_addr || !dshm_len ||
+			!(address >= dshm_addr && address < (dshm_addr + dshm_len))) {
+			goto out;
+		}
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		cnt++;
+		POPPK_DSHMV("[%d]: handle_pte_fault addr 0x%lx hits (dshm 0x%lx ~ 0x%lx) "
+			"(current->_nid %d _pid %d at_remote %d is_worker %d) #%lld\n",
+			current->pid, address, dshm_addr, dshm_addr + dshm_len,
+			current->origin_nid, current->origin_pid,
+			current->at_remote, current->is_worker,
+			cnt);
+#else
+		POPPK_DSHM("[%d]: address 0x%lx hits (dshm 0x%lx ~ 0x%lx)\n",
+				current->pid, address, dshm_addr, dshm_addr + dshm_len);
+#endif
+#endif
+		ret = page_server_handle_pte_fault(
 				mm, vma, address, pmd, pte, entry, flags);
 		if (ret == VM_FAULT_RETRY) {
 			int backoff = ++current->backoff_weight;
@@ -3335,9 +3384,13 @@ static int handle_pte_fault(struct mm_struct *mm,
 		} else {
 			current->backoff_weight /= 2;
 		}
-		if (ret != VM_FAULT_CONTINUE) return ret;
+		if (ret != VM_FAULT_CONTINUE) return ret; // distributed && ret = 0 exit
 	}
 #endif
+
+#ifdef CONFIG_POPCORN_DSHM
+out:
+#endif
 	if (!pte_present(entry)) {
 		if (pte_none(entry)) {
 			if (vma_is_anonymous(vma))
@@ -3345,7 +3398,7 @@ static int handle_pte_fault(struct mm_struct *mm,
 							 pte, pmd, flags);
 			else
 				return do_fault(mm, vma, address, pte, pmd,
-						flags, entry);
+						flags, entry); /* File mapped faults */
 		}
 #ifdef CONFIG_POPCORN
 		page_server_panic(true, mm, address, pte, entry);
diff --git a/mm/mmap.c b/mm/mmap.c
index 9a0084358929..c14745d21b12 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -52,6 +52,8 @@
 #include <popcorn/bundle.h>
 #include <popcorn/types.h>
 #include <popcorn/vma_server.h>
+#include <../../kernel/popcorn/types.h>
+#include <popcorn/pcn_kmsg.h>
 #endif
 
 #include "internal.h"
@@ -298,7 +300,8 @@ SYSCALL_DEFINE1(brk, unsigned long, brk)
 	unsigned long min_brk;
 	bool populate;
 
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	if (distributed_remote_process(current)) {
 		while (!down_write_trylock(&mm->mmap_sem))
 			schedule();
@@ -362,7 +365,8 @@ set_brk:
 	up_write(&mm->mmap_sem);
 	if (populate)
 		mm_populate(oldbrk, newbrk - oldbrk);
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	if (distributed_remote_process(current)) {
 		if (vma_server_brk_remote(oldbrk, brk)) {
 			return brk;
@@ -1370,7 +1374,29 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 	/* Obtain the address to map to. we verify (or select) it and ensure
 	 * that it represents a valid section of the address space.
 	 */
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	int interesting = 0;
+	if (distributed_process(current) && addr == 0x7fffb6dfc000) {
+		interesting = 1;
+		POPPK_DSHM("%s(): [%d] [bf] my interesting addr 0x7fffb6dfc000\n",
+					__func__, current->pid);
+	}
+#endif
 	addr = get_unmapped_area(file, addr, len, pgoff, flags);
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	if (distributed_process(current)) {
+		//POPPK_DSHM("\n\n[%d] %s(): "
+		POPPK_MMAP("\n\n[%d] %s(): "
+				"allocated len 0x%lx = [[[%ldM]]], ret addr 0x%lx (~ [[[0x%lx]]]) "
+				"current->_nid %d _pid %d\n\n\n",
+				current->pid, __func__, len, len / 1024 / 1024, addr, addr + len,
+				current->origin_nid, current->origin_pid);
+	}
+	if (interesting) {
+		POPPK_DSHM("%s(): [af] available vma hole found addr = 0x%lx\n\n\n",
+					__func__, addr);
+	}
+#endif
 	if (offset_in_page(addr))
 		return addr;
 
@@ -1521,7 +1547,8 @@ SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,
 
 	flags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);
 
-#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
+	/* Popcorn dshm doesn't delegate request from here */
 	if (distributed_remote_process(current)) {
 		retval = vma_server_mmap_remote(file, addr, len, prot, flags, pgoff);
 		goto out_fput;
@@ -1535,6 +1562,587 @@ out_fput:
 	return retval;
 }
 
+#ifdef CONFIG_POPCORN_DSHM
+#include <popcorn/process_server.h>
+#include <linux/kthread.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+static inline struct task_struct *___get_task_struct(pid_t pid)
+{
+    struct task_struct *tsk = NULL;
+    rcu_read_lock();
+    tsk = find_task_by_vpid(pid);
+    if (likely(tsk)) {
+        get_task_struct(tsk);
+    }
+    rcu_read_unlock();
+    return tsk;
+}
+
+#include <linux/mmu_context.h>
+#include <../kernel/popcorn/util.h> // Not good
+extern pid_t gttid; // local
+//rc->remote_tgids[dst_nid] = res->ttid; save for remote
+/* create kthread inheriting user's mm for handling ops */
+/* Check remote_worker_main() at kernel/popcorn/process_server.c */
+extern void run_remote_worker(struct remote_context *rc);
+static int dshm_origin_worker_main(void *data) {
+	/* Can use input argument */
+	struct task_struct *tsk = ___get_task_struct(gttid); // parent
+	printk("[dbg] I'm worker [%d]\n", current->pid);
+	POPPK_DSHM("\t[dbg] this worker is for "
+		"dealing with VMA_OP, WORKER_TID_UPDATE, etc. "
+		"Not for PAGE_FAULT.\n");
+	POPPK_DSHM("\t[dbg] tsk %p gttid %d\n", tsk, gttid);
+	BUG_ON(!tsk);
+	POPPK_DSHM("\t[dbg] tsk->remote %p\n", tsk->remote);
+	BUG_ON(!tsk->remote);
+	POPPK_DSHM("\t[dbg] tsk->remote->mm %p\n", tsk->remote->mm);
+	BUG_ON(!tsk->remote->mm);
+	// Directly accessing these ptrs is dangerous but this is for debugging
+
+
+	// wait..... change tsk to current
+	POPPK_DSHM("\t[dbg] %s(): construct_mm/leverage_the_same_mm "
+			"(current->mm = tsk->mm %p)\n", __func__, tsk->mm);
+
+#if 0
+	struct mm_struct *mm;
+	/* construct_mm */
+	mm = mm_alloc();
+	BUG_ON(!mm);
+	// DSHM testing 1020
+	//memcpy(mm, tsk->mm, sizeof(*mm)); // testing
+
+	arch_pick_mmap_layout(mm);
+
+	char exe_path[512];
+	if (get_file_path(tsk->mm->exe_file, exe_path, sizeof(exe_path))) {
+		printk("%s: cannot get path to exe binary\n", __func__);
+		BUG();
+		//ret = -ESRCH;
+		//pcn_kmsg_put(req);
+		//goto out;
+	}
+
+	struct file *f;
+	f = filp_open(exe_path, O_RDONLY | O_LARGEFILE | O_EXCL, 0);
+	if (IS_ERR(f)) {
+		PCNPRINTK_ERR("cannot open executable from %s\n", exe_path);
+		BUG();
+		//mmdrop(mm);
+		//return -EINVAL;
+	}
+	set_mm_exe_file(mm, f);
+	filp_close(f, NULL);
+
+    mm->task_size = tsk->mm->task_size;
+    mm->start_stack = tsk->mm->start_stack;
+    mm->start_brk = tsk->mm->start_brk;
+    mm->brk = tsk->mm->brk;
+    mm->env_start = tsk->mm->env_start;
+    mm->env_end = tsk->mm->env_end;
+    mm->arg_start = tsk->mm->arg_start;
+    mm->arg_end = tsk->mm->arg_end;
+    mm->start_code = tsk->mm->start_code;
+    mm->end_code = tsk->mm->end_code;
+    mm->start_data = tsk->mm->start_data;
+    mm->end_data = tsk->mm->end_data;
+    mm->def_flags = tsk->mm->def_flags;
+
+	//mm->mm_rb.rb_node = tsk->mm->mm_rb.rb_node;
+    use_mm(mm);
+	POPPK_DSHM("[dbg] %s(): mm->mm_rb.rb_node ***%p*** "
+				"tsk->mm->mm_rb.rb_node ***%p***\n",
+				__func__, mm->mm_rb.rb_node, tsk->mm->mm_rb.rb_node);
+	POPPK_DSHM("[dbg] %s(): check my mm->start_stack 0x%lx mm->start_brk 0x%lx "
+				"mm->start_code 0x%lx mm->start_data 0x%lx\n", __func__,
+				mm->start_stack, mm->start_brk, mm->start_code, mm->start_data);
+#else // DSHM testing 1020
+	/* Use user process mm */
+	//memcpy(mm, tsk->mm, sizeof(*mm));
+	//current->mm = tsk->mm; // leverage it (should I add? I think so)
+	//use_mm(current->mm);
+	use_mm(tsk->mm); // leverage it (should I add? I think so) //  unuse_mm()?
+#endif
+
+
+	/* mm->mm_rb.rb_node is NULL
+		but we cannot assign it to tsk->mm->mm_rb.rb_node
+		otherwise it will not be able to create the same addr */
+
+	/* rc->mm set outside. Should I overwrite it with new kworker's mm? */
+    //rc->mm = mm;  /* No need to increase mm_users due to mm_alloc() */
+
+	/* The trick here is to let current inherit parent's Popcorn info */
+	/* Copy parent's info */
+	current->remote = tsk->remote;
+	current->at_remote = tsk->at_remote;
+
+	/* Match popcorn exit primitives */
+	POPPK_DSHM("\t[dbg] %s(): make this kthread Popcorn worker thread\n", __func__);
+	current->is_worker = true;
+
+	/* Main loop */
+	if (!my_nid) {
+		/* Should I use current instead of tsk? */
+		current->remote_pid = tsk->remote_pid; // ??
+		POPPK_DSHM("\t[dbg] %s(): current->remote_pid %d "
+					"current->remote->remote_tgids[1] %d\n", __func__,
+					current->remote_pid, current->remote->remote_tgids[1]);
+
+		/* comp - current's and tsk's remote_work_pended (comp) is different. */
+		POPPK_DSHM("\t[dbg] current->remote_work_pended %p "
+					"tsk->remote_work_pended %p\n",
+					&current->remote->remote_work_pended,
+					&tsk->remote->remote_work_pended);
+
+		/* Learn from remote worker */
+		current->flags &= ~PF_RANDOMIZE;    /* Disable ASLR for now*/
+		current->flags &= ~PF_KTHREAD;  /* Demote to a user thread */
+		current->personality = tsk->personality;
+		set_user_nice(current, 0);
+
+		POPPK_DSHM("\t %s: [%d] [dbg] I'm origin worker: "
+					"current->remote %p =? tsk->remote %p "
+					"(dbg curr->_nid %d _pid %d)\n",
+					__func__, current->pid, current->remote, tsk->remote,
+					current->origin_nid, current->origin_pid);
+#if 0
+		process_remote_works(tsk);
+#else
+		process_remote_works(current); // there are two copies of XXXX (parent: tsk and me, worker: current) // current doesn't work. tsk->remote_work never completed.
+#endif
+	} else {
+		/* Check remote_worker_main() */
+		might_sleep();
+
+		/* comp - is inside rc */
+
+		POPPK_DSHM("\t %s(): [[%d]] create (I'm) remote worker thread\n",
+					__func__, current->pid);
+
+		current->flags &= ~PF_RANDOMIZE;    /* Disable ASLR for now*/
+		current->flags &= ~PF_KTHREAD;  /* Demote to a user thread */
+		current->personality = tsk->personality;
+		current->origin_nid = tsk->origin_nid;
+		current->origin_pid = tsk->origin_pid;
+		set_user_nice(current, 0);
+
+#if 0
+		run_remote_worker(tsk->remote);
+#else
+		POPPK_DSHM("\t %s: [%d] [dbg] I'm remote worker: "
+					"current->remote %p =? "
+					"tsk->remote %p (dbg curr->_nid %d _pid %d)\n",
+					__func__, current->pid, current->remote, tsk->remote,
+					current->origin_nid, current->origin_pid);
+		run_remote_worker(current->remote);
+#endif
+	}
+	/* Another way is to still use user context for handling and when user exit
+		kill this kworker. But the problem is in the hanlding process, the kworker
+		still sometimes use current... */
+
+	/* deconstruct_mm */
+	POPPK_DSHM("\t[dbg][%d] %s(): deconstruct_mm (working on)\n",
+				current->pid, __func__);
+	//mmput(mm); // get_task_mm
+	//mmput(current->mm); // get_task_mm
+
+	POPPK_DSHM("\t[dbg][%d] %s(): parasiting on user's mm, "
+				"so do nothing and don't touch\n",
+				current->pid, __func__);
+
+	POPPK_DSHM("\t[dbg][%d] %s(): kworker put rc "
+				"(user thread doesn't do anything)\n",
+				current->pid, __func__);
+#if 0
+	// kernel/exit.c -> process_server_task_exit() -> exit_origin/remote_task()
+	if (current->mm) {
+		POPPK_DSHM("\t[dbg][%d] %s(): current->mm %p\n",
+					current->pid, __func__, current->mm);
+		if (current->mm->remote) {
+			POPPK_DSHM("\t[dbg][%d] %s(): current->mm->remote %p\n",
+						current->pid, __func__, current->mm->remote);
+			//put_task_remote(current); // popcorn: origin fail, or remote done does it
+		} else {
+			POPPK_DSHM("\t[dbg][%d] %s(): current->mm->remote NULL\n",
+						current->pid, __func__);
+		}
+	} else {
+		POPPK_DSHM("\t[dbg][%d] %s(): current->mm NULL\n",
+					current->pid, __func__);
+	}
+#endif
+	POPPK_DSHM("\t[dbg][%d] %s(): unuse_mm() start\n",
+				current->pid, __func__);
+	unuse_mm(current->mm);
+	POPPK_DSHM("\t[dbg][%d] %s(): unuse_mm() end\n",
+				current->pid, __func__);
+
+	POPPK_DSHM("\n\n\t\t\t[%d] %s(): kworker exit\n\n\n",
+				current->pid, __func__);
+	//mmput(mm); // get_task_mm
+	put_task_struct(tsk);
+	return 0;
+}
+
+/* copied from mmap_pgoff */
+SYSCALL_DEFINE6(pcn_dshm_mmap, unsigned long, addr, unsigned long, len,
+		unsigned long, prot, unsigned long, flags,
+		unsigned long, fd, unsigned long, pgoff)
+{
+	struct file *file = NULL;
+	unsigned long retval;
+
+	// alloc: __do_migration() kernel/popcorn/process_server.c
+	//		//// allocated
+	//
+	// print nid and set I'm at remote
+	//
+	// ////////get tid (process id) + address to register
+	// ///////		task_pid_vnr(current)
+	pid_t ttid = task_pid_vnr(current); /* int */
+	struct remote_context *rc;
+	struct task_struct *tsk = current; // TODO change at the end at once
+	int i;
+
+	//struct task_struct *g, *p;
+	struct task_struct *p;
+	POPPK_DSHM("[%d]: show thread list "
+				"(change task_struct's popcorn meta data) ->\n",
+				current->pid);
+	rcu_read_lock();
+	//for_each_process_thread(g, p) {
+	for_each_thread(current, p) {
+		POPPK_DSHM("\t\t[%d] p->_nid %d _pid %d\n",
+				p->pid, p->origin_nid, p->origin_pid);
+		if (!my_nid) { // force changing here doesn't seem a good idea
+			p->at_remote = false;
+		} else {
+			p->at_remote = true;
+		}
+	}
+	rcu_read_unlock();
+
+	/* Create worker thread and rc. Make it as a Popcorn distributed thread */
+	/* 1. Create rc (check alloc_remote_context()) */
+	POPPK_DSHM("[%d] %s(): dshm only API. My ttid is %d fd %lu\n",
+								current->pid, __func__, ttid, fd);
+	rc = alloc_remote_context(my_nid, current->tgid, my_nid ? true: false); // nid, tgid, remote
+	BUG_ON(!rc);
+	//if (IS_ERR(rc)) return PTR_ERR(rc);
+
+    for (i = 0; i < FAULTS_HASH; i++) {
+        INIT_HLIST_HEAD(&rc->faults[i]);
+        spin_lock_init(&rc->faults_lock[i]);
+    }
+
+    INIT_LIST_HEAD(&rc->vmas);
+    spin_lock_init(&rc->vmas_lock);
+
+    rc->stop_remote_worker = false;
+
+    rc->remote_worker = NULL;
+    INIT_LIST_HEAD(&rc->remote_works);
+    spin_lock_init(&rc->remote_works_lock);
+    init_completion(&rc->remote_works_ready); // remote worker
+
+	/* DSHM moves origin worker's "comp" and "work" to rc */
+    init_completion(&rc->remote_work_pended); // origin worker
+	rc->remote_work = NULL; // origin handles remote works
+
+	rc->mm = NULL;
+	memset(rc->remote_tgids, 0x00, sizeof(rc->remote_tgids));
+
+	/* 2. set up rc */
+	if (cmpxchg(&current->mm->remote, 0, rc)) { // not distributed
+		POPPK_DSHM("[%d] %s(): [not first] don't alloc rc\n", ttid, __func__);
+		kfree(rc);
+	} else {
+        rc->mm = tsk->mm; /* rc->mm uses user process's mm. Is this safe for mm_users? */
+        rc->remote_tgids[my_nid] = tsk->tgid;
+		POPPK_DSHM("[%d] %s(): [first] alloc rc. My ttid %d tgid %d. "
+					"Set up my remote_tgids[my_nid] tgid %d\n",
+					current->pid, __func__, ttid, tsk->tgid, tsk->tgid);
+
+        __lock_remote_contexts_out(dst_nid); // 2node now
+        list_add(&rc->list, &__remote_contexts_out());
+        __unlock_remote_contexts_out(dst_nid); // 2node now
+	}
+	/* At this moment, this process is distributed!!! */
+	tsk->remote = get_task_remote(tsk); // get rc (mm->remote) and save to tsk->remote // init:1 here:2
+	BUG_ON(!tsk->remote);
+
+	/* Popcorn DSHM
+		Should I assume user needs to specify interested ID
+								for supporting more regions */
+	BUG_ON(!addr);
+
+	/* RPC for handshaking/registering
+		Register my ttid at local and give it to remote when asked */
+	pcn_dshm_join(ttid, addr, len); // 2node now
+	/* Info is save in "current". Need to pass to kthread */
+	/* Handshake has registered local ttid.
+		Now create a kthread for sitting there */
+	/* At this point: since rc->remote_tgids[dst_nid] = res->ttid;
+		origin: rc->remote_tgids[dst_nid(1)]
+		remote: rc->remote_tgids[dst_nid(0)]
+		current->remote/origin_pid = res->ttid; */
+
+	/* 2. Create kworker */
+	POPPK_DSHM("check kernel/popcorn/process_server.c\n");
+	POPPK_DSHM("check __process_remote_works(): origin spin\n");
+	POPPK_DSHM("check clone_remote_thread(): remote create kthread\n");
+	// forget about contect_in/out
+	// assume I know its rc
+	//__lock_remote_contexts_in(nid_from);
+	if (!my_nid) {
+		/* Set popcorn_distributed identities */
+		current->at_remote = false;
+		/* TODO check, may need more */
+		current->origin_nid = 1; // hardcode
+
+
+		/* Note: Traditional Popcorn's rc->remote_wrker at origin is NULL */
+        rc->remote_worker =
+                kthread_run(dshm_origin_worker_main,
+							NULL, "dshm_origin_worker_main");
+		POPPK_DSHM("\n\t\torigin worker [[[[%d]]]] created\n\n",
+					rc->remote_worker->pid);
+		POPPK_DSHM("let remote go first\n");
+		//msleep(5000);
+	} else {
+		/* check remote_worker_main(rc, req) */
+        //__unlock_remote_contexts_in(nid_from);
+		POPPK_DSHM("Need to handle munmap or EXIT reqs. + sleep 5s\n");
+
+		/* Set popcorn_distributed identities */
+		current->at_remote = true;
+		current->origin_nid = 0;
+
+		POPPK_DSHM("rc->remote_tgids[current->origin_nid] %d\n",
+					rc->remote_tgids[current->origin_nid]);
+		while (!rc->remote_tgids[current->origin_nid]) {
+			POPPK_DSHM("rc->remote_tgids[current->origin_nid] %d\n",
+						rc->remote_tgids[current->origin_nid]);
+			schedule();
+		}
+		POPPK_DSHM("rc->remote_tgids[current->origin_nid] %d\n",
+					rc->remote_tgids[current->origin_nid]);
+		/* ref: __delegate_vma_op() __alloc_vma_op_request() */
+		/* Update origin worker pid */
+		current->origin_pid = rc->remote_tgids[current->origin_nid];
+		current->tgid = rc->remote_tgids[current->origin_nid];;
+		rc->tgid = rc->remote_tgids[current->origin_nid];;
+		/* Remote worker for compatipility to Popcorn:
+			1. handle VMA OP like munmap()
+			2. exit clean remote main worker */
+		POPPK_DSHM(" [WORKING ON] create remote main worker\n");
+        rc->remote_worker =
+                kthread_run(dshm_origin_worker_main,
+							NULL, "dshm_origin_worker_main");
+		//gttid = rc->remote_worker->pid; // this will destroy dshm_origin_worker_main creation
+		POPPK_DSHM("\n\t\tremote worker [[[[%d]]]] created\n\n",
+					rc->remote_worker->pid);
+		/* TODO check, may need more */
+		POPPK_DSHM("[HACK but should work well] let origin go first. "
+			"since the latter one can use the logic of vma_server_fetch_vma()\n");
+		msleep(2000);
+	}
+	printk("\n<workers created>\n\n");
+	msleep(5000); // Since kworker may not be ready yet.
+	// sending UPDATE reqs too early will hang.......
+
+	/* Option 1: can current exchange kworker's pid and unblock the kworkers? */
+	/* Not taken now */
+
+	/* Adjust process's tid to kthread's tid */
+	if (!my_nid) {
+	} else {
+		//rc->remote_tgids[current->origin_nid] = 0;
+		//while (!rc->remote_tgids[current->origin_nid]) {
+		//	POPPK_DSHM("rc->remote_tgids[current->origin_nid] %d\n",
+		//				rc->remote_tgids[current->origin_nid]);
+		//	schedule();
+		//}
+		/* This make sure the pcn_dshm_join() will update the pid again. */
+	}
+	/* This make sure the pcn_dshm_join() will update the pid again. */
+	//pcn_dshm_join(rc->remote_worker->pid, addr, len); // 2node now
+
+#if 1
+	if (!my_nid) { /* origin: 2 node now */
+		int dst_nid = 1;
+		update_origin_worker_pid_t *req = pcn_kmsg_get(sizeof(*req));
+		BUG_ON(!req);
+		req->remote_pid = rc->remote_tgids[dst_nid]; // dst
+		req->ttid = rc->remote_worker->pid;
+		POPPK_DSHM("[dbg] %s(): UPDATE req to [current->remote_pid %d] -> "
+					"update to rc->remote_worker->pid %d\n", __func__,
+					current->remote_pid, rc->remote_worker->pid);
+		POPPK_DSHM("\n\n\t\t\t");
+		POPPK_DSHM("%s(): [dbg][%d]: rc->tgid [[[[%d]]] (old)(not rkworker yet)\n",
+			__func__, current->pid, current->mm->remote->remote_tgids[dst_nid]);
+		current->mm->remote->remote_tgids[dst_nid] = current->origin_pid;
+		POPPK_DSHM("%s(): [dbg][%d]: rc->tgid [[[[%d]]] (new)(not rkworker yet)\n",
+			__func__, current->pid, current->mm->remote->remote_tgids[dst_nid]);
+		BUG_ON(pcn_kmsg_post(
+			//PCN_KMSG_TYPE_UPDATE_REMOTE_WORKER_PID, dst_nid, req, sizeof(*req)));
+			PCN_KMSG_TYPE_UPDATE_ORIGIN_WORKER_PID, dst_nid, req, sizeof(*req)));
+		//msleep(4000);
+	} else { /* remote: 2 node now */
+		int dst_nid = 0;
+		/* Update origin worker pid */
+		update_remote_worker_pid_t *req = pcn_kmsg_get(sizeof(*req));
+		BUG_ON(!req);
+		req->origin_pid = current->origin_pid; // dst
+		req->ttid = rc->remote_worker->pid;
+		POPPK_DSHM("[dbg] %s(): UPDATE req to [current->origin_pid %d] -> "
+					"update to rc->remote_worker->pid %d\n", __func__,
+					current->origin_pid, rc->remote_worker->pid);
+		POPPK_DSHM("\n\n\t\t\t");
+		POPPK_DSHM("%s(): [dbg][%d]: rc->tgid [[[[%d]]] (old)\n", __func__,
+				current->pid, current->mm->remote->remote_tgids[dst_nid]);
+		current->mm->remote->remote_tgids[dst_nid] = current->origin_pid;
+		POPPK_DSHM("%s() [dbg][%d]: rc->tgid [[[[%d]]] (new)\n", __func__,
+				current->pid, current->mm->remote->remote_tgids[dst_nid]);
+		POPPK_DSHM("worker for vma op. tgid just for task_struct->mm (pgfaults)\n"
+					"Would here too late?\n\n");
+		BUG_ON(pcn_kmsg_post(
+			//PCN_KMSG_TYPE_UPDATE_ORIGIN_WORKER_PID, dst_nid, req, sizeof(*req)));
+			PCN_KMSG_TYPE_UPDATE_REMOTE_WORKER_PID, dst_nid, req, sizeof(*req)));
+		msleep(3000); /* make sure popcorn origin runs before than remote */
+		printk("\n<sent update info> sleep 3s for making sure others (!origin) run first\n\n");
+	}
+    rc->remote_tgids[my_nid] = rc->remote_worker->pid; // should I update mytgid to worker's pid?
+
+	printk("\n<sent update info> sleep 5s for easier debugging\n\n");
+	msleep(5000);
+#else
+	printk("\nskip update worker's pid\n\n");
+#endif
+	/* Both workers can work properly. Now exchange workers info again. */
+	//rc->remote_worker->pid;
+
+	//if (current->at_remote) {
+	POPPK_DSHM("[dbg][%d]: rc->tgid [[[[%d]]] (outdated here)\n",
+			current->pid, current->mm->remote->remote_tgids[0]);
+	//}
+	POPPK_DSHM("[%d]: check thread list again ->\n",
+				current->pid);
+	rcu_read_lock();
+	for_each_thread(current, p) { // kthread worker is not shown here
+		POPPK_DSHM("\t\t[%d] p->_nid %d _pid %d\n",
+				p->pid, p->origin_nid, p->origin_pid);
+		if (current != p && (current->origin_pid != p->origin_pid)) {
+				/* Corner case: overwrite the first vcpu's wrong fault */
+				//(p->origin_nid < 0 || p->origin_pid < 0)) {
+			p->origin_nid = current->origin_nid;
+			p->origin_pid = current->origin_pid;
+			POPPK_DSHM("\t\t\t[%d] p->_nid %d _pid %d (overwrite)\n",
+						p->pid, p->origin_nid, p->origin_pid);
+		}
+	}
+	rcu_read_unlock();
+
+	/* 3. VMA part - leverage Popcorn */
+	printk("\ndo Popcorn VMA\n\n");
+	if (!(flags & MAP_ANONYMOUS)) {
+		WARN_ON("!anon mmap\n");
+		audit_mmap_fd(fd, flags);
+		file = fget(fd);
+		if (!file)
+			return -EBADF;
+		if (is_file_hugepages(file))
+			len = ALIGN(len, huge_page_size(hstate_file(file)));
+		retval = -EINVAL;
+		if (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))
+			goto out_fput;
+	} else if (flags & MAP_HUGETLB) {
+		struct user_struct *user = NULL;
+		struct hstate *hs;
+
+		WARN_ON("!hugetlb\n");
+		hs = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & SHM_HUGE_MASK);
+		if (!hs)
+			return -EINVAL;
+
+		len = ALIGN(len, huge_page_size(hs));
+		/*
+		 * VM_NORESERVE is used because the reservations will be
+		 * taken when vm_ops->mmap() is called
+		 * A dummy user value is used because we are not locking
+		 * memory so no accounting is necessary
+		 */
+		file = hugetlb_file_setup(HUGETLB_ANON_FILE, len,
+				VM_NORESERVE,
+				&user, HUGETLB_ANONHUGE_INODE,
+				(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
+		if (IS_ERR(file))
+			return PTR_ERR(file);
+	}
+
+	flags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);
+#ifdef CONFIG_POPCORN_DSHM
+	// Remove these two flags
+
+	POPPK_DSHM("[%d] %s(): anon mmap (prot 0x%lx)\n",
+				current->pid, __func__, prot);
+	// prot e.g., PROT_READ
+	if (distributed_remote_process(current)) {
+		/* Don't mmap but find & replay vma. Ask origin. */
+		down_read(&current->mm->mmap_sem); // match vma_server_fetch_vma()
+		POPPK_DSHM("[%d] %s remote fetch vma\n",
+					current->pid, __func__);
+		retval = vma_server_fetch_vma(current, addr);
+		if (!retval)
+			retval = addr;
+
+		POPPK_DSHM("[%d] %s addr %lx (O) ret %lx (got)\n",
+					current->pid, __func__, addr, retval);
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		// check vma info because debugging cow
+		{
+			struct vm_area_struct * vma = find_vma(current->mm, retval);
+			POPPK_DSHM("[%d] %s addr %lx: vma_is_anonymous(vma) (%c)?\n",
+						current->pid, __func__, retval,
+						vma_is_anonymous(vma) ? 'O' : 'X');
+		}
+
+		// Sanity check
+		if (unlikely(retval != addr)) {
+			printk(KERN_ERR "vma_server_fetch_vma fail\n");
+		}
+
+		BUG_ON(!find_vma(current->mm, addr));
+#endif
+		up_read(&current->mm->mmap_sem); // match vma_server_fetch_vma()
+		goto out_fput;
+	}
+#endif
+
+	retval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);
+	POPPK_DSHM("[%d] %s(): vm_mmap_pgoff done\n",
+							current->pid, __func__);
+out_fput:
+	if (file)
+		fput(file);
+	POPPK_DSHM("[%d] %s(): fd %lu Done\n\n\n",
+				current->pid, __func__, fd);
+	return retval;
+}
+#else
+SYSCALL_DEFINE6(pcn_dshm_mmap, unsigned long, addr, unsigned long, len,
+		unsigned long, prot, unsigned long, flags,
+		unsigned long, fd, unsigned long, pgoff)
+{
+    PCNPRINTK_ERR("Kernel is not configured to use Popcorn DSHM\n");
+    return -EPERM;
+}
+#endif
+
 #ifdef __ARCH_WANT_SYS_OLD_MMAP
 struct mmap_arg_struct {
 	unsigned long addr;
@@ -2129,6 +2737,17 @@ struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)
 
 	rb_node = mm->mm_rb.rb_node;
 
+#ifdef CONFIG_POPCORN_DSHM
+	if (!rb_node) {
+		printk("rb_node NULL\n");
+	}
+	if (addr == 0x7fffb6dfc000) {
+		//req->remote_pid
+		printk("[dbg] %s(): (0x7fffb6dfc000) addr %lx rb_node %p\n",
+				__func__, addr, rb_node);
+	}
+#endif
+
 	while (rb_node) {
 		struct vm_area_struct *tmp;
 
@@ -2716,7 +3335,8 @@ int vm_munmap(unsigned long start, size_t len)
 	int ret;
 	struct mm_struct *mm = current->mm;
 
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	if (distributed_process(current)) {
 		while (!down_write_trylock(&mm->mmap_sem))
 			schedule();
@@ -2736,7 +3356,9 @@ SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)
 {
 	profile_munmap(addr);
 
-#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
+	/* Since pcn_dshm_mmap() has made the thread distributed,
+		we don't wanna handle all request but only our APIs */
 	if (unlikely(distributed_process(current))) {
 		if (current->at_remote) {
 			return vma_server_munmap_remote(addr, len);
@@ -2748,6 +3370,23 @@ SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len)
 }
 
 
+SYSCALL_DEFINE2(pcn_dshm_munmap, unsigned long, addr, size_t, len)
+{
+#ifdef CONFIG_POPCORN_DSHM
+	if (distributed_process(current)) {
+		POPPK_DSHM("\n\n[%d] %s(): DSHM munmap() -> barrier() -> vm_munmap()\n",
+					current->pid, __func__);
+		return popcorn_distributed_barrier(addr, len);
+	} else {
+		printk("%s(): not a popcorn thread\n", __func__);
+		BUG();
+	}
+	return vm_munmap(addr, len);
+#else
+	BUG();
+#endif
+}
+
 /*
  * Emulation of deprecated remap_file_pages() syscall.
  */
diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c
index 5fbdd367bbed..8fadab47156c 100644
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -181,6 +181,12 @@ void __mmu_notifier_invalidate_page(struct mm_struct *mm,
 
 	id = srcu_read_lock(&srcu);
 	hlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list, hlist) {
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+		//if (address == 0x7ffd64000000) {
+		//	printk("%s %s(): mn->ops->invalidate_page() %p\n",
+		//		__FILE__, __func__, mn->ops->invalidate_page);
+		//}
+#endif
 		if (mn->ops->invalidate_page)
 			mn->ops->invalidate_page(mn, mm, address);
 	}
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 12c62604b18d..c0bc3a87b869 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -372,7 +372,8 @@ SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 	if (!arch_validate_prot(prot))
 		return -EINVAL;
 
-#ifdef CONFIG_POPCORN
+//#ifdef CONFIG_POPCORN
+#if defined(CONFIG_POPCORN) && !defined(CONFIG_POPCORN_DSHM)
 	if (distributed_remote_process(current)) {
 		error = vma_server_mprotect_remote(start, len, prot);
 		if (error) return error;
diff --git a/mm/shmem.c b/mm/shmem.c
index 1b11ccc0a3b7..4a9d58b08e9b 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -84,6 +84,8 @@ static struct vfsmount *shm_mnt;
 /* Symlink up to this size is kmalloc'ed instead of using a swappable page */
 #define SHORT_SYMLINK_LEN 128
 
+//#include <popcorn/debug.h>
+
 /*
  * shmem_fallocate communicates with shmem_fault or shmem_writepage via
  * inode->i_private (with i_mutex making sure that it has only one user at
@@ -1292,6 +1294,30 @@ static int shmem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	int error;
 	int ret = VM_FAULT_LOCKED;
 
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_SHMEM
+	static u64 cnt = 0, cnt2 = 0;
+	cnt++;
+	if (current->pid > 1000) // pid debug - heuristic
+		cnt2++;
+
+	if (cnt < 5 || !(cnt % DEBUG_SKIP_SHM_CNT) ||
+			(current->pid > 1000 && cnt2 < 5) ||
+			(current->pid > 1000 && !(cnt2 % DEBUG_SKIP_SHM_CNT))) {
+		// pid debug - heuristic
+		POPPK_SHMEM("[%d] %s(): %lx - %lx ofs %lx (pgs) pgf@ %p #%llu #%llu\n",
+					current->pid, __func__,
+					vma ? vma->vm_start : 0,
+					vma ? vma->vm_end : 0,
+					vmf ? vmf->pgoff : 0, // (pgs)
+					vmf ? vmf->virtual_address : 0, cnt, cnt2);
+		//dump_stack();
+		// page_fault -> do_page_fault (arch) -> __do_page_fault (arch)
+		//	-> handle_mm_fault (comm) -> handle_pte_fault() (DeX)
+		//	-> (!present&&!pte&&!anon) do_fault (TODO)
+		//			-> do_shared_fault (or do_cow_fault/do_read_fault)
+		//	-> __do_fault -> shmem_fault
+	}
+#endif
 	/*
 	 * Trinity finds that probing a hole which tmpfs is punching can
 	 * prevent the hole-punch from ever completing: which in turn
@@ -1404,8 +1430,41 @@ out_nomem:
 	return retval;
 }
 
+#if defined(CONFIG_POPCORN_DSHM)
+unsigned long pophype_shmem_vma = 0x0; // only support one vma on a node now
+//TODO extern it in mm/memory.c
+#endif
 static int shmem_mmap(struct file *file, struct vm_area_struct *vma)
 {
+#if defined(CONFIG_POPCORN_DSHM) && POP_DBG_DSHM
+	// struct file { at ./include/linux/fs.h
+	POPPK_SHMEM("[%d] %s(): \"%s\" %lx - %lx link to shmem_fault\n",
+					current->pid, __func__, file->f_path.dentry->d_iname,
+					vma ? vma->vm_start : 0,
+					vma ? vma->vm_end : 0);
+	//dump_stack();
+	// SyS_mmap -> SyS_mmap_pgoff -> vm_mmap_pgoff
+	//	-> do_mmap -> mmap_region -> shmem_mmap
+
+	// TODO use syscall to register my process instead of auto detecting pid>1000 (trick)
+	if (current->pid > 1000) {
+		BUG_ON(!vma);
+		if (!pophype_shmem_vma) {
+			POPPK_SHMEM("[%d] %s(): save vma 0x%lx\n",
+						current->pid, __func__, vma->vm_start);
+		} else {
+			if (pophype_shmem_vma == vma->vm_start) {
+				POPPK_SHMEM("[%d] %s(): SHMEM joined vma 0x%lx\n",
+								current->pid, __func__, vma->vm_start);
+			} else { //if more, WARN
+				POPPK_SHMEM(KERN_WARNING "[WARNING][%d] %s(): SHMEM old vma 0x%lx -> 0x%lx (new) "
+						"(This happens. Seems file-mapped.)\n",
+						current->pid, __func__, pophype_shmem_vma, vma->vm_start);
+			}
+		}
+		pophype_shmem_vma = vma->vm_start;
+	}
+#endif
 	file_accessed(file);
 	vma->vm_ops = &shmem_vm_ops;
 	return 0;
@@ -2072,6 +2131,10 @@ static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 	pgoff_t start, index, end;
 	int error;
 
+#if defined(CONFIG_POPCORN_DSHM)
+	POPPK_SHMEM("[%d] %s(): len %llx ofs %llx\n",
+			current->pid, __func__, len, offset);
+#endif
 	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))
 		return -EOPNOTSUPP;
 
@@ -2442,6 +2505,10 @@ static int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *s
 	char *kaddr;
 	struct shmem_inode_info *info;
 
+#if defined(CONFIG_POPCORN_DSHM)
+	POPPK_SHMEM("[%d] %s(): \"%s\"\n",
+			current->pid, __func__, symname);
+#endif
 	len = strlen(symname) + 1;
 	if (len > PAGE_CACHE_SIZE)
 		return -ENAMETOOLONG;
@@ -3229,6 +3296,10 @@ int __init shmem_init(void)
 {
 	int error;
 
+#if defined(CONFIG_POPCORN_DSHM)
+	POPPK_SHMEM("[init][%d] %s():\n", current->pid, __func__);
+#endif
+
 	/* If rootfs called this, don't re-init */
 	if (shmem_inode_cachep)
 		return 0;
@@ -3340,6 +3411,11 @@ static struct file *__shmem_file_setup(const char *name, loff_t size,
 	if (shmem_acct_size(flags, size))
 		return ERR_PTR(-ENOMEM);
 
+#if defined(CONFIG_POPCORN_DSHM)
+	POPPK_SHMEM("[sysv][%d] %s(): \"%s\"\n", current->pid, __func__, name);
+	//dump_stack();
+	// [sysv] SyS_shmget -> ipcget -> newseg -> shmem_kernel_file_setup
+#endif
 	res = ERR_PTR(-ENOMEM);
 	this.name = name;
 	this.len = strlen(name);
diff --git a/msg_layer/common.h b/msg_layer/common.h
index b2364c43a66c..4862bf0442e6 100644
--- a/msg_layer/common.h
+++ b/msg_layer/common.h
@@ -19,6 +19,12 @@
 #include "config.h"
 
 #define MAX_NUM_NODES		ARRAY_SIZE(ip_addresses)
+#define MAX_CONN_PER_NODE	1
+#if (MAX_CONN_PER_NODE > 1)
+#define MULTI_MSG_CANNEL_PER_NODE 1
+#else
+#define MULTI_MSG_CANNEL_PER_NODE 0
+#endif
 
 static uint32_t ip_table[MAX_NUM_NODES] = { 0 };
 
diff --git a/msg_layer/config.h b/msg_layer/config.h
index 435283311371..60da165725eb 100644
--- a/msg_layer/config.h
+++ b/msg_layer/config.h
@@ -6,8 +6,39 @@
  */
 
 const char *ip_addresses[] = {
-	/* Node 0 */ "10.4.4.100",
-	/* Node 1 */ "10.4.4.101",
+	/* Node 0 */ "10.2.10.17",
+	/* Node 0 */ "10.2.10.16",
+	/* Node 1 */ "10.2.10.15",
+	/* Node 2 */ "10.2.10.14",
+	/* Node 0 */ "10.2.10.13",
+	/* Node 0 */ "10.2.10.12",
+	/* Node 1 */ "10.2.10.11",
+	/* Node 2 */ "10.2.10.10",
+
+//	/* Node 0 */ "10.1.10.17",
+//	/* Node 0 */ "10.1.10.16",
+//	/* Node 1 */ "10.1.10.15",
+//	/* Node 2 */ "10.1.10.14",
+//	/* Node 0 */ "10.1.10.13",
+//	/* Node 0 */ "10.1.10.12",
+//	/* Node 1 */ "10.1.10.11",
+//	/* Node 2 */ "10.1.10.10",
+
+
+//	/* Node 0 */ "10.2.10.10",
+//	/* Node 0 */ "10.2.10.11",
+//	/* Node 1 */ "10.2.10.12",
+//	/* Node 2 */ "10.2.10.13",
+//	/* Node 0 */ "10.2.10.14",
+//	/* Node 0 */ "10.2.10.15",
+//	/* Node 1 */ "10.2.10.16",
+//	/* Node 2 */ "10.2.10.17",
+
+//	/* Node 0 */ "10.1.10.17",
+//	/* Node 0 */ "10.1.10.16",
+//	/* Node 1 */ "10.1.10.15",
+//	/* Node 2 */ "10.1.10.14",
+	///* Node 3 */ "10.1.12.123",
 	/*   ...  */
 };
 
diff --git a/msg_layer/msg_test.c b/msg_layer/msg_test.c
index 41614070c1aa..1eca5ef984cf 100644
--- a/msg_layer/msg_test.c
+++ b/msg_layer/msg_test.c
@@ -9,13 +9,26 @@
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 
+//#include <popcorn/sync.h>
 #include "../../kernel/popcorn/types.h"
 #include "common.h"
 
-#define MAX_THREADS 32
+#ifdef CONFIG_X86_64
+//#define MAX_THREADS 16
+#else
+//#define MAX_THREADS 96
+#endif
+#define MAX_THREADS 288
+
+
 #define DEFAULT_PAYLOAD_SIZE_KB	4
 #define DEFAULT_NR_ITERATIONS 1
 
+static int cnt = 0; /* 4's remote info */
+#define REMOTE_HANDLE_WRITE_TIME 0
+#define LOCAL_RR_PAGE_TIME 0
+
+
 enum TEST_REQUEST_FLAG {
 	TEST_REQUEST_FLAG_REPLY = 0,
 	TEST_REQUEST_FLAG_RDMA_WRITE = 1,
@@ -37,16 +50,31 @@ DEFINE_PCN_KMSG(test_request_t, TEST_REQUEST_FIELDS);
 	unsigned long flags;
 DEFINE_PCN_KMSG(test_rdma_request_t, TEST_RDMA_REQUEST_FIELDS);
 
+#define TEST_RDMA_DSMRR_REQUEST_FIELDS \
+	dma_addr_t rdma_addr; \
+	u32 rdma_key; \
+	size_t size; \
+	int id; \
+	unsigned long done; \
+	unsigned long flags;
+DEFINE_PCN_KMSG(test_dsmrr_request_t, TEST_RDMA_DSMRR_REQUEST_FIELDS);
+
 #define TEST_RESPONSE_FIELDS \
 	unsigned long done;
 DEFINE_PCN_KMSG(test_response_t, TEST_RESPONSE_FIELDS);
 
+#define TEST_PAGE_RESPONSE_FIELDS \
+	unsigned long done; \
+	int id;
+DEFINE_PCN_KMSG(test_page_response_t, TEST_PAGE_RESPONSE_FIELDS);
 
 enum test_action {
 	TEST_ACTION_SEND = 0,
 	TEST_ACTION_POST,
 	TEST_ACTION_RDMA_WRITE,
 	TEST_ACTION_RDMA_READ,
+	TEST_ACTION_DSM_RR,
+	TEST_ACTION_CLEAR = 9,
 	TEST_ACTION_MAX,
 };
 
@@ -326,35 +354,44 @@ static int kthread_rdma_farm2_data(void* arg0)
 /**
  * Fundamental performance tests
  */
+char per_t_buf[MAX_THREADS][PCN_KMSG_MAX_SIZE];
 static int test_send(void *arg)
 {
 	struct test_params *param = arg;
 	DECLARE_COMPLETION_ONSTACK(done);
-	test_request_t *req;
+	test_request_t *req = (void *)per_t_buf[param->tid];
+	//test_request_t *req;
+	//char buffer[256];
 	int i;
-	char buffer[256];
 	size_t msg_size = PCN_KMSG_SIZE(param->payload_size);
 
+	printk("pid: %d\n", current->pid);
+
 	__barrier_wait(param->barrier);
 	for (i = 0; i < param->nr_iterations; i++) {
+#if 0
 		if (msg_size > sizeof(buffer)) {
-			req = kmalloc(sizeof(msg_size), GFP_KERNEL);
+			//req = kmalloc(sizeof(msg_size), GFP_KERNEL); /* BUG */
+			req = kmalloc(msg_size, GFP_KERNEL);
 			BUG_ON(!req);
 		} else {
 			req = (void *)buffer;
 		}
-
-		req->flags = 0;
-		set_bit(TEST_REQUEST_FLAG_REPLY, &req->flags);
+#endif
+		req->flags = 1;
+//		req->flags = 0;
+//		set_bit(TEST_REQUEST_FLAG_REPLY, &req->flags); // alignment fault on ARM
 		req->done = (unsigned long)&done;
 		*(unsigned long *)req->msg = 0xcafe00dead00beef;
 
 		pcn_kmsg_send(PCN_KMSG_TYPE_TEST_REQUEST, !my_nid, req, msg_size);
 
 		wait_for_completion(&done);
+#if 0
 		if (msg_size > sizeof(buffer)) {
 			kfree(req);
 		}
+#endif
 	}
 	__barrier_wait(param->barrier);
 	return 0;
@@ -371,8 +408,9 @@ static int test_post(void *arg)
 	for (i = 0; i < param->nr_iterations; i++) {
 		req = pcn_kmsg_get(PCN_KMSG_SIZE(param->payload_size));
 
-		req->flags = 0;
-		set_bit(TEST_REQUEST_FLAG_REPLY, &req->flags);
+		req->flags = 1;
+//		req->flags = 0;
+//		set_bit(TEST_REQUEST_FLAG_REPLY, &req->flags); // alignment fault on ARM
 		req->done = (unsigned long)&done;
 		*(unsigned long *)req->msg = 0xcafe00dead00beef;
 
@@ -451,6 +489,333 @@ static int test_rdma_read(void *arg)
 	return 0;
 }
 
+static int test_rdma_dsm_rr(void *arg)
+{
+	int i, my_id;
+	struct test_params *param = arg;
+
+#if LOCAL_RR_PAGE_TIME
+	/* TODO per t */
+	ktime_t dt1, t1e, t1s;
+	ktime_t t2e, t2s;
+	ktime_t t3e, t3s;
+	ktime_t t4e, t4s;
+	ktime_t t5e, t5s;
+	long long t2 = 0, t3 = 0, t4 = 0, t5 = 0;
+#endif
+
+#if 0
+    remote_page_response_t *rp;
+    struct wait_station *ws = get_wait_station(tsk);
+    struct pcn_kmsg_rdma_handle *rh = NULL;
+    remote_page_request_t *req; //
+
+	// t1: get send buffer/rdma buffer from pool
+    req = pcn_kmsg_get(sizeof(*req));
+	req->origin_ws = ws->ws_id;
+
+	rh = pcn_kmsg_pin_rdma_buffer(NULL, PAGE_SIZE);
+	if (IS_ERR(rh)) {
+		pcn_kmsg_put(req);
+		return PTR_ERR(rh);
+	}
+	req->rdma_addr = rh->dma_addr;
+	req->rdma_key = rh->rkey;
+
+	// t1
+
+	// t2
+    pcn_kmsg_post(PCN_KMSG_TYPE_REMOTE_PAGE_REQUEST, //
+					from_nid, req, sizeof(*req));
+	// t2 end
+    rp = wait_at_station(ws);
+#endif
+
+	/* write */
+	DECLARE_COMPLETION_ONSTACK(done);
+	test_dsmrr_request_t *req;
+	struct pcn_kmsg_rdma_handle *rh;
+
+	/* Warm up */
+	req = pcn_kmsg_get(sizeof(*req));
+	rh = pcn_kmsg_pin_rdma_buffer(NULL, PAGE_SIZE); /* max write size */
+	BUG_ON(!rh || !req);
+	req->rdma_addr = rh->dma_addr;
+	req->id = param->tid;
+	req->rdma_key = rh->rkey;
+	req->size = param->payload_size;
+	req->done = (unsigned long)&done;
+	*(unsigned long *)rh->addr = 0xcafecaf00eadcafe; // touch. need?
+//	printk("[%d] %p sent ->\n", req->id, req);
+	pcn_kmsg_post(PCN_KMSG_TYPE_TEST_RDMA_DSMRR_REQUEST,
+							!my_nid, req, sizeof(*req));
+	wait_for_completion(&done);
+	pcn_kmsg_unpin_rdma_buffer(rh);
+	//pcn_kmsg_put(req);
+
+//	printk("[%d] wait barrier\n", req->id);
+	__barrier_wait(param->barrier);
+//	printk("[%d] warmup done = benchmark start\n", req->id);
+#if LOCAL_RR_PAGE_TIME
+	t1s = ktime_get();
+#endif
+	for (i = 0; i < param->nr_iterations; i++) {
+#if LOCAL_RR_PAGE_TIME
+		// t2
+		t2s = ktime_get();
+#endif
+//		printk("[%d] iter %d\n", req->id, i);
+		req = pcn_kmsg_get(sizeof(*req));
+		rh = pcn_kmsg_pin_rdma_buffer(NULL, PAGE_SIZE);
+		req->rdma_addr = rh->dma_addr;
+		req->rdma_key = rh->rkey;
+		req->size = param->payload_size;
+		req->done = (unsigned long)&done;
+		req->id = my_id;
+		//*(unsigned long *)rh->addr = 0xcafecaf00eadcafe; // touch. need?
+#if LOCAL_RR_PAGE_TIME
+		t2e = ktime_get();
+		t2 += ktime_to_ns(ktime_sub(t2e, t2s));
+		// t2
+#endif
+
+#if LOCAL_RR_PAGE_TIME
+		// t3
+		t3s = ktime_get();
+#endif
+		pcn_kmsg_post(PCN_KMSG_TYPE_TEST_RDMA_DSMRR_REQUEST,
+								!my_nid, req, sizeof(*req));
+#if LOCAL_RR_PAGE_TIME
+		t3e = ktime_get();
+		t3 += ktime_to_ns(ktime_sub(t3e, t3s));
+		// t3
+#endif
+
+#if LOCAL_RR_PAGE_TIME
+		// t4
+		t4s = ktime_get();
+#endif
+		wait_for_completion(&done);
+#if LOCAL_RR_PAGE_TIME
+		t4e = ktime_get();
+		t4 += ktime_to_ns(ktime_sub(t4e, t4s));
+		// t4
+#endif
+
+#if LOCAL_RR_PAGE_TIME
+		// t5
+		t5s = ktime_get();
+#endif
+		pcn_kmsg_unpin_rdma_buffer(rh);
+		//pcn_kmsg_put(req);
+#if LOCAL_RR_PAGE_TIME
+		t5e = ktime_get();
+		t5 += ktime_to_ns(ktime_sub(t5e, t5s));
+		// t5
+#endif
+	}
+#if LOCAL_RR_PAGE_TIME
+	t1e = ktime_get();
+	dt1 = ktime_sub(t1e, t1s);
+	//dt2 = ktime_sub(t2e, t2s);
+	//dt3 = ktime_sub(t3e, t3s);
+	//dt4 = ktime_sub(t4e, t4s);
+	//dt5 = ktime_sub(t5e, t5s);
+	/* TODO per t */
+	printk("%s(): dsm rr lat done %lld ns %lld us!!!\n",
+					__func__, ktime_to_ns(dt1) / param->nr_iterations,
+					ktime_to_ns(dt1) / param->nr_iterations / 1000);
+	printk("t2 %lld ns %lld us!!!\n",
+					t2 / param->nr_iterations,
+					t2 / param->nr_iterations / 1000);
+	printk("t3 %lld ns %lld us!!!\n",
+					t3 / param->nr_iterations,
+					t3 / param->nr_iterations / 1000);
+	printk("t4 %lld ns %lld us!!!\n",
+					t4 / param->nr_iterations,
+					t4 / param->nr_iterations / 1000);
+	printk("t5 %lld ns %lld us!!!\n",
+					t5 / param->nr_iterations,
+					t5 / param->nr_iterations / 1000);
+
+
+	printk("\n\n");
+#endif
+
+	//
+	// send
+	//
+#if 0
+	struct test_params *param = arg;
+	DECLARE_COMPLETION_ONSTACK(done);
+	test_request_t *req;
+	int i;
+	char buffer[256];
+	size_t msg_size = PCN_KMSG_SIZE(param->payload_size);
+
+	__barrier_wait(param->barrier);
+	for (i = 0; i < param->nr_iterations; i++) {
+		if (msg_size > sizeof(buffer)) {
+			req = kmalloc(sizeof(msg_size), GFP_KERNEL);
+			BUG_ON(!req);
+		} else {
+			req = (void *)buffer;
+		}
+
+		req->flags = 0;
+		set_bit(TEST_REQUEST_FLAG_REPLY, &req->flags);
+		req->done = (unsigned long)&done;
+		*(unsigned long *)req->msg = 0xcafe00dead00beef;
+
+		pcn_kmsg_send(PCN_KMSG_TYPE_TEST_REQUEST, !my_nid, req, msg_size);
+
+		wait_for_completion(&done);
+		if (msg_size > sizeof(buffer)) {
+			kfree(req);
+		}
+	}
+	__barrier_wait(param->barrier);
+#endif
+	__barrier_wait(param->barrier);
+	return 0;
+}
+
+static int test_clear_all(void *arg)
+{
+	//printk("remove me: cnt brfore = %d\n", cnt);
+	cnt = 0;
+	//printk("remove me: cnt after = %d\n", cnt);
+	return 0;
+}
+
+void *_buffer[MAX_POPCORN_THREADS] = {NULL}; /* For RDMA write */
+/* For remote handling time */
+#define ITER 1000001
+#define ONE_M 1000000
+static void process_test_dsmrr_request(struct work_struct *work)
+{
+
+//  send example from DSM
+//	START_KMSG_WORK(test_request_t, req, work);
+//	if (test_bit(TEST_REQUEST_FLAG_REPLY, &req->flags)) {
+//		test_response_t *res = pcn_kmsg_get(sizeof(*res));
+//		res->done = req->done;
+//
+//		pcn_kmsg_post(PCN_KMSG_TYPE_TEST_RESPONSE,
+//				PCN_KMSG_FROM_NID(req), res, sizeof(*res));
+//	}
+//	END_KMSG_WORK(req);
+
+	int ret;
+	START_KMSG_WORK(test_dsmrr_request_t, req, work);
+	test_page_response_t *res;
+#if REMOTE_HANDLE_WRITE_TIME
+	ktime_t t2e, t2s;
+	ktime_t t3e, t3s;
+	ktime_t t4e, t4s;
+	ktime_t t5e, t5s;
+	static long long t2 = 0, t3 = 0, t4 = 0, t5 = 0;
+#endif
+
+//	printk("->-> recv 4 [%d]\n", req->id);
+
+#if REMOTE_HANDLE_WRITE_TIME
+	// tr2
+	t2s = ktime_get();
+#endif
+	//res = kmalloc(sizeof(*res), GFP_KERNEL);
+	res = pcn_kmsg_get(sizeof(*res));
+	res->done = req->done;
+	res->id = req->id;
+#if REMOTE_HANDLE_WRITE_TIME
+	//*(unsigned long *)_buffer = 0xbaffdeafbeefface; // touch. need?
+	t2e = ktime_get();
+	// tr2
+#endif
+
+#if REMOTE_HANDLE_WRITE_TIME
+	//tr3: directly any kernel_vaddr
+	t3s = ktime_get();
+#endif
+	ret = pcn_kmsg_rdma_write(PCN_KMSG_FROM_NID(req),
+			req->rdma_addr, _buffer[req->id], req->size, req->rdma_key);
+#if REMOTE_HANDLE_WRITE_TIME
+	t3e = ktime_get();
+	//tr3
+#endif
+
+#if REMOTE_HANDLE_WRITE_TIME
+	//tr4
+	t4s = ktime_get();
+#endif
+	pcn_kmsg_post(PCN_KMSG_TYPE_TEST_RDMA_DSMRR_RESPONSE,
+	//pcn_kmsg_send(PCN_KMSG_TYPE_TEST_RDMA_DSMRR_RESPONSE,
+					PCN_KMSG_FROM_NID(req), res, sizeof(*res));
+//	printk("<-<- response 4 [%d]\n", req->id);
+#if REMOTE_HANDLE_WRITE_TIME
+	t4e = ktime_get();
+	// had put *res
+	//tr4
+#endif
+
+#if REMOTE_HANDLE_WRITE_TIME
+	//tr5
+	//free_page((unsigned long)_buffer);
+	t5s = ktime_get();
+#endif
+	//pcn_kmsg_put(res);
+	END_KMSG_WORK(req);
+#if REMOTE_HANDLE_WRITE_TIME
+	t5e = ktime_get();
+	//tr5
+#endif
+
+#if REMOTE_HANDLE_WRITE_TIME
+	t2 += ktime_to_ns(ktime_sub(t2e, t2s));
+	t3 += ktime_to_ns(ktime_sub(t3e, t3s));
+	t4 += ktime_to_ns(ktime_sub(t4e, t4s));
+	t5 += ktime_to_ns(ktime_sub(t5e, t5s));
+
+	cnt++;
+	if (cnt <= 1) {
+		t2 = 0, t3 = 0, t4 = 0, t5 = 0;
+	}
+
+	if (cnt >= ITER) {
+        printk("%s(): t2 %lld ns %lld us!!!\n",
+                        __func__,
+                        t2 / ONE_M,
+                        t2 / ONE_M / 1000);
+        printk("%s(): t3 %lld ns %lld us!!!\n",
+                        __func__,
+                        t3 / ONE_M,
+                        t3 / ONE_M / 1000);
+        printk("%s(): t4 %lld ns %lld us!!!\n",
+                        __func__,
+                        t4 / ONE_M,
+                        t4 / ONE_M / 1000);
+        printk("%s(): t5 %lld ns %lld us!!!\n",
+                        __func__,
+                        t5 / ONE_M,
+                        t5 / ONE_M / 1000);
+	}
+#endif
+}
+
+static int handle_test_dsmrr_response(struct pcn_kmsg_message *msg)
+{
+	//t5
+	test_page_response_t *res = (test_page_response_t *)msg;
+//	printk("-> recv response 4 [%d] \n", res->id);
+	if (res->done) {
+//		printk("-> recv response 4 comp* [%d]\n", res->id);
+		complete((struct completion *)res->done);
+	}
+	//t5
+	pcn_kmsg_done(res);
+	return 0;
+}
+
 static void process_test_rdma_request(struct work_struct *work)
 {
 	START_KMSG_WORK(test_rdma_request_t, req, work);
@@ -488,22 +853,28 @@ static struct test_desc tests[] = {
 	[TEST_ACTION_POST]			= { test_post, "synchronous post" },
 	[TEST_ACTION_RDMA_WRITE]	= { test_rdma_write, "RDMA write" },
 	[TEST_ACTION_RDMA_READ]		= { test_rdma_read, "RDMA read" },
+	[TEST_ACTION_DSM_RR]		= { test_rdma_dsm_rr, "RDMA RR" }, /* 4 */
+	[TEST_ACTION_CLEAR]			= { test_clear_all, "CLEAR ALL" }, /* 9 */
 };
 
 static void __run_test(enum test_action action, struct test_params *param)
 {
-	struct test_params thread_params[MAX_THREADS] = {};
-	struct task_struct *tsks[MAX_THREADS] = { NULL };
+	/* Stack frame over 4k */
+	struct test_params *thread_params; /* test_params thread_params[MAX_THREADS] */
+	struct task_struct **tsks; /* task_struct *tsks[MAX_THREADS] */
 	struct test_barrier barrier;
 	ktime_t t_start, t_end;
 	DECLARE_COMPLETION_ONSTACK(done);
 	unsigned long elapsed;
 	int i;
 
-	printk("Starting testing %s with %lu payload, %u thread%s, %lu iteration%s\n",
-			tests[action].description, param->payload_size,
-			param->nr_threads, param->nr_threads == 1 ? "" : "s",
-			param->nr_iterations, param->nr_iterations == 1 ? "" : "s");
+	thread_params = kzalloc(sizeof(*thread_params) * MAX_THREADS, GFP_KERNEL);
+	tsks = kzalloc(sizeof(struct task_struct*) * MAX_THREADS, GFP_KERNEL);
+	printk("%s: %d %lu %u %lu\n",
+			tests[action].description, action,
+			param->payload_size,
+			param->nr_threads,
+			param->nr_iterations);
 
 	__barrier_init(&barrier, param->nr_threads + 1);
 	param->barrier = &barrier;
@@ -523,14 +894,26 @@ static void __run_test(enum test_action action, struct test_params *param)
 	__barrier_wait(&barrier);
 	t_end = ktime_get();
 
+	kfree(thread_params);
+	kfree(tsks);
 
 	elapsed = ktime_to_ns(ktime_sub(t_end, t_start));
 
-	printk("Done testing %s\n", tests[action].description);
+	//printk("Done testing %s\n", tests[action].description);
 	printk("  %9lu ns in total\n", elapsed);
-	printk("  %3lu.%05lu ns per operation\n",
-			elapsed / param->nr_iterations,
-			(elapsed % param->nr_iterations) * 1000 /  param->nr_iterations);
+	//printk("  %3lu.%05lu ns per operation\n",
+	//		elapsed / param->nr_iterations,
+	//		(elapsed % param->nr_iterations) * 1000 /  param->nr_iterations);
+	printk("lat: %3lu.%05lu us per operation\n",
+		elapsed / param->nr_iterations / 1000,
+		((elapsed % param->nr_iterations) * 1000 * 1000) /
+									(param->nr_iterations));
+
+	printk("tps: %lu MB/s\n",
+			((param->nr_iterations * param->payload_size * param->nr_threads)
+									* (1000 * 1000) /
+											(elapsed / 1000)) // 1/ns * 1000 => 1/us (GB)
+													/ 1000 / 1000);
 }
 
 
@@ -580,8 +963,10 @@ static int __parse_cmd(const char __user *buffer, size_t count, struct test_para
 		if (nr_threads > MAX_THREADS) {
 			printk(KERN_ERR "# of threads cannot be larger than %d\n",
 					MAX_THREADS);
+			params->payload_size = 24;
 			kfree(cmd);
-			return -EINVAL;
+			//return -EINVAL;
+			return 0;
 		}
 		params->nr_threads = nr_threads;
 	}
@@ -596,7 +981,6 @@ static ssize_t start_test(struct file *file, const char __user *buffer, size_t c
 {
 	int ret;
 	int action;
-
 	struct test_params params = {
 		.payload_size = DEFAULT_PAYLOAD_SIZE_KB << 10,
 		.nr_threads = 1,
@@ -611,6 +995,11 @@ static ssize_t start_test(struct file *file, const char __user *buffer, size_t c
 	if (!try_module_get(THIS_MODULE))
 		return -EPERM;
 
+	if (params.nr_threads > MAX_THREADS) {
+		printk("action %d thread exceed %d threads.\n", action, MAX_THREADS);
+		return 0; /* For simplifying script */
+	}
+
 	/* do the coresponding work */
 	switch(action) {
 	case TEST_ACTION_SEND:
@@ -619,12 +1008,20 @@ static ssize_t start_test(struct file *file, const char __user *buffer, size_t c
 		break;
 	case TEST_ACTION_RDMA_WRITE:
 	case TEST_ACTION_RDMA_READ:
+	case TEST_ACTION_DSM_RR:
 		if (pcn_kmsg_has_features(PCN_KMSG_FEATURE_RDMA)) {
-			__run_test(action, &params);
+			if (params.payload_size == PAGE_SIZE)
+				__run_test(action, &params);
+			else
+				printk("action %d only support page size %lu.\n",
+												action, PAGE_SIZE);
 		} else {
 			printk(KERN_ERR "Transport does not support RDMA.\n");
 		}
 		break;
+	case TEST_ACTION_CLEAR:
+		test_clear_all(NULL);
+		break;
 	default:
 		printk("Unknown test no #%d\n", action);
 	}
@@ -641,7 +1038,9 @@ static void __show_usage(void)
 	printk(" Default: %d KB payload, iterate %d time%s, single thread\n",
 			DEFAULT_PAYLOAD_SIZE_KB,
 			DEFAULT_NR_ITERATIONS, DEFAULT_NR_ITERATIONS == 1 ? "" : "s");
-
+	printk("echo 0 4096 16 50000 > /proc/msg_test\n");
+	printk("echo 0 24 1 50000 > /proc/msg_test\n");
+	printk("echo 0 65536 16 50000 > /proc/msg_test\n");
 	printk(" Tests:\n");
 	for (i = 0; i < TEST_ACTION_MAX; i++) {
 		if (!tests[i].test_fn) continue;
@@ -671,13 +1070,20 @@ static struct file_operations kmsg_test_ops = {
 
 DEFINE_KMSG_WQ_HANDLER(test_send_request);
 DEFINE_KMSG_WQ_HANDLER(test_rdma_request);
+DEFINE_KMSG_WQ_HANDLER(test_dsmrr_request);
 
 static struct proc_dir_entry *kmsg_test_proc = NULL;
 
 static int __init msg_test_init(void)
 {
+	int i;
 	printk("\nLoading Popcorn messaging layer tester...\n");
 
+	for (i = 0; i < MAX_POPCORN_THREADS; i++) {
+		_buffer[i] = (void *)__get_free_page(GFP_KERNEL); // move to global
+		BUG_ON(!_buffer[i]);
+	}
+
 #ifdef CONFIG_POPCORN_STAT
 	printk(KERN_WARNING " * You are collecting statistics "
 			"and may get inaccurate performance data now *\n");
@@ -693,15 +1099,23 @@ static int __init msg_test_init(void)
 	REGISTER_KMSG_WQ_HANDLER(PCN_KMSG_TYPE_TEST_REQUEST, test_send_request);
 	REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_TEST_RESPONSE, test_send_response);
 	REGISTER_KMSG_WQ_HANDLER(PCN_KMSG_TYPE_TEST_RDMA_REQUEST, test_rdma_request);
+	REGISTER_KMSG_WQ_HANDLER(PCN_KMSG_TYPE_TEST_RDMA_DSMRR_REQUEST,
+												test_dsmrr_request);
+	REGISTER_KMSG_HANDLER(PCN_KMSG_TYPE_TEST_RDMA_DSMRR_RESPONSE,
+												test_dsmrr_response);
 
 	__show_usage();
 	return 0;
 }
 
-static void __exit msg_test_exit(void) 
+static void __exit msg_test_exit(void)
 {
+	int i;
 	if (kmsg_test_proc) proc_remove(kmsg_test_proc);
 
+	for (i = 0;i < MAX_POPCORN_THREADS; i++)
+		free_page((unsigned long)_buffer[i]);
+
 	printk("Unloaded Popcorn messaging layer tester. Good bye!\n");
 }
 
diff --git a/msg_layer/rdma.c b/msg_layer/rdma.c
index 677800b00668..6529f66abed5 100644
--- a/msg_layer/rdma.c
+++ b/msg_layer/rdma.c
@@ -1,6 +1,7 @@
 #include <linux/module.h>
 #include <linux/bitmap.h>
 #include <linux/seq_file.h>
+#include <linux/delay.h>
 
 #include <rdma/rdma_cm.h>
 #include <popcorn/stat.h>
@@ -11,9 +12,30 @@
 #define RDMA_PORT 11453
 #define RDMA_ADDR_RESOLVE_TIMEOUT_MS 5000
 
+#define DEVELOP_DBG 1
+#if DEVELOP_DBG
+#define DEVPRINTK(...) printk(KERN_INFO __VA_ARGS__)
+#else
+#define DEVPRINTK(...)
+#endif
+
+#define SENDRECV_DBG 0
+#if SENDRECV_DBG
+#define SRPRINTK(...) printk(KERN_INFO __VA_ARGS__)
+#else
+#define SRPRINTK(...)
+#endif
+
+
+#if MULTI_MSG_CANNEL_PER_NODE
+//static atomic_t send_rond_robin[MAX_NUM_NODES] = { ATOMIC_INIT(0) };
+//static atomic_t write_rond_robin[MAX_NUM_NODES] = { ATOMIC_INIT(0) };
+#endif
+
+/* this is related to rb size */
 #define MAX_RECV_DEPTH	((PAGE_SIZE << (MAX_ORDER - 1)) / PCN_KMSG_MAX_SIZE)
 #define MAX_SEND_DEPTH	(MAX_RECV_DEPTH)
-#define RDMA_SLOT_SIZE	(PAGE_SIZE * 2)
+#define RDMA_SLOT_SIZE	(PAGE_SIZE * 2) // Max per RDMA region size
 #define NR_RDMA_SLOTS	((PAGE_SIZE << (MAX_ORDER - 1)) / RDMA_SLOT_SIZE)
 
 static unsigned int use_rb_thr = PAGE_SIZE / 2;
@@ -59,6 +81,7 @@ struct rdma_work {
 
 struct rdma_handle {
 	int nid;
+	unsigned int channel;
 	enum {
 		RDMA_INIT,
 		RDMA_ADDR_RESOLVED,
@@ -70,9 +93,10 @@ struct rdma_handle {
 	} state;
 	struct completion cm_done;
 
-	struct recv_work *recv_works;
-	void *recv_buffer;
-	dma_addr_t recv_buffer_dma_addr;
+	/* Support multi continuous phy addr mem regions */
+	struct recv_work *recv_works[MSG_POOL_SIZE];
+	void *recv_buffer[MSG_POOL_SIZE];
+	dma_addr_t recv_buffer_dma_addr[MSG_POOL_SIZE];
 
 	struct rdma_cm_id *cm_id;
 	struct ib_device *device;
@@ -81,11 +105,13 @@ struct rdma_handle {
 };
 
 /* RDMA handle for each node */
-static struct rdma_handle *rdma_handles[MAX_NUM_NODES] = { NULL };
+static struct rdma_handle *rdma_handles[MAX_NUM_NODES][MAX_CONN_PER_NODE];
+//= { NULL }; // warnning
+/* Jack TODO: multi conn to a node - rdma_handles[MAX_NUM_NODES][MAX_CONN]*/
 
 /* Global protection domain (pd) and memory region (mr) */
-static struct ib_pd *rdma_pd = NULL;
-static struct ib_mr *rdma_mr = NULL;
+static struct ib_pd *rdma_pd = NULL; // Jack
+static struct ib_mr *rdma_mr = NULL; // Jack
 
 /* Global RDMA sink */
 static DEFINE_SPINLOCK(__rdma_slots_lock);
@@ -145,8 +171,18 @@ static struct send_work *__get_send_work_map(struct pcn_kmsg_message *msg, size_
 	struct send_work *sw;
 	void *map_start = NULL;
 
+retry:
+	/* get a send work */
 	spin_lock_irqsave(&send_work_pool_lock, flags);
-	BUG_ON(!send_work_pool);
+	if (!send_work_pool) {
+		dump_stack();
+		BUG(); // 64k will triger this bug (req addr more -> get_work more)
+		spin_unlock_irqrestore(&send_work_pool_lock, flags);
+		printk(KERN_WARNING "send_work pool full type %d\n", msg->header.type);
+		//io_schedule();
+		udelay(100); /* rr msg usually takes only xx us */
+		goto retry;
+	}
 	sw = send_work_pool;
 	send_work_pool = sw->next;
 	spin_unlock_irqrestore(&send_work_pool_lock, flags);
@@ -154,6 +190,7 @@ static struct send_work *__get_send_work_map(struct pcn_kmsg_message *msg, size_
 	sw->done = NULL;
 	sw->flags = 0;
 
+	/* Get a buf */
 	if (!msg) {
 		struct rb_alloc_header *ah;
 		sw->addr = ring_buffer_get_mapped(&send_buffer,
@@ -326,17 +363,91 @@ void rdma_kmsg_put(struct pcn_kmsg_message *msg)
 	__put_send_work(sw);
 }
 
+#ifdef CONFIG_POPCORN_STAT_MSG
+atomic64_t recv_cq_cnt = ATOMIC64_INIT(0);
+atomic64_t rdma_write_cnt = ATOMIC64_INIT(0);
+
+atomic64_t t_cq_sig_handle = ATOMIC64_INIT(0);
+atomic64_t t_cq_handle_end = ATOMIC64_INIT(0);
+// t_cq_sig_handle = signal to __process_recv // poll_cq
+// t_cq_handle_end = after __process to ib_req_notify_cq end
+
+atomic64_t t_rdma_w_prepare = ATOMIC64_INIT(0);
+atomic64_t t_rdma_w_post = ATOMIC64_INIT(0);
+atomic64_t t_rdma_w_wait = ATOMIC64_INIT(0);
+atomic64_t t_rdma_w_clean = ATOMIC64_INIT(0);
+#define MICROSECOND 1000000
+#define NANOSECOND 1000000000
+#endif
 void rdma_kmsg_stat(struct seq_file *seq, void *v)
 {
 	if (seq) {
 		seq_printf(seq, POPCORN_STAT_FMT,
 				(unsigned long long)ring_buffer_usage(&send_buffer),
-#ifdef CONFIG_POPCORN_STAT
+#ifdef CONFIG_POPCORN_STAT_MSG
 				(unsigned long long)send_buffer.peak_usage,
 #else
 				0ULL,
 #endif
 				"Send buffer usage");
+
+#ifdef CONFIG_POPCORN_STAT_MSG
+		/* rdma_write */
+		seq_printf(seq, "%4s  %7ld.%09ld (s)  %3s %-10ld   %3s %-6ld (ns)\n",
+            "wp", (atomic64_read(&t_rdma_w_prepare)) / NANOSECOND,
+                    (atomic64_read(&t_rdma_w_prepare)) % NANOSECOND,
+            "cnt", atomic64_read(&rdma_write_cnt),
+            "per", atomic64_read(&rdma_write_cnt) ?
+		 atomic64_read(&t_rdma_w_prepare)/atomic64_read(&rdma_write_cnt) : 0);
+		seq_printf(seq, "%4s  %7ld.%09ld (s)  %3s %-10ld   %3s %-6ld (ns)\n",
+            "wpo", (atomic64_read(&t_rdma_w_post)) / NANOSECOND,
+                    (atomic64_read(&t_rdma_w_post)) % NANOSECOND,
+            "cnt", atomic64_read(&rdma_write_cnt),
+            "per", atomic64_read(&rdma_write_cnt) ?
+		 atomic64_read(&t_rdma_w_post)/atomic64_read(&rdma_write_cnt) : 0);
+		seq_printf(seq, "%4s  %7ld.%09ld (s)  %3s %-10ld   %3s %-6ld (us)\n",
+            "wwai", (atomic64_read(&t_rdma_w_wait)) / NANOSECOND,
+                    (atomic64_read(&t_rdma_w_wait)) % NANOSECOND,
+//            "wwai", (atomic64_read(&t_rdma_w_wait) / 1000) / MICROSECOND,
+//                    (atomic64_read(&t_rdma_w_wait) / 1000) % MICROSECOND,
+            "cnt", atomic64_read(&rdma_write_cnt),
+            "per", atomic64_read(&rdma_write_cnt) ?
+	 atomic64_read(&t_rdma_w_wait)/atomic64_read(&rdma_write_cnt) / 1000 : 0);
+		seq_printf(seq, "%4s  %7ld.%09ld (s)  %3s %-10ld   %3s %-6ld (ns)\n",
+            "wcln", (atomic64_read(&t_rdma_w_clean)) / NANOSECOND,
+                    (atomic64_read(&t_rdma_w_clean)) % NANOSECOND,
+            "cnt", atomic64_read(&rdma_write_cnt),
+            "per", atomic64_read(&rdma_write_cnt) ?
+		 atomic64_read(&t_rdma_w_clean)/atomic64_read(&rdma_write_cnt) : 0);
+
+		/* cq */
+		seq_printf(seq, "%4s  %7ld.%09ld (s)  %3s %-10ld   %3s %-6ld (ns)\n",
+            "cqsi", (atomic64_read(&t_cq_sig_handle)) / NANOSECOND,
+                    (atomic64_read(&t_cq_sig_handle)) % NANOSECOND,
+            "cnt", atomic64_read(&recv_cq_cnt),
+            "per", atomic64_read(&recv_cq_cnt) ?
+		atomic64_read(&t_cq_sig_handle)/atomic64_read(&recv_cq_cnt) : 0);
+		seq_printf(seq, "%4s  %7ld.%09ld (s)  %3s %-10ld   %3s %-6ld (ns)\n",
+            "cqed", (atomic64_read(&t_cq_handle_end)) / NANOSECOND,
+                    (atomic64_read(&t_cq_handle_end)) % NANOSECOND,
+            "cnt", atomic64_read(&recv_cq_cnt),
+            "per", atomic64_read(&recv_cq_cnt) ?
+		 atomic64_read(&t_cq_handle_end)/atomic64_read(&recv_cq_cnt) : 0);
+#endif
+	} else {
+#ifdef CONFIG_POPCORN_STAT_MSG
+		atomic64_set(&recv_cq_cnt, 0);
+		atomic64_set(&rdma_write_cnt, 0);
+
+		atomic64_set(&t_cq_sig_handle, 0);
+		atomic64_set(&t_cq_handle_end, 0);
+
+		atomic64_set(&t_rdma_w_prepare, 0);
+		atomic64_set(&t_rdma_w_post, 0);
+		atomic64_set(&t_rdma_w_wait, 0);
+		atomic64_set(&t_rdma_w_clean, 0);
+
+#endif
 	}
 }
 
@@ -346,9 +457,41 @@ void rdma_kmsg_stat(struct seq_file *seq, void *v)
  */
 static int __send_to(int to_nid, struct send_work *sw, size_t size)
 {
-	struct rdma_handle *rh = rdma_handles[to_nid];
-	struct ib_send_wr *bad_wr = NULL;
 	int ret;
+	struct ib_send_wr *bad_wr = NULL;
+	int channel;
+	struct rdma_handle *rh;
+/*
+	if (MAX_CONN_PER_NODE > 1) {
+		//int atomic_val = atomic_inc_return(&send_rond_robin[to_nid]);
+		//channel = atomic_val % (MAX_CONN_PER_NODE);
+		//DEVPRINTK("-> send dbg: atomic_val %d mod this %d channel %d\n", atomic_val, (MAX_CONN_PER_NODE), channel);
+		channel = atomic_inc_return(&send_rond_robin[to_nid]) %
+													MAX_CONN_PER_NODE;
+	} else {
+		channel = 0;
+	}
+*/
+#if MULTI_MSG_CANNEL_PER_NODE
+	// too much overhead
+	//channel = atomic_inc_return(&send_rond_robin[to_nid]) %
+	//											MAX_CONN_PER_NODE;
+	channel = current->pid % MAX_CONN_PER_NODE;
+#else
+	channel = 0;
+#endif
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+    BUG_ON(channel < 0);
+#endif
+
+
+	rh = rdma_handles[to_nid][channel];
+#if MULTI_CONN_PER_NODE
+	((struct pcn_kmsg_message *)sw->addr)->header.channel = channel;
+#endif
+	//msg->header.from_nid = my_nid;
+	SRPRINTK("-> send: to_nid %d channel %u\n", to_nid, channel);
+
 
 #ifdef CONFIG_POPCORN_CHECK_SANITY
 	BUG_ON(size > sw->sgl.length);
@@ -432,7 +575,7 @@ struct pcn_kmsg_rdma_handle *rdma_kmsg_pin_rdma_buffer(void *msg, size_t size)
 		return ERR_PTR(-EINVAL);
 	}
 #endif
-	rh->rkey = rdma_mr->rkey;
+	rh->rkey = rdma_mr->rkey; // Jack
 	rh->private = (void *)
 		(unsigned long)__get_rdma_buffer(&rh->addr, &rh->dma_addr);
 
@@ -448,35 +591,91 @@ void rdma_kmsg_unpin_rdma_buffer(struct pcn_kmsg_rdma_handle *handle)
 int rdma_kmsg_write(int to_nid, dma_addr_t rdma_addr, void *addr, size_t size, u32 rdma_key)
 {
 	DECLARE_COMPLETION_ONSTACK(done);
-	struct rdma_work *rw;
 	struct ib_send_wr *bad_wr = NULL;
-
+	struct rdma_work *rw;
 	dma_addr_t dma_addr;
 	int ret;
+	int channel;
+#ifdef CONFIG_POPCORN_STAT_MSG
+	ktime_t t5e, t5s;
+	ktime_t t4e, t4s;
+	ktime_t t3e, t3s;
+	ktime_t t2e, t2s = ktime_get();
+#endif
+
+/*
+	if (MAX_CONN_PER_NODE > 1) {
+		//int atomic_val = atomic_inc_return(&write_rond_robin[to_nid]);
+		//channel = atomic_val % (MAX_CONN_PER_NODE);
+		//DEVPRINTK("-> write dbg: atomic_val %d mod this %d channel %d\n", atomic_val, (MAX_CONN_PER_NODE), channel);
+		channel = atomic_inc_return(&write_rond_robin[to_nid]) %
+													MAX_CONN_PER_NODE;
+	} else {
+		channel = 0;
+	}
+*/
+#if MULTI_MSG_CANNEL_PER_NODE
+	//channel = atomic_inc_return(&write_rond_robin[to_nid]) %
+	//											MAX_CONN_PER_NODE;
+	channel = current->pid % MAX_CONN_PER_NODE;
+#else
+	channel = 0;
+#endif
+
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+    BUG_ON(channel < 0);
+#endif
+
 
 	dma_addr = ib_dma_map_single(rdma_mr->device, addr, size, DMA_TO_DEVICE);
 	ret = ib_dma_mapping_error(rdma_mr->device, dma_addr);
 	BUG_ON(ret);
 
+	/* rdma_key done by "rdma_kmsg_pin_rdma_buffer()" */
 	rw = __get_rdma_work(dma_addr, size, rdma_addr, rdma_key);
 	BUG_ON(!rw);
 
 	rw->done = &done;
+#ifdef CONFIG_POPCORN_STAT_MSG
+	t2e = ktime_get();
+	atomic64_add(ktime_to_ns(ktime_sub(t2e, t2s)), &t_rdma_w_prepare);
 
-	ret = ib_post_send(rdma_handles[to_nid]->qp, &rw->wr.wr, &bad_wr);
+	t3s = ktime_get();
+#endif
+	ret = ib_post_send(rdma_handles[to_nid][channel]->qp, &rw->wr.wr, &bad_wr);
 	if (ret || bad_wr) {
 		printk("Cannot post rdma write, %d, %p\n", ret, bad_wr);
 		if (ret == 0) ret = -EINVAL;
 		goto out;
 	}
+#ifdef CONFIG_POPCORN_STAT_MSG
+	t3e = ktime_get();
+	atomic64_add(ktime_to_ns(ktime_sub(t3e, t3s)), &t_rdma_w_post);
+
+	t4s = ktime_get();
+#endif
 	/* XXX polling??? */
 	if (!try_wait_for_completion(&done)) {
 		wait_for_completion(&done);
 	}
+#ifdef CONFIG_POPCORN_STAT_MSG
+	t4e = ktime_get();
+	atomic64_add(ktime_to_ns(ktime_sub(t4e, t4s)), &t_rdma_w_wait);
+
+	atomic64_inc(&rdma_write_cnt);
+#endif
 
 out:
+#ifdef CONFIG_POPCORN_STAT_MSG
+	t5s = ktime_get();
+#endif
 	ib_dma_unmap_single(rdma_mr->device, dma_addr, size, DMA_TO_DEVICE);
 	__put_rdma_work(rw);
+#ifdef CONFIG_POPCORN_STAT_MSG
+	t5e = ktime_get();
+	atomic64_add(ktime_to_ns(ktime_sub(t5e, t5s)), &t_rdma_w_clean);
+#endif
+
 	return ret;
 }
 
@@ -492,17 +691,49 @@ int rdma_kmsg_read(int from_nid, void *addr, dma_addr_t rdma_addr, size_t size,
 void rdma_kmsg_done(struct pcn_kmsg_message *msg)
 {
 	/* Put back the receive work */
-	int ret;
+	int ret, i, index;
 	struct ib_recv_wr *bad_wr = NULL;
 	int from_nid = PCN_KMSG_FROM_NID(msg);
-	struct rdma_handle *rh = rdma_handles[from_nid];
-	int index = ((void *)msg - rh->recv_buffer) / PCN_KMSG_MAX_SIZE;
+#if MULTI_CONN_PER_NODE
+	unsigned int channel = msg->header.channel;
+#else
+	unsigned int channel = 0;
+#endif
+	struct rdma_handle *rh = rdma_handles[from_nid][channel]; // iter
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+	bool found = false;
+#endif
+
+	SRPRINTK("\t<- recv: kmsg_done: from_nid %d channel %u\n",
+												from_nid, channel);
+
+	/* Look for the right pool */
+	for (i = 0; i < MSG_POOL_SIZE ;i++) {
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		if (!rh->recv_buffer[i]) continue;
+#endif
+		if ((void *)msg >= rh->recv_buffer[i] &&
+			(void *)msg < rh->recv_buffer[i] + (PAGE_SIZE << (MAX_ORDER - 1))) {
+			index = ((void *)msg - rh->recv_buffer[i]) / PCN_KMSG_MAX_SIZE;
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+			found = true;
+#endif
+			break;
+		} // else { /* try next */ }
+	}
 
 #ifdef CONFIG_POPCORN_CHECK_SANITY
+	if (index < 0 || index >= MAX_RECV_DEPTH || !found) {
+		printk(KERN_WARNING "i %d idx %d\n", i, index);
+		PCNPRINTK_ERR("\t<- (warnning)recv: kmsg_done: "
+								"from_nid %d channel %u\n",
+										from_nid, channel);
+	}
+	BUG_ON(!found);
 	BUG_ON(index < 0 || index >= MAX_RECV_DEPTH);
 #endif
 
-	ret = ib_post_recv(rh->qp, &rh->recv_works[index].wr, &bad_wr);
+	ret = ib_post_recv(rh->qp, &rh->recv_works[i][index].wr, &bad_wr);
 	BUG_ON(ret || bad_wr);
 }
 
@@ -577,8 +808,15 @@ void cq_comp_handler(struct ib_cq *cq, void *context)
 {
 	int ret;
 	struct ib_wc wc;
+#ifdef CONFIG_POPCORN_STAT_MSG
+	ktime_t t3e, t3s;
+	ktime_t t2e, t2s;
+#endif
 
 retry:
+#ifdef CONFIG_POPCORN_STAT_MSG
+	t2s = ktime_get();
+#endif
 	while ((ret = ib_poll_cq(cq, 1, &wc)) > 0) {
 		if (wc.opcode < 0 || wc.status) {
 			__process_faulty_work(&wc);
@@ -589,7 +827,20 @@ retry:
 			__process_sent(&wc);
 			break;
 		case IB_WC_RECV:
+#ifdef CONFIG_POPCORN_STAT_MSG
+			t2e = ktime_get();
+			atomic64_add(ktime_to_ns(ktime_sub(t2e, t2s)), &t_cq_sig_handle);
+#endif
+
+#if MULTI_CONN_PER_NODE
+			((struct pcn_kmsg_message *)(((struct recv_work *)(wc.wr_id))->addr))->header.channel = *((unsigned int *)cq->cq_context);
+#endif
+//			printk("<- recv: cq->cq_context = %d (%p)\n",
+//					*((int *)cq->cq_context), cq->cq_context);
 			__process_recv(&wc);
+#ifdef CONFIG_POPCORN_STAT_MSG
+			t3s = ktime_get();
+#endif
 			break;
 		case IB_WC_RDMA_WRITE:
 		case IB_WC_RDMA_READ:
@@ -602,8 +853,18 @@ retry:
 			printk("Unknown completion op %d\n", wc.opcode);
 			break;
 		}
+#ifdef CONFIG_POPCORN_STAT_MSG
+		t2s = ktime_get();
+#endif
 	}
 	ret = ib_req_notify_cq(cq, IB_CQ_NEXT_COMP | IB_CQ_REPORT_MISSED_EVENTS);
+#ifdef CONFIG_POPCORN_STAT_MSG
+	if (wc.opcode == IB_WC_RECV) {
+		t3e = ktime_get();
+		atomic64_add(ktime_to_ns(ktime_sub(t3e, t3s)), &t_cq_handle_end);
+		atomic64_inc(&recv_cq_cnt);
+	}
+#endif
 	if (ret > 0) goto retry;
 }
 
@@ -625,24 +886,32 @@ static __init int __setup_pd_cq_qp(struct rdma_handle *rh)
 			rdma_pd = NULL;
 			goto out_err;
 		}
+		DEVPRINTK("ib_alloc_pd pass\n");
 	}
 
 	/* create completion queue */
 	if (!rh->cq) {
 		struct ib_cq_init_attr cq_attr = {
-			.cqe = MAX_SEND_DEPTH + MAX_RECV_DEPTH + NR_RDMA_SLOTS,
+			.cqe = (MAX_SEND_DEPTH) +
+					(MAX_RECV_DEPTH * MSG_POOL_SIZE) + NR_RDMA_SLOTS,
 			.comp_vector = 0,
 		};
 
-		rh->cq = ib_create_cq(
-				rh->device, cq_comp_handler, NULL, rh, &cq_attr);
+		DEVPRINTK("createing cq rh->channel %d\n", rh->channel);
+		rh->cq = ib_create_cq(rh->device,
+			cq_comp_handler, (void *)&(rh->channel), rh, &cq_attr);
 		if (IS_ERR(rh->cq)) {
 			ret = PTR_ERR(rh->cq);
 			goto out_err;
 		}
+		DEVPRINTK("ib_create_cq pass\n");
+
+		/* Manually copy channel_id to rh->cq for handler usage */
+		rh->cq->cq_context = &(rh->channel);
 
 		ret = ib_req_notify_cq(rh->cq, IB_CQ_NEXT_COMP);
 		if (ret < 0) goto out_err;
+		DEVPRINTK("rdma_create_cq pass\n");
 	}
 
 	/* create queue pair */
@@ -651,10 +920,10 @@ static __init int __setup_pd_cq_qp(struct rdma_handle *rh)
 			.event_handler = NULL, // qp_event_handler,
 			.qp_context = rh,
 			.cap = {
-				.max_send_wr = MAX_SEND_DEPTH,
-				.max_recv_wr = MAX_RECV_DEPTH + NR_RDMA_SLOTS,
-				.max_send_sge = PCN_KMSG_MAX_SIZE >> PAGE_SHIFT,
-				.max_recv_sge = PCN_KMSG_MAX_SIZE >> PAGE_SHIFT,
+				.max_send_wr = (MAX_SEND_DEPTH * MSG_POOL_SIZE),
+				.max_recv_wr = (MAX_RECV_DEPTH * MSG_POOL_SIZE) + NR_RDMA_SLOTS,
+				.max_send_sge = (PCN_KMSG_MAX_SIZE >> PAGE_SHIFT), // per msg
+				.max_recv_sge = (PCN_KMSG_MAX_SIZE >> PAGE_SHIFT), // per msg
 			},
 			.sq_sig_type = IB_SIGNAL_REQ_WR,
 			.qp_type = IB_QPT_RC,
@@ -662,10 +931,31 @@ static __init int __setup_pd_cq_qp(struct rdma_handle *rh)
 			.recv_cq = rh->cq,
 		};
 
+#ifdef CONFIG_POPCORN_CHECK_SANITY
+		struct ib_device_attr dev_cap;
+		int rc;
+		rc = ib_query_device(rh->device, &dev_cap);
+		BUG_ON(rc);
+		printk("dev_cap.max_qp_wr %d dev_cap.max_sge %d\n",
+						dev_cap.max_qp_wr, dev_cap.max_sge);
+		printk("msg_pools %d DEPTH s %lu r %lu rdma slot %lu max_kmsg_size %lu\n",
+								MSG_POOL_SIZE, MAX_SEND_DEPTH,
+								MAX_RECV_DEPTH, NR_RDMA_SLOTS, PCN_KMSG_MAX_SIZE);
+		printk("s %lu r %lu ssg %lu rsg %lu\n", MAX_SEND_DEPTH * MSG_POOL_SIZE,
+						(MAX_RECV_DEPTH * MSG_POOL_SIZE) + NR_RDMA_SLOTS,
+						(PCN_KMSG_MAX_SIZE >> PAGE_SHIFT),
+						(PCN_KMSG_MAX_SIZE >> PAGE_SHIFT));
+		BUG_ON(qp_attr.cap.max_send_wr > dev_cap.max_qp_wr ||
+				qp_attr.cap.max_recv_wr  > dev_cap.max_qp_wr ||
+				qp_attr.cap.max_send_sge > dev_cap.max_sge ||
+				qp_attr.cap.max_recv_sge > dev_cap.max_sge);
+#endif
+
 		ret = rdma_create_qp(rh->cm_id, rdma_pd, &qp_attr);
 		if (ret) goto out_err;
 		rh->qp = rh->cm_id->qp;
 	}
+	DEVPRINTK("rdma_create_qp pass\n");
 	return 0;
 
 out_err:
@@ -674,61 +964,69 @@ out_err:
 
 static __init int __setup_buffers_and_pools(struct rdma_handle *rh)
 {
-	int ret = 0, i;
+	int ret = 0, i, j;
 	dma_addr_t dma_addr;
 	char *recv_buffer = NULL;
 	struct recv_work *rws = NULL;
 	const size_t buffer_size = PCN_KMSG_MAX_SIZE * MAX_RECV_DEPTH;
 
-	/* Initalize receive buffers */
-	recv_buffer = kmalloc(buffer_size, GFP_KERNEL);
-	if (!recv_buffer) {
-		return -ENOMEM;
-	}
-	rws = kmalloc(sizeof(*rws) * MAX_RECV_DEPTH, GFP_KERNEL);
-	if (!rws) {
-		ret = -ENOMEM;
-		goto out_free;
-	}
-
-	/* Populate receive buffer and work requests */
-	dma_addr = ib_dma_map_single(
-			rh->device, recv_buffer, buffer_size, DMA_FROM_DEVICE);
-	ret = ib_dma_mapping_error(rh->device, dma_addr);
-	if (ret) goto out_free;
-
-	for (i = 0; i < MAX_RECV_DEPTH; i++) {
-		struct recv_work *rw = rws + i;
-		struct ib_recv_wr *wr, *bad_wr = NULL;
-		struct ib_sge *sgl;
-
-		rw->header.type = WORK_TYPE_RECV;
-		rw->dma_addr = dma_addr + PCN_KMSG_MAX_SIZE * i;
-		rw->addr = recv_buffer + PCN_KMSG_MAX_SIZE * i;
-
-		sgl = &rw->sgl;
-		sgl->lkey = rdma_pd->local_dma_lkey;
-		sgl->addr = rw->dma_addr;
-		sgl->length = PCN_KMSG_MAX_SIZE;
-
-		wr = &rw->wr;
-		wr->sg_list = sgl;
-		wr->num_sge = 1;
-		wr->next = NULL;
-		wr->wr_id = (u64)rw;
+	BUG_ON(buffer_size > (PAGE_SIZE << (MAX_ORDER - 1)));
+	MSGPRINTK("recv pools = %d \n", MSG_POOL_SIZE);
+	for (j = 0; j < MSG_POOL_SIZE; j++) {
+		/* Initalize receive buffers */
+		recv_buffer = kmalloc(buffer_size, GFP_KERNEL);
+		if (!recv_buffer) {
+			return -ENOMEM;
+		}
+		rh->recv_buffer[j] = recv_buffer;
 
-		ret = ib_post_recv(rh->qp, wr, &bad_wr);
-		if (ret || bad_wr) goto out_free;
+		rws = kmalloc(sizeof(*rws) * MAX_RECV_DEPTH, GFP_KERNEL);
+		if (!rws) {
+			ret = -ENOMEM;
+			goto out_free;
+		}
+		rh->recv_works[j] = rws;
+
+		/* Populate receive buffer and work requests */
+		dma_addr = ib_dma_map_single(
+				rh->device, recv_buffer, buffer_size, DMA_FROM_DEVICE);
+		ret = ib_dma_mapping_error(rh->device, dma_addr);
+		if (ret) goto out_free;
+		rh->recv_buffer_dma_addr[j] = dma_addr;
+
+		for (i = 0; i < MAX_RECV_DEPTH; i++) {
+			struct recv_work *rw = rws + i;
+			struct ib_recv_wr *wr, *bad_wr = NULL;
+			struct ib_sge *sgl;
+
+			rw->header.type = WORK_TYPE_RECV;
+			rw->dma_addr = dma_addr + PCN_KMSG_MAX_SIZE * i;
+			rw->addr = recv_buffer + PCN_KMSG_MAX_SIZE * i;
+
+			sgl = &rw->sgl;
+			sgl->lkey = rdma_pd->local_dma_lkey;
+			sgl->addr = rw->dma_addr;
+			sgl->length = PCN_KMSG_MAX_SIZE;
+
+			wr = &rw->wr;
+			wr->sg_list = sgl;
+			wr->num_sge = 1;
+			wr->next = NULL;
+			wr->wr_id = (u64)rw;
+
+			ret = ib_post_recv(rh->qp, wr, &bad_wr);
+			if (ret || bad_wr) goto out_free;
+		}
 	}
-	rh->recv_works = rws;
-	rh->recv_buffer = recv_buffer;
-	rh->recv_buffer_dma_addr = dma_addr;
-
 	return ret;
 
 out_free:
-	if (recv_buffer) kfree(recv_buffer);
-	if (rws) kfree(rws);
+	for (j = 0; j < MSG_POOL_SIZE; j++) {
+		if (rh->recv_buffer[j])
+			kfree(rh->recv_buffer[j]);
+		if (rh->recv_works[j])
+			kfree(rh->recv_works[j]);
+	}
 	return ret;
 }
 
@@ -780,7 +1078,7 @@ static __init int __setup_rdma_buffer(const int nr_chunks)
 	 * rdma_handles[my_nid] is for accepting connection without qp & cp.
 	 * So, let's use rdma_handles[1] for nid 0 and rdma_handles[0] otherwise.
 	 */
-	ret = ib_post_send(rdma_handles[!my_nid]->qp, &reg_wr.wr, &bad_wr);
+	ret = ib_post_send(rdma_handles[!my_nid][0]->qp, &reg_wr.wr, &bad_wr);
 	if (ret || bad_wr) {
 		printk("Cannot register mr, %d %p\n", ret, bad_wr);
 		if (bad_wr) ret = -EINVAL;
@@ -793,7 +1091,7 @@ static __init int __setup_rdma_buffer(const int nr_chunks)
 		goto out_dereg;
 	}
 
-	rdma_mr = mr;
+	rdma_mr = mr; //Jack
 	//printk("lkey: %x, rkey: %x, length: %x\n", mr->lkey, mr->rkey, mr->length);
 	return 0;
 
@@ -823,9 +1121,8 @@ static int __init __setup_work_request_pools(void)
 		if (ret) goto out_unmap;
 		send_buffer.dma_addr_base[i] = dma_addr;
 	}
-
 	/* Initialize send work request pool */
-	for (i = 0; i < MAX_SEND_DEPTH; i++) {
+	for (i = 0; i < MAX_SEND_DEPTH * MSG_POOL_SIZE; i++) {
 		struct send_work *sw;
 
 		sw = kzalloc(sizeof(*sw), GFP_KERNEL);
@@ -845,11 +1142,11 @@ static int __init __setup_work_request_pools(void)
 		sw->wr.num_sge = 1;
 		sw->wr.opcode = IB_WR_SEND;
 		sw->wr.send_flags = IB_SEND_SIGNALED;
+		//sw->id = j; /* does this matter? if not just for MAX_SEND_DEPTH * MSG_POOL_SIZE here in the beginning (=>remove id) - trying now */
 
 		sw->next = send_work_pool;
 		send_work_pool = sw;
 	}
-
 	/* Initalize rdma work request pool */
 	__refill_rdma_work(NR_RDMA_SLOTS);
 	return 0;
@@ -914,11 +1211,14 @@ int cm_client_event_handler(struct rdma_cm_id *cm_id, struct rdma_cm_event *cm_e
 	return 0;
 }
 
-static int __connect_to_server(int nid)
+static int __connect_to_server(int nid, int iter)
 {
 	int ret;
 	const char *step;
-	struct rdma_handle *rh = rdma_handles[nid];
+	struct rdma_handle *rh = rdma_handles[nid][iter]; // MAX_CONN_PER_NODE
+	int my_nid_iter[2] = {my_nid, iter};
+	//my_nid_iter[0] = my_nid
+	//my_nid_iter[1] = iter;
 
 	step = "create rdma id";
 	rh->cm_id = rdma_create_id(&init_net,
@@ -929,21 +1229,21 @@ static int __connect_to_server(int nid)
 	{
 		struct sockaddr_in addr = {
 			.sin_family = AF_INET,
-			.sin_port = htons(RDMA_PORT),
+			.sin_port = htons(RDMA_PORT + iter), // MAX_CONN_PER_NODE
 			.sin_addr.s_addr = ip_table[nid],
 		};
 
 		ret = rdma_resolve_addr(rh->cm_id, NULL,
 				(struct sockaddr *)&addr, RDMA_ADDR_RESOLVE_TIMEOUT_MS);
 		if (ret) goto out_err;
-		ret = wait_for_completion_interruptible(&rh->cm_done);
+		ret = wait_for_completion_interruptible(&rh->cm_done); // per conn
 		if (ret || rh->state != RDMA_ADDR_RESOLVED) goto out_err;
 	}
 
 	step = "resolve routing path";
 	ret = rdma_resolve_route(rh->cm_id, RDMA_ADDR_RESOLVE_TIMEOUT_MS);
 	if (ret) goto out_err;
-	ret = wait_for_completion_interruptible(&rh->cm_done);
+	ret = wait_for_completion_interruptible(&rh->cm_done); // per conn
 	if (ret || rh->state != RDMA_ROUTE_RESOLVED) goto out_err;
 
 	/* cm_id->device is valid after the address and route are resolved */
@@ -960,14 +1260,16 @@ static int __connect_to_server(int nid)
 	step = "connect";
 	{
 		struct rdma_conn_param conn_param = {
-			.private_data = &my_nid,
-			.private_data_len = sizeof(my_nid),
+			//.private_data = &my_nid,
+			//.private_data_len = sizeof(my_nid),
+			.private_data = &my_nid_iter,
+			.private_data_len = sizeof(my_nid_iter),
 		};
 
 		rh->state = RDMA_CONNECTING;
-		ret = rdma_connect(rh->cm_id, &conn_param);
+		ret = rdma_connect(rh->cm_id, &conn_param); // connect to remote
 		if (ret) goto out_err;
-		ret = wait_for_completion_interruptible(&rh->cm_done);
+		ret = wait_for_completion_interruptible(&rh->cm_done); // per conn
 		if (ret) goto out_err;
 		if (rh->state != RDMA_CONNECTED) {
 			ret = -ETIMEDOUT;
@@ -975,11 +1277,11 @@ static int __connect_to_server(int nid)
 		}
 	}
 
-	MSGPRINTK("Connected to %d\n", nid);
+	MSGPRINTK("(Client) connected to %d-%d\n", nid, iter);
 	return 0;
 
 out_err:
-	PCNPRINTK_ERR("Unable to %s, %pI4, %d\n", step, ip_table + nid, ret);
+	PCNPRINTK_ERR("Unable to %s, %pI4 - %d, %d\n", step, ip_table + nid, iter, ret);
 	return ret;
 }
 
@@ -987,9 +1289,9 @@ out_err:
 /****************************************************************************
  * Server-side connection handling
  */
-static int __accept_client(int nid)
+static int __accept_client(int nid, int iter)
 {
-	struct rdma_handle *rh = rdma_handles[nid];
+	struct rdma_handle *rh = rdma_handles[nid][iter];
 	struct rdma_conn_param conn_param = {};
 	int ret;
 
@@ -1014,8 +1316,10 @@ static int __accept_client(int nid)
 }
 static int __on_client_connecting(struct rdma_cm_id *cm_id, struct rdma_cm_event *cm_event)
 {
-	int peer_nid = *(int *)cm_event->param.conn.private_data;
-	struct rdma_handle *rh = rdma_handles[peer_nid];
+	int peer_nid = *((int *)cm_event->param.conn.private_data + 0);
+	int peer_nid_iter = *((int *)cm_event->param.conn.private_data + 1);
+	//int peer_nid_iter = *(int *)cm_event->param.conn.private_data;
+	struct rdma_handle *rh = rdma_handles[peer_nid][peer_nid_iter];
 
 	cm_id->context = rh;
 	rh->cm_id = cm_id;
@@ -1032,7 +1336,7 @@ static int __on_client_connected(struct rdma_cm_id *cm_id, struct rdma_cm_event
 	rh->state = RDMA_CONNECTED;
 	complete(&rh->cm_done);
 
-	MSGPRINTK("Connected to %d\n", rh->nid);
+	MSGPRINTK("(Server) connected to %d-%u\n", rh->nid, rh->channel);
 	return 0;
 }
 
@@ -1066,27 +1370,29 @@ int cm_server_event_handler(struct rdma_cm_id *cm_id, struct rdma_cm_event *cm_e
 	return 0;
 }
 
-static int __listen_to_connection(void)
+static int __listen_to_connection(int iter)
 {
 	int ret;
 	struct sockaddr_in addr = {
 		.sin_family = AF_INET,
-		.sin_port = htons(RDMA_PORT),
+		.sin_port = htons(RDMA_PORT + iter), // MAX_CONN_PER_NODE
 		.sin_addr.s_addr = ip_table[my_nid],
 	};
 
 	struct rdma_cm_id *cm_id = rdma_create_id(&init_net,
 			cm_server_event_handler, NULL, RDMA_PS_IB, IB_QPT_RC);
 	if (IS_ERR(cm_id)) return PTR_ERR(cm_id);
-	rdma_handles[my_nid]->cm_id = cm_id;
+	rdma_handles[my_nid][iter]->cm_id = cm_id; // MAX_CONN_PER_NODE
+
+	MSGPRINTK("listening from %d - %d\n", my_nid, iter);
 
-	ret = rdma_bind_addr(cm_id, (struct sockaddr *)&addr);
+	ret = rdma_bind_addr(cm_id, (struct sockaddr *)&addr); // done by addr
 	if (ret) {
 		PCNPRINTK_ERR("Cannot bind server address, %d\n", ret);
 		return ret;
 	}
 
-	ret = rdma_listen(cm_id, MAX_NUM_NODES);
+	ret = rdma_listen(cm_id, MAX_NUM_NODES * MAX_CONN_PER_NODE);
 	if (ret) {
 		PCNPRINTK_ERR("Cannot listen to incoming requests, %d\n", ret);
 		return ret;
@@ -1098,23 +1404,27 @@ static int __listen_to_connection(void)
 
 static int __establish_connections(void)
 {
-	int i, ret;
+	int i, j, ret;
 
-	ret = __listen_to_connection();
-	if (ret) return ret;
+	for (j = 0; j < MAX_CONN_PER_NODE; j++) {
+		ret = __listen_to_connection(j);
+		if (ret) return ret;
+	}
 
 	/* Wait for a while so that nodes are ready to listen to connections */
 	msleep(100);
 
 	for (i = 0; i < my_nid; i++) {
-		if ((ret = __connect_to_server(i))) return ret;
+		for (j = 0; j < MAX_CONN_PER_NODE; j++)
+			if ((ret = __connect_to_server(i, j))) return ret;
 		set_popcorn_node_online(i, true);
 	}
 
 	set_popcorn_node_online(my_nid, true);
 
 	for (i = my_nid + 1; i < MAX_NUM_NODES; i++) {
-		if ((ret = __accept_client(i))) return ret;
+		for (j = 0; j < MAX_CONN_PER_NODE; j++)
+			if ((ret = __accept_client(i, j))) return ret;
 		set_popcorn_node_online(i, true);
 	}
 
@@ -1124,31 +1434,35 @@ static int __establish_connections(void)
 
 void __exit exit_kmsg_rdma(void)
 {
-	int i;
+	int i, j, k;
 
 	/* Detach from upper layer to prevent race condition during exit */
 	pcn_kmsg_set_transport(NULL);
 
 	for (i = 0; i < MAX_NUM_NODES; i++) {
-		struct rdma_handle *rh = rdma_handles[i];
-		set_popcorn_node_online(i, false);
-		if (!rh) continue;
-
-		if (rh->recv_buffer) {
-			ib_dma_unmap_single(rh->device, rh->recv_buffer_dma_addr,
-					PCN_KMSG_MAX_SIZE * MAX_RECV_DEPTH, DMA_FROM_DEVICE);
-			kfree(rh->recv_buffer);
-			kfree(rh->recv_works);
-		}
+		for (k = 0; k < MAX_CONN_PER_NODE; k++) {
+			struct rdma_handle *rh = rdma_handles[i][k];
+			set_popcorn_node_online(i, false);
+			if (!rh) continue;
+
+			for (j = 0; j < MSG_POOL_SIZE; j++) {
+				if (rh->recv_buffer[j]) {
+					ib_dma_unmap_single(rh->device, rh->recv_buffer_dma_addr[j],
+							PCN_KMSG_MAX_SIZE * MAX_RECV_DEPTH, DMA_FROM_DEVICE);
+					kfree(rh->recv_buffer[j]);
+					kfree(rh->recv_works[j]);
+				}
+			}
 
-		if (rh->qp && !IS_ERR(rh->qp)) rdma_destroy_qp(rh->cm_id);
-		if (rh->cq && !IS_ERR(rh->cq)) ib_destroy_cq(rh->cq);
-		if (rh->cm_id && !IS_ERR(rh->cm_id)) rdma_destroy_id(rh->cm_id);
+			if (rh->qp && !IS_ERR(rh->qp)) rdma_destroy_qp(rh->cm_id);
+			if (rh->cq && !IS_ERR(rh->cq)) ib_destroy_cq(rh->cq);
+			if (rh->cm_id && !IS_ERR(rh->cm_id)) rdma_destroy_id(rh->cm_id);
 
-		kfree(rdma_handles[i]);
+			kfree(rdma_handles[i][k]);
+		}
 	}
 
-	/* MR is set correctly iff rdma buffer and pd are correctly allocated */
+	/* MR is set correctly if rdma buffer and pd are correctly allocated */
 	if (rdma_mr && !IS_ERR(rdma_mr)) {
 		ib_dereg_mr(rdma_mr);
 		ib_dma_unmap_single(rdma_pd->device, __rdma_sink_dma_addr,
@@ -1200,21 +1514,34 @@ struct pcn_kmsg_transport transport_rdma = {
 
 int __init init_kmsg_rdma(void)
 {
-	int i;
+	int i, j;
 
 	MSGPRINTK("\nLoading Popcorn messaging layer over RDMA...\n");
 
+
+#if MAX_CONN_PER_NODE > 1
+#if !MULTI_CONN_PER_NODE
+	BUG();
+#endif
+#endif
+
 	if (!identify_myself()) return -EINVAL;
 	pcn_kmsg_set_transport(&transport_rdma);
 
 	for (i = 0; i < MAX_NUM_NODES; i++) {
-		struct rdma_handle *rh;
-		rh = rdma_handles[i] = kzalloc(sizeof(struct rdma_handle), GFP_KERNEL);
-		if (!rh) goto out_free;
-
-		rh->nid = i;
-		rh->state = RDMA_INIT;
-		init_completion(&rh->cm_done);
+		//send_rond_robin[i] = ATOMIC_INIT(0);
+		//write_rond_robin[i] = ATOMIC_INIT(0);
+		for (j = 0; j < MAX_CONN_PER_NODE; j++) {
+			struct rdma_handle *rh;
+			rh = rdma_handles[i][j] =
+					kzalloc(sizeof(struct rdma_handle), GFP_KERNEL);
+			if (!rh) goto out_free;
+
+			rh->nid = i;
+			rh->channel = j;
+			rh->state = RDMA_INIT;
+			init_completion(&rh->cm_done);
+		}
 	}
 
 	if (__establish_connections())
diff --git a/msg_layer/ring_buffer.c b/msg_layer/ring_buffer.c
index fd5ae614b344..d01a164ef697 100644
--- a/msg_layer/ring_buffer.c
+++ b/msg_layer/ring_buffer.c
@@ -8,7 +8,6 @@
 #define RB_HEADER_MAGIC 0xa9
 #endif
 #define RB_ALIGN 64
-#define RB_NR_CHUNKS 8
 
 struct ring_buffer_header {
 	bool reclaim:1;
@@ -166,6 +165,8 @@ void *ring_buffer_get_mapped(struct ring_buffer *rb, size_t size, dma_addr_t *dm
 	/* Is buffer full? */
 	if (rb->wraparounded && rb->head_chunk == rb->tail_chunk) {
 		if (rb->tail + sizeof(*header) + size > rb->head) {
+			//printk(KERN_ERR "rb full roll back to kmalloc!!!\n");
+			WARN_ON_ONCE("rb full roll back to kmalloc!!!\n");
 			spin_unlock_irqrestore(&rb->lock, flags);
 			return NULL;
 		}
diff --git a/msg_layer/ring_buffer.h b/msg_layer/ring_buffer.h
index 80d9ab8ced51..bd2ec54f63f6 100644
--- a/msg_layer/ring_buffer.h
+++ b/msg_layer/ring_buffer.h
@@ -1,7 +1,10 @@
 #ifndef __POPCORN_RING_BUFFER_H__
 #define __POPCORN_RING_BUFFER_H__
 
-#define RB_MAX_CHUNKS	16
+#define MSG_POOL_SIZE (16 * 5) /* send/recv pool */ /* 16*8 doesn't work*/
+//#define MSG_POOL_SIZE (16 * 7) /* send/recv pool */ /* 16*6 doesn't work on Jack_sc on fox4*/
+#define RB_MAX_CHUNKS	128 /* Max. Actual used size  RB_NR_CHUNKS */
+#define RB_NR_CHUNKS	128 /* Actual used size */
 #define RB_CHUNK_ORDER	(MAX_ORDER - 1)
 #define RB_CHUNK_SIZE	(PAGE_SIZE << RB_CHUNK_ORDER)
 
diff --git a/virt/kvm/arm/vgic-v3-emul.c b/virt/kvm/arm/vgic-v3-emul.c
index e661e7fb9d91..f1bfa3462eb1 100644
--- a/virt/kvm/arm/vgic-v3-emul.c
+++ b/virt/kvm/arm/vgic-v3-emul.c
@@ -49,6 +49,9 @@
 
 #include "vgic.h"
 
+#include <popcorn/debug.h>
+extern struct kvm_vcpu *first_vcpu;
+
 static bool handle_mmio_rao_wi(struct kvm_vcpu *vcpu,
 			       struct kvm_exit_mmio *mmio, phys_addr_t offset)
 {
@@ -944,12 +947,52 @@ void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg)
 	mpidr = SGI_AFFINITY_LEVEL(reg, 3);
 	mpidr |= SGI_AFFINITY_LEVEL(reg, 2);
 	mpidr |= SGI_AFFINITY_LEVEL(reg, 1);
+#if POPHYPE_KVM
+	// Pophype problem:
+	// Tihs may never be called because there is no vcpu[1]'s metadata in kernel
+	// from access_gic_sgi() at ./arch/arm64/kvm/sys_regs.c
+	KVMPK("     %s %s(): SGI %d from CPU <%d> to CPU <?> "
+				"(printing here because current pophyp cannot reach "
+				"missing vcpu kernel metadata)\n",
+				__FILE__, __func__, sgi, vcpu_id);
+	KVMPK("[dbg]     reg 0x%llx sgi %d "
+					"broadcast (%c) target_cpus 0x%x mpidr 0x%llx\n",
+					reg, sgi, broadcast ? 'O' : 'X', target_cpus, mpidr);
+
+	//KVMPK("[dbg] vcpu %p kvm %p arch %p vgic_dist %p %p %p <vs> "
+	//			"first_vcpu %p kvm %p arch %p vgic_dist %p %p %p\n",
+	//			vcpu, vcpu->kvm, vcpu->kvm->arch,
+	//			&vcpu->kvm->arch.vgic,
+	//			&vcpu->kvm->arch.vgic.irq_pending,
+	//			vcpu->kvm->arch.vgic.irq_pending.private,
+	//			first_vcpu, first_vcpu->kvm, first_vcpu->kvm->arch,
+	//			&first_vcpu->kvm->arch.vgic,
+	//			&first_vcpu->kvm->arch.vgic.irq_pending,
+	//			first_vcpu->kvm->arch.vgic.irq_pending.private);
+
+	// send(1)
+	// reg = 0xfff0ff07ff0000;
+	// sgi = 7;
+	// broadcast = false;
+	// target_cpus = 0;
+	// mpidr = 0xFF00;
+	//  dbg reg 0xfff0ff07ff0000 sgi 7 broadcast (X) target_cpus 0 mpidr 0xff00
+	//  notthing shown in next loop
+
+	// send(0) //self
+	// reg 0x7000001 sgi 7 broadcast (X) target_cpus 1 mpidr 0x0
+	// SGI 7 from CPU <0> to CPU <0> =>
+#endif
 
 	/*
 	 * We take the dist lock here, because we come from the sysregs
 	 * code path and not from the MMIO one (which already takes the lock).
 	 */
+	KVMPKV("%s %s(): trying to get lock <%d> (make sure I'm on the right vcpu)\n",
+			__FILE__, __func__, vcpu->vcpu_id); // dbg - first_vcpu = 0
+	KVMPKV("%s %s(): trying to get lock\n", __FILE__, __func__); // dbg
 	spin_lock(&dist->lock);
+	KVMPKV("%s %s(): LOCKED\n", __FILE__, __func__); // dbg
 
 	/*
 	 * We iterate over all VCPUs to find the MPIDRs matching the request.
@@ -958,7 +1001,16 @@ void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg)
 	 * VCPUs when most of the times we just signal a single VCPU.
 	 */
 	kvm_for_each_vcpu(c, c_vcpu, kvm) {
-
+#if POPHYPE_KVM && defined(CONFIG_POPCORN_CHECK_SANITY)
+		//unsigned long *reg2 = x->private + c;
+		KVMPK("     %s(): target:<%d>=<%d> target_cpus 0x%x %s %s %s\n",
+					__func__, c, c_vcpu->vcpu_id, target_cpus,
+					!broadcast && target_cpus == 0 ? "O*" : "X",
+					broadcast && c == vcpu_id ? "O*" : "X",
+					!broadcast ? "O" : "X*");
+		//KVMPK("     %s %s(): sgi %d -1 x->private %p reg2 %p\n",
+		//			__FILE__, __func__, sgi, x->private, reg2);
+#endif
 		/* Exit early if we have dealt with all requested CPUs */
 		if (!broadcast && target_cpus == 0)
 			break;
@@ -971,6 +1023,14 @@ void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg)
 			int level0;
 
 			level0 = match_mpidr(mpidr, target_cpus, c_vcpu);
+#if POPHYPE_KVM && defined(CONFIG_POPCORN_CHECK_SANITY)
+			if (level0 == -1) {
+				KVMPK("[Attention]     "
+					"%s(): <%d> level0 == -1 [continue %s]\n",
+							__func__, c,
+							level0 == -1 ? "O*" : "X");
+			}
+#endif
 			if (level0 == -1)
 				continue;
 
@@ -981,13 +1041,32 @@ void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg)
 		/* Flag the SGI as pending */
 		vgic_dist_irq_set_pending(c_vcpu, sgi);
 		updated = 1;
+#if POPHYPE_KVM
+		// Pophype problem:
+		// Tihs may never be called because there is no vcpu[1]'s metadata in kernel
+		// from access_gic_sgi() at ./arch/arm64/kvm/sys_regs.c
+		// if (vcpu_id == 1 && c == 0) { // TODO: assuming only 2vcpu in total now
+		KVMPK("     %s(): inject SGI %d from CPU <%d> to CPU <%d>\n",
+					__func__, sgi, vcpu_id, c);
+		//}
+#endif
 		kvm_debug("SGI%d from CPU%d to CPU%d\n", sgi, vcpu_id, c);
 	}
+#if POPHYPE_KVM
+	//KVMPK("%s %s(): SGI %d loop done 1\n", __FILE__, __func__, sgi);
+#endif
 	if (updated)
 		vgic_update_state(vcpu->kvm);
+#if POPHYPE_KVM
+	//KVMPK("%s %s(): SGI %d loop done 2\n", __FILE__, __func__, sgi);
+#endif
 	spin_unlock(&dist->lock);
+	KVMPKV("%s %s(): UNLOCKED\n", __FILE__, __func__); // dbg
 	if (updated)
 		vgic_kick_vcpus(vcpu->kvm);
+#if POPHYPE_KVM
+	//KVMPK("%s %s(): SGI %d loop done 3\n", __FILE__, __func__, sgi);
+#endif
 }
 
 static int vgic_v3_create(struct kvm_device *dev, u32 type)
diff --git a/virt/kvm/arm/vgic.c b/virt/kvm/arm/vgic.c
index 5d10f104f3eb..f80575563b41 100644
--- a/virt/kvm/arm/vgic.c
+++ b/virt/kvm/arm/vgic.c
@@ -34,6 +34,8 @@
 #include <asm/kvm.h>
 #include <kvm/iodev.h>
 
+#include <popcorn/debug.h>
+
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
@@ -159,6 +161,10 @@ static int vgic_init_bitmap(struct vgic_bitmap *b, int nr_cpus, int nr_irqs)
 	if (!b->private)
 		return -ENOMEM;
 
+	KVMPK("%s %s(): struct vgic_bitmap ->private %p (need irq_pending) "
+			"nr_cpus %d nr_irqs %d\n",
+			__FILE__, __func__, b->private, nr_cpus, nr_irqs);
+
 	b->shared = b->private + nr_cpus;
 
 	return 0;
@@ -242,6 +248,10 @@ static int vgic_init_bytemap(struct vgic_bytemap *x, int nr_cpus, int nr_irqs)
 	if (!x->private)
 		return -ENOMEM;
 
+	KVMPK("%s %s(): struct vgic_bitmap ->private %p (need irq_pending) "
+			"nr_cpus %d nr_irqs %d\n",
+			__FILE__, __func__, x->private, nr_cpus, nr_irqs);
+
 	x->shared = x->private + nr_cpus * VGIC_NR_PRIVATE_IRQS / sizeof(u32);
 	return 0;
 }
@@ -1531,6 +1541,21 @@ static int vgic_update_irq_pending(struct kvm *kvm, int cpuid,
 	int enabled;
 	bool ret = true, can_inject = true;
 
+#if POPHYPE_KVM && defined(CONFIG_POPCORN_CHECK_SANITY)
+	{
+		static u64 cnt = 0;
+		cnt++;
+		if (cnt < 5 || !(cnt % 1000)) {
+			// many: after a while....... this is called. I thought it's never called
+			KVMPKV("%s %s(): inject interrupt at <%d> "
+					"dbg will grab dist->lock #%lld\n",
+					__FILE__, __func__, cpuid, cnt);
+		}
+		if (cnt == 1) {
+			dump_stack();
+		}
+	}
+#endif
 	trace_vgic_update_irq_pending(cpuid, irq_num, level);
 
 	if (irq_num >= min(kvm->arch.vgic.nr_irqs, 1020))
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index b814ae6822b6..51994ea436dc 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -60,6 +60,9 @@
 #include "async_pf.h"
 #include "vfio.h"
 
+#include <popcorn/debug.h>
+struct kvm_vcpu *first_vcpu = NULL;
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/kvm.h>
 
@@ -279,6 +282,9 @@ static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	int need_tlb_flush, idx;
 
+//#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY) && defined(__aarch64__)
+//	printk("%s %s(): (arm called?)\n", __FILE__, __func__);
+//#endif
 	/*
 	 * When ->invalidate_page runs, the linux pte has been zapped
 	 * already but the page is still allocated until
@@ -297,18 +303,66 @@ static void kvm_mmu_notifier_invalidate_page(struct mmu_notifier *mn,
 	 * pte after kvm_unmap_hva returned, without noticing the page
 	 * is going to be freed.
 	 */
+
+#if defined(CONFIG_POPCORN_DSHM)
+	//if (address == 0x7ffd64000000) { // logic is wrong
+	//	kvm->tlbs_dirty = 1;
+	//	printk("%s %s(): %lx "
+	//			"force kvm->tlbs_dirty to be 1 (TODO TRY IT)\n",
+	//				__FILE__, __func__, address);
+	//}
+#endif
 	idx = srcu_read_lock(&kvm->srcu);
 	spin_lock(&kvm->mmu_lock);
 
 	kvm->mmu_notifier_seq++;
 	need_tlb_flush = kvm_unmap_hva(kvm, address) | kvm->tlbs_dirty;
+	// kvm->tlbs_dirty always zero
+	// kvm_unmap_hva()
+	// x86:  need_tlb_flush often = 1
+	// arm: need_tlb_flush = 0
+
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (address == 0x7ffd64000000) {
+		//POPPK_GDSHM("%s(): %lx need_tlb_flush %d %ld\n",
+		//		__func__, address, need_tlb_flush, kvm->tlbs_dirty);
+		//
+		// arm: always 0
+
+		// force tlb flush
+		//printk("%s %s(): %lx "
+		//		"force need_tlb_flush to be 1 (TODO TRY IT)\n",
+		//			__FILE__, __func__, address);
+		//need_tlb_flush = 1; // doesn't work
+	}
+#endif
 	/* we've to flush the tlb before the pages can be freed */
 	if (need_tlb_flush)
-		kvm_flush_remote_tlbs(kvm);
+		kvm_flush_remote_tlbs(kvm); // arm doesn't have
+	// x86: at virt/kvm/kvm_main.c?
+	// arm: at ./arch/arm/kvm/mmu.c flush all VM TLB entries for v7/8
+	//		-> kvm_call_hyp(__kvm_tlb_flush_vmid, kvm);	at ./arch/arm/kvm/mmu.c
+
+	// kvm_tlb_flush_vmid_ipa() -> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, kvm, ipa);
 
 	spin_unlock(&kvm->mmu_lock);
 
 	kvm_arch_mmu_notifier_invalidate_page(kvm, address);
+	// x86: -> if APIC PAGE -> kvm_make_all_cpus_request()
+	// arm: do nothing...
+
+#if defined(CONFIG_POPCORN_DSHM) && defined(CONFIG_POPCORN_CHECK_SANITY)
+	if (address == 0x7ffd64000000) {
+	//	printk("%s %s(): %lx trying to put s2 page\n",
+	//			__FILE__, __func__, address);
+	//	put_page(virt_to_page(pte));
+		// force tlb flush
+		//printk("%s %s(): %lx "
+		//		"force need_tlb_flush to be 1 (TODO TRY IT)\n",
+		//			__FILE__, __func__, address);
+		//need_tlb_flush = 1; // doesn't work
+	}
+#endif
 
 	srcu_read_unlock(&kvm->srcu, idx);
 }
@@ -448,8 +502,12 @@ static int kvm_mmu_notifier_test_young(struct mmu_notifier *mn,
 	return young;
 }
 
+#if defined(CONFIG_POPCORN_DSHM)
+void kvm_mmu_notifier_release(struct mmu_notifier *mn, struct mm_struct *mm)
+#else
 static void kvm_mmu_notifier_release(struct mmu_notifier *mn,
 				     struct mm_struct *mm)
+#endif
 {
 	struct kvm *kvm = mmu_notifier_to_kvm(mn);
 	int idx;
@@ -2306,6 +2364,18 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 		goto unlock_vcpu_destroy;
 	}
 
+	KVMPK("\n\n\n\n======================================\n"
+			"%s %s(): id %u vcpu->vcpu_id %d fd %d vcpu %p\n"
+			"==========================================\n\n\n",
+			__FILE__, __func__, id, vcpu->vcpu_id, r, vcpu);
+	//if (!first_vcpu) {
+		/* ARM comes twice when 1vcpu, the last one is correct
+			So it's not first_vcpu but last_vcpu..... */
+		KVMPK("%s %s(): first_vcpu %p (installed)\n", __FILE__, __func__, vcpu);
+		first_vcpu = vcpu;
+		dump_stack();
+	//}
+
 	kvm->vcpus[atomic_read(&kvm->online_vcpus)] = vcpu;
 
 	/*
